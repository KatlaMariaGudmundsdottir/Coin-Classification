{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling training data\n",
    "Given the relatively small size of our training data, we manually labelled the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imaging library Pillow\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data from Kaggle\n",
    "unlabeled_data_folder = os.path.join(\"Data\", \"train\")\n",
    "\n",
    "# XML files created by LabelIMG\n",
    "data_labels_folder = os.path.join(\"Labelled_Training_Data\")\n",
    "\n",
    "# Cropped images sorted by coin type\n",
    "extracted_data_folder = os.path.join(\"Extracted_Training_Data\")\n",
    "os.makedirs(extracted_data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(input_image_path, output_folder, object_list):\n",
    "    # Open the input image\n",
    "    with Image.open(input_image_path) as img:\n",
    "        for index, object in enumerate(object_list):\n",
    "            # Crop the image\n",
    "            name = object['name']\n",
    "            cropped_img = img.crop((int(object['xmin']), int(object['ymin']), int(object['xmax']), int(object['ymax'])))\n",
    "            output_coin_type = os.path.join(output_folder, name)\n",
    "            # Ensure the output folder exists\n",
    "            os.makedirs(output_coin_type, exist_ok=True)\n",
    "            \n",
    "            # Construct the output image path, weird naming to avoid duplicates from same image, index placed before filetype\n",
    "            image_name = os.path.basename(input_image_path[:-4]) + '_' + str(index) + '.JPG'\n",
    "            output_image_path = os.path.join(output_coin_type, image_name)\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_img.save(output_image_path)\n",
    "            print(f'Cropped image saved to: {output_image_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_file_content(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    object_list = []\n",
    "    for object in root.findall('object'):\n",
    "        d = dict()\n",
    "        d['name'] = object.findtext('name')\n",
    "        bounding_box = object.find('bndbox')\n",
    "        d['xmin'] = bounding_box.findtext('xmin')\n",
    "        d['ymin'] = bounding_box.findtext('ymin')\n",
    "        d['xmax'] = bounding_box.findtext('xmax')\n",
    "        d['ymax'] = bounding_box.findtext('ymax')\n",
    "        object_list.append(d)\n",
    "\n",
    "    return object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(unlabeled_data_folder):\n",
    "    label_folder = os.path.join(data_labels_folder, folder)\n",
    "    folder = os.path.join(unlabeled_data_folder, folder)\n",
    "    for image in os.listdir(folder):\n",
    "        # Find corresponding XML_file\n",
    "        for label_file in os.listdir(label_folder):\n",
    "            if (image[:-4] in label_file):\n",
    "                label_filepath = os.path.join(label_folder, label_file)\n",
    "                image_filepath = os.path.join(folder, image)\n",
    "                object_list = read_xml_file_content(label_filepath)\n",
    "\n",
    "                crop_and_save_image(image_filepath, extracted_data_folder, object_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt with classical segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in images\n",
    "Let's first load in the images into the notebook, based on the folders downloaded from the Kaggle challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"Data/train\"\n",
    "train_folders = sorted(os.listdir(path_to_train))\n",
    "\n",
    "# Load in all images from the training set into this dictionary\n",
    "train_images_grouped = {}\n",
    "\n",
    "for folder in train_folders:\n",
    "    train_images = []\n",
    "    folder_path = os.path.join(path_to_train, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    images = os.listdir(folder_path)\n",
    "    for image in images:\n",
    "        image_path = os.path.join(folder_path, image)\n",
    "        loaded_image = cv.imread(image_path)\n",
    "        loaded_image = cv.cvtColor(loaded_image, cv.COLOR_BGR2RGB) \n",
    "        train_images.append(loaded_image)\n",
    "    folder = folder.replace(\" \", \"\")  # Remove whitespace\n",
    "    import re\n",
    "    folder = re.sub(\"[0-9.]\", \"\", folder) # Remove numbers and '.'\n",
    "    train_images_grouped[str(folder)] = train_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some samples from the six different background types for testing our segmentation model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the image_nums at once to see different images\n",
    "image_num = 2\n",
    "\n",
    "# Or modify each index individually to see different images\n",
    "neutral_bg_sample = train_images_grouped[\"neutral_bg\"][image_num]\n",
    "noisy_bg_sample = train_images_grouped[\"noisy_bg\"][image_num]\n",
    "hand_bg_sample = train_images_grouped[\"hand\"][image_num]\n",
    "neutral_bg_sample_ood = train_images_grouped[\"neutral_bg_outliers\"][image_num]\n",
    "noisy_bg_sample_ood = train_images_grouped[\"noisy_bg_outliers\"][image_num]\n",
    "hand_bg_sample_ood = train_images_grouped[\"hand_outliers\"][image_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Here we check if the number of training images loaded for each class is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0 \n",
    "for group in train_images_grouped:\n",
    "    group_len = len(train_images_grouped[group])\n",
    "    N += group_len\n",
    "    print(f\"Group name: '{group}' \\n with length: {group_len} \\n\")\n",
    "\n",
    "print(f\"N: {N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with thresholding, morphology & watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions. \n",
    "\n",
    "Since we used GIMP to analyse the overall colors of each coin based on the reference images, `gimp2opencvHSV` helps us convert from GIMP HSV values to OpenCV HSV values. \n",
    "\n",
    "And using HSV as our thresholding is standard, as it's easier to segment colors with this colorspace. This was also apparent in Lab 1. Thus, we have `extract_hsv_channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimp2opencvHSV(h, s, v):\n",
    "    \"\"\"\n",
    "    Convert GIMP HSV values to OpenCV HSV values.\n",
    "    GIMP uses the range [0, 360] for H, [0, 100] for S and V.\n",
    "    OpenCV uses the range [0, 180] for H, [0, 255] for S and V.\n",
    "    \"\"\"\n",
    "    return 180 * (h / 360), 255 * (s / 100), 255 * (v / 100)\n",
    "\n",
    "def extract_hsv_channels(image):\n",
    "    \"\"\"\n",
    "    Extracts the HSV channels from an image.\n",
    "    \"\"\"\n",
    "    hsv_image = cv.cvtColor(image, cv.COLOR_RGB2HSV)\n",
    "    h, s, v = cv.split(hsv_image)\n",
    "    return h, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the actual segmentation model. It involves\n",
    "\n",
    "1. **Thresholding based on HSV:** The threshold values were estimated from the reference images and their analysis in GIMP. The problems we face here is finding good thresholds which generalizes well.\n",
    "2. **Thresholding based on grayscale:** Here, we utilize simple grayscale thresholding, which is simple thresholding that binarizes the image. We also tried adding Otsu's thresholding, which uses the grayscale histogram of an image to detect an optimal threshold value that separates two regions with maximum inter-class variance. We tried all combinations, each with subpar results.\n",
    "3. **Morphology with closing and opening:** This is to close the holes *inside* the coins, which were removed by the threshold. If we didn't do this, the later openings and watershed algorithm would completely destroy the \"hole\" coin. Then, we applied opening. This was to break down the bridges connecting two or more coins, as well as remove background noise which were missed by the threshold.\n",
    "4. **Watershed algorithm:** The [watershed](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html) method is popular to find contours of shapes that \"touch\" each other. We need this because in the given images, coins may touch even after thresholding. \n",
    "5. **Finding contours based on results from watershed algorithm:** Now that we have the \"marked shapes\" from the watershed algorithm, we find the contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(img_original):\n",
    "    img = img_original.copy()\n",
    "\n",
    "    # We will use HSV color space for thresholding\n",
    "    h, s, v = extract_hsv_channels(img)\n",
    "\n",
    "    # The following are GIMP HSV values\n",
    "    h_upper = 65\n",
    "    h_lower = 18\n",
    "\n",
    "    s_upper = 85\n",
    "    s_lower = 10\n",
    "\n",
    "    v_upper = 95\n",
    "    v_lower = 40\n",
    "\n",
    "    # Convert to HSV values\n",
    "    img = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "\n",
    "    # Main thresholding for coin and background separation\n",
    "    img_thres = cv.inRange(\n",
    "        img,\n",
    "        gimp2opencvHSV(h_lower, s_lower, v_lower),\n",
    "        gimp2opencvHSV(h_upper, s_upper, v_upper),\n",
    "    )\n",
    "\n",
    "    # Additional mask for dimly lit background. This was NOT getting thresholded.\n",
    "    background_mask = cv.inRange(\n",
    "        img, gimp2opencvHSV(32, 12, 72), gimp2opencvHSV(38, 28, 78)\n",
    "    )\n",
    "\n",
    "    # Additional mask for light silver coins. This was getting thresholded, likely due to bright lighting issues.\n",
    "    light_silver_coin_mask = cv.inRange(\n",
    "        img, gimp2opencvHSV(190, 6.2, 72.3), gimp2opencvHSV(205, 10.3, 81.2)\n",
    "    )\n",
    "\n",
    "    # Apply the masks\n",
    "    img[img_thres == 0 & ~(light_silver_coin_mask == 255)] = 0\n",
    "    img[background_mask == 255] = 0\n",
    "\n",
    "    # For some reason, there isn't a direct way to convert from HSV from gray?\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2RGB)\n",
    "\n",
    "    # Threshold on grayscale image using simple thresholding & Otsu's thresholding\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    ret, img_thresh_gray = cv.threshold(\n",
    "        img_gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU\n",
    "    )\n",
    "\n",
    "    thresh = cv.bitwise_not(img_thresh_gray)\n",
    "\n",
    "    # Closing small holes\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, kernel, iterations=5)\n",
    "\n",
    "    # Removing small background noise and breaking down \"bridges\"\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations=8)\n",
    "\n",
    "    # Closing small holes again\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, kernel, iterations=5)\n",
    "\n",
    "    # Watershed algorithm\n",
    "    # Sure background area\n",
    "    sure_bg = cv.dilate(thresh, kernel, iterations=8)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv.distanceTransform(thresh, cv.DIST_L2, 5)\n",
    "    ret, sure_fg = cv.threshold(dist_transform, 0.2 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # The signal wasn't strong enough for a clear distance transform,\n",
    "    # so increasing the threshold (0.2) we get less of the coins but less of the bridges,\n",
    "    # but by lowering the threshold, we preserve more of the coins but also more of the bridges...\n",
    "    # thus we tried to erode again to remove the bridges again.\n",
    "\n",
    "    kernel = np.ones((16, 16), np.uint8)\n",
    "    sure_fg = cv.erode(sure_fg, kernel, iterations=8)\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    sure_fg = cv.dilate(sure_fg, kernel, iterations=4)\n",
    "\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Marker labelling\n",
    "    # Connected Components determines the connectivity of blob-like regions in a binary image.\n",
    "    ret, markers = cv.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "\n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    markers = cv.watershed(img, markers)\n",
    "    img[markers == -1] = [255, 0, 0]  # Optionally mark -1 boundaries if needed\n",
    "\n",
    "    # Create an output image to draw on\n",
    "    output_img = img_original.copy()\n",
    "\n",
    "    all_contours = []\n",
    "    # Process each region\n",
    "    for label in np.unique(markers):\n",
    "        if label == 0 or label == 1:  # Background or borders\n",
    "            continue\n",
    "\n",
    "        # Create a mask for the current region\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        mask[markers == label] = 255\n",
    "\n",
    "        # Find contours and get the bounding box\n",
    "        contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for cntr in contours:\n",
    "            area = cv.contourArea(cntr)\n",
    "\n",
    "            # Check if contour is big enough\n",
    "            if area < 70000 or area > 700000:\n",
    "                continue\n",
    "            \n",
    "            # The contours found above weren't good circles either, so the conditions below were removed.\n",
    "            # # Check if contour is \"cirular\" enough\n",
    "            # perimeter = cv.arcLength(cntr, True)\n",
    "\n",
    "            # # Calculate circularity\n",
    "            # if perimeter == 0:\n",
    "            #     continue  # Avoid division by zero\n",
    "\n",
    "            # circularity = 4 * np.pi * (area / (perimeter**2))\n",
    "\n",
    "            # if circularity < 0.1:\n",
    "            #     continue\n",
    "\n",
    "            # # Calculate convexity\n",
    "            # hull = cv.convexHull(cntr)\n",
    "            # hull_area = cv.contourArea(hull)\n",
    "\n",
    "            # if hull_area == 0:\n",
    "            #     continue\n",
    "            # convexity = area / hull_area\n",
    "\n",
    "            # if convexity < 0.8:\n",
    "            #     continue\n",
    "\n",
    "            # Calculate bounding box\n",
    "            x, y, w, h = cv.boundingRect(cntr)\n",
    "            # Draw bounding box\n",
    "            cv.rectangle(output_img, (x, y), (x + w, y + h), (255, 0, 0), 15)\n",
    "\n",
    "            # Draw contour (optional)\n",
    "            # cv.drawContours(output_img, [cntr], -1, (0, 0, 255), 2)\n",
    "\n",
    "            all_contours.append(cntr)\n",
    "\n",
    "    return output_img, all_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, coins are getting recognized, okay. But there are a few obvious problems:\n",
    "1. Backgrounds are getting falsely detected as coins. This is most likely due to insufficient thresholding and morphology operations. See the yellow spots of noisy background and parts of the hand. We have tried to calculate the shape attributes, such as convexity and circularity, but they don't work well because the contours of real coins and false coins were both also imperfect.\n",
    "2. The bounding boxes of coins are not perfectly alligned. Although not a big issue if we train our classifier well, it introduces an extra layer of noise and imperfection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to show more images to test on\n",
    "TOTAL_IMAGES = 1\n",
    "\n",
    "for image_num in range(TOTAL_IMAGES):\n",
    "    neutral_bg_sample = train_images_grouped[\"neutral_bg\"][image_num]\n",
    "    noisy_bg_sample = train_images_grouped[\"noisy_bg\"][image_num]\n",
    "    hand_bg_sample = train_images_grouped[\"hand\"][image_num]\n",
    "    neutral_bg_sample_ood = train_images_grouped[\"neutral_bg_outliers\"][image_num]\n",
    "    noisy_bg_sample_ood = train_images_grouped[\"noisy_bg_outliers\"][image_num]\n",
    "    hand_bg_sample_ood = train_images_grouped[\"hand_outliers\"][image_num]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(get_contours(neutral_bg_sample)[0])\n",
    "    plt.title(\"Neutral background\")\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(get_contours(noisy_bg_sample)[0])\n",
    "    plt.title(\"Noisy background\")\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(get_contours(hand_bg_sample)[0])\n",
    "    plt.title(\"Hand background\")\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(get_contours(neutral_bg_sample_ood)[0])\n",
    "    plt.title(\"Neutral background\")\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(get_contours(noisy_bg_sample_ood)[0])\n",
    "    plt.title(\"Noisy background\")\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(get_contours(hand_bg_sample_ood)[0])\n",
    "    plt.title(\"Hand background\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of poor segmentation. We can see why the segmentation model might misclassify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the bounding boxes from the images\n",
    "def crop_bounding_boxes(img_original, contours):\n",
    "    '''\n",
    "    Takes in an image with multiple coins and its contours that represent the coins.\n",
    "    Outputs a list of cropped images.\n",
    "    '''\n",
    "\n",
    "    img = img_original.copy()\n",
    "    cropped_images = []\n",
    "\n",
    "    for cntr in contours:\n",
    "        area = cv.contourArea(cntr)\n",
    "\n",
    "        # Check if contour is big enough\n",
    "        if area < 70000 or area > 700000:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv.boundingRect(cntr)\n",
    "        cropped_images.append(img[y:y+h, x:x+w])\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "# Apply filters\n",
    "noisy_bg_sample_filtered, contours = get_contours(noisy_bg_sample)\n",
    "\n",
    "# Remember to pass in the original image, otherwise we get the red bounding boxes as well\n",
    "cropped_coins = crop_bounding_boxes(noisy_bg_sample, contours)\n",
    "\n",
    "# Plot the cropped coins\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, coin in enumerate(cropped_coins):\n",
    "    plt.subplot(1, len(cropped_coins), i+1)\n",
    "    plt.imshow(coin)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other failed classical segmentation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried using hough transform and background subtraction as well, both with hard-to-refine results. They weren't that much better than attempt 1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message for Om\n",
    "I didn't add any other segmentation techniques before feeding hough transform here, it just converts to grayscale. If you want to feed it a thresholded image you can do that as well, you just have to take out the imread(src) and maybe the resizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_transform(image):\n",
    "    # Loads an image\n",
    "    src = image\n",
    "    # Check if image is loaded fine\n",
    "    if src is None:\n",
    "        print ('Error: No Imgae')\n",
    "\n",
    "    # Load image\n",
    "    large_img = cv.imread(src)\n",
    "    small_to_large_image_size_ratio = 0.2\n",
    "    small_img = cv.resize(large_img, # original image\n",
    "                       (0,0), # set fx and fy, not the final size\n",
    "                       fx=small_to_large_image_size_ratio, \n",
    "                       fy=small_to_large_image_size_ratio, \n",
    "                       interpolation=cv.INTER_NEAREST)\n",
    "    gray = cv.cvtColor(small_img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 30,\n",
    "    param1=60, param2=80,\n",
    "    minRadius=25, maxRadius=90)\n",
    "\n",
    "    # Add circles and centerpoint\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            # circle center\n",
    "            cv.circle(small_img, center, 1, (0, 100, 100), 3)\n",
    "            # circle outline\n",
    "            radius = i[2]\n",
    "            cv.circle(small_img, center, radius, (255, 0, 255), 3)\n",
    " \n",
    "    plt.imshow(cv.cvtColor(small_img, cv.COLOR_RGB2BGR))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define paths\n",
    "path_neutral_bg = os.path.join(\"data/train/\", \"1. neutral_bg\", \"L1010277.JPG\")\n",
    "path_noisy_bg = os.path.join(\"data/train/\", \"2. noisy_bg\", \"L1010325.JPG\")\n",
    "\n",
    "hough_transform(path_neutral_bg)\n",
    "hough_transform(path_noisy_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four distinct backgrounds in the given images. One for neutral background, two for noisy backgrounds, and one for hand background. For each type, the images are aligned, even for test images. So, we can find take the simple mean for each set, and get a model for each type of background. Then, by subtracting the test image with the background model, we theoretically should get only the coins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the background models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of neutral background\n",
    "# ---\n",
    "# Get the background model\n",
    "neutral_bg_images = train_images_grouped[\"neutral_bg\"]\n",
    "neutral_bg_images_ood = train_images_grouped[\"neutral_bg_outliers\"]\n",
    "print(f\"Number of images in neutral background: {len(neutral_bg_images)}\")\n",
    "print(f\"Number of images in neutral background ood: {len(neutral_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "neutral_bg_images = np.array(neutral_bg_images)\n",
    "neutral_bg_images_ood = np.array(neutral_bg_images_ood)\n",
    "\n",
    "neutral_bg_avg = np.mean(neutral_bg_images, axis=0, dtype=np.int32)\n",
    "neutral_bg_avg_ood = np.mean(neutral_bg_images_ood, axis=0, dtype=np.int32)\n",
    "\n",
    "# Take average between non-ood and ood\n",
    "neutral_bg_avg = (neutral_bg_avg + neutral_bg_avg_ood) // 2\n",
    "# ---\n",
    "\n",
    "# Distribution of noisy background type A\n",
    "# ---\n",
    "# Get the background model\n",
    "noisy_bg_images = train_images_grouped[\"noisy_bg\"]\n",
    "print(f\"Number of images in noisy background: {len(noisy_bg_images)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "noisy_bg_images = np.array(noisy_bg_images)\n",
    "noisy_bg_avg = np.mean(noisy_bg_images, axis=0, dtype=np.int32)\n",
    "# ---\n",
    "\n",
    "# Distribution of noisy background type B\n",
    "# ---\n",
    "# Get the background model\n",
    "noisy_bg_images_ood = train_images_grouped[\"noisy_bg_outliers\"]\n",
    "print(f\"Number of images in noisy background ood: {len(noisy_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "noisy_bg_images_ood = np.array(noisy_bg_images_ood)\n",
    "noisy_bg_avg_ood = np.mean(noisy_bg_images_ood, axis=0, dtype=np.int32)\n",
    "# ---\n",
    "\n",
    "# Distribution of hand background\n",
    "# ---\n",
    "# Get the background model\n",
    "hand_bg_images = train_images_grouped[\"hand\"]\n",
    "hand_bg_images_ood = train_images_grouped[\"hand_outliers\"]\n",
    "print(f\"Number of images in hand background: {len(hand_bg_images)}\")\n",
    "print(f\"Number of images in hand background ood: {len(hand_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "hand_bg_images = np.array(hand_bg_images)\n",
    "hand_bg_avg = np.mean(hand_bg_images, axis=0, dtype=np.int32)\n",
    "\n",
    "hand_bg_images_ood = np.array(hand_bg_images_ood)\n",
    "hand_bg_avg_ood = np.mean(hand_bg_images_ood, axis=0, dtype=np.int32)\n",
    "\n",
    "# Take average between non-ood and ood\n",
    "hand_bg_avg = (hand_bg_avg + hand_bg_avg_ood) // 2\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the segmentation model with background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt something similar with attempt 1, but with background subtraction instead. This is the pipeline:\n",
    "\n",
    "1. **Subtract background:** We first detect the background, by calculating the MSE between the test image and the background model. The background model with the lowest MSE is the detected background. If the background is a hand, we will perform normal thresholding and not do background subtraction. This is because the hands are not aligned. Otherwise, background subtraction is performed.\n",
    "2. **Enhance contrast:** Increasing contrast will help separate the coins from the background more. We also tried this above with the 1st attempt, with poor results. This means the image will be in grayscale.\n",
    "3. **Thresholding:** Since we are in grayscale, let's rely on the increased contrast and Li thresholding. It worked better than Otsu, which is why we used it.\n",
    "4. **Morphology:** We used morphology to remove small holes (which again, were thresholded away) and eroded it once more. We didn't want to dilate it back to reduce the chances of bridges between coins.\n",
    "5. **Contour detection:** We used the contour finding function from OpenCV as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import filters, measure, morphology\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_image(image, title=\"Image\", cmap_type=\"gray\"):\n",
    "    plt.imshow(image, cmap=cmap_type)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def enhance_contrast(image, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:  # Color image\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        cl = clahe.apply(l)\n",
    "        limg = cv2.merge([cl, a, b])\n",
    "        enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    else:  # Grayscale image\n",
    "        enhanced_img = clahe.apply(image)\n",
    "    return enhanced_img\n",
    "\n",
    "\n",
    "# Apply thresholding\n",
    "def apply_threshold(image, method=\"otsu\"):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
    "    thresh_val = (\n",
    "        filters.threshold_otsu(gray) if method == \"otsu\" else filters.threshold_li(gray)\n",
    "    )\n",
    "    binary = gray > thresh_val\n",
    "    return binary\n",
    "\n",
    "\n",
    "def clean_image(binary_image):\n",
    "    # Removing small holes\n",
    "    cleaned = morphology.remove_small_holes(binary_image, area_threshold=500)\n",
    "\n",
    "    # Erode without going back to the original size\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (13, 13))\n",
    "    cleaned = cv2.erode(cleaned.astype(np.uint8), kernel, iterations=3)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def detect_coins(binary_image, original_image):\n",
    "    all_contours = []\n",
    "    output_img = original_image.copy()\n",
    "    contours, _ = cv2.findContours(\n",
    "        binary_image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    for contour in contours:\n",
    "        if (\n",
    "            cv2.contourArea(contour) > 70000 and cv2.contourArea(contour) < 700000\n",
    "        ):  # Adjust size threshold based on your need\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "            all_contours.append(contour)\n",
    "\n",
    "    return output_img, all_contours\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    # Compute the mean squared error between the two images\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "\n",
    "def detect_background(image):\n",
    "    metrics = {}\n",
    "    backgrounds = [neutral_bg_avg, noisy_bg_avg, noisy_bg_avg_ood, hand_bg_avg]\n",
    "\n",
    "    # Compute MSE for each background\n",
    "    for idx, bg in enumerate(\n",
    "        [neutral_bg_avg, noisy_bg_avg, noisy_bg_avg_ood, hand_bg_avg], 0\n",
    "    ):\n",
    "        mse_val = mse(image, bg)\n",
    "        metrics[idx] = mse_val\n",
    "\n",
    "    best_match_by_mse = min(metrics, key=lambda k: metrics[k])\n",
    "\n",
    "    print(\"Metrics (MSE):\", metrics)\n",
    "    print(\"Best match by MSE:\", best_match_by_mse)\n",
    "\n",
    "    return backgrounds[best_match_by_mse], best_match_by_mse\n",
    "\n",
    "\n",
    "def apply(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    bg, idx = detect_background(image)\n",
    "    background = bg.copy()\n",
    "\n",
    "    if idx == 3:  # Hand background\n",
    "\n",
    "        # Convert to HSV\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "        # Threshold hand away\n",
    "        hand_mask_light_skin = cv2.inRange(\n",
    "            img, gimp2opencvHSV(310, 5, 70), gimp2opencvHSV(360, 25, 85)\n",
    "        )\n",
    "        hand_mask_dark_skin = cv2.inRange(\n",
    "            img, gimp2opencvHSV(0, 20, 65), gimp2opencvHSV(25, 55, 83)\n",
    "        )\n",
    "\n",
    "        # Remove hand\n",
    "        img[hand_mask_light_skin == 255] = 0\n",
    "        img[hand_mask_dark_skin == 255] = 0\n",
    "\n",
    "        # Convert back to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        # Enhance contrast\n",
    "        enhance_contrast_image = enhance_contrast(img, clip_limit=5.0)\n",
    "\n",
    "        # Apply thresholding\n",
    "        thresholded_image = apply_threshold(enhance_contrast_image)\n",
    "\n",
    "        # Clean image\n",
    "        cleaned_image = clean_image(thresholded_image)\n",
    "\n",
    "        # Detect coins\n",
    "        detected_coins_image, contours = detect_coins(\n",
    "            cleaned_image.astype(np.uint8), img\n",
    "        )\n",
    "\n",
    "        return detected_coins_image, contours\n",
    "\n",
    "    else:\n",
    "        img = np.array(img)\n",
    "        img = img.astype(np.int32)\n",
    "        img = cv2.absdiff(img, background)\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        enhance_contrast_image = enhance_contrast(img)\n",
    "\n",
    "        thresholded_image = apply_threshold(enhance_contrast_image, method=\"li\")\n",
    "    \n",
    "        cleaned_image = clean_image(thresholded_image)\n",
    "\n",
    "        detected_coins_image, contours = detect_coins(\n",
    "            cleaned_image.astype(np.uint8), image\n",
    "        )\n",
    "\n",
    "        return detected_coins_image, contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the background detection works, but it is still much affected by the operations we performed, such as thresholding and morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = neutral_bg_sample.copy()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "detected_coins_image, contours = apply(image)\n",
    "\n",
    "plt.imshow(detected_coins_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet-50 for object localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time was running out, and classical segmentation methods were too sensitive to the parameters. We went with a transfer learning approach to find the bounding boxes of coins, i.e. object localization. In this sense, we went with ResNet-50, pre-trained on ImageNet, with size [97.8MB](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html).\n",
    "\n",
    "More specifically, we are using the [faster R-CNN model with a ResNet50 backbone](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html).\n",
    "\n",
    "Region-based Convolutional Neural Networks (R-CNN) is specifically to tackle object detection in computer vision tasks. Faster R-CNN is not strictly necessary, but it potentially lets us generalize the application to real-time coin detection in the future.\n",
    "\n",
    "Moreover, ResNet50 is a residual neural network. It is quite deep (50 layers) which helps it learn more detailed features of the image. Since our images are quite noisy and contains a lot of variation, this will make it more accurate for coin localization. Moreover, it is also one of the only two provided pre-trained models from PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** The folder structure of the training dataset must follow exactly:\n",
    "```\n",
    "dataset\n",
    "├── images\n",
    "│   ├── L1010277.JPG\n",
    "├── annotations\n",
    "│   ├── L1010277 [1].xml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the labelled training dataset into the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File folder organization\n",
    "unlabeled_data_folder = os.path.join(\"Data\", \"train\")\n",
    "\n",
    "data_labels_folder = os.path.join(\"Labelled_Training_Data\")\n",
    "\n",
    "coin_detection_dataset = os.path.join(\"dataset\")\n",
    "coin_detection_images = os.path.join(\"dataset\", \"images\")\n",
    "coin_detection_annotations = os.path.join(\"dataset\", \"annotations\")\n",
    "os.makedirs(coin_detection_dataset, exist_ok=True)\n",
    "os.makedirs(os.path.join(coin_detection_dataset, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(coin_detection_dataset, \"annotations\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Data/train/folder\n",
    "for folder in os.listdir(unlabeled_data_folder):\n",
    "    # Labelled_Training_Data/label_folder\n",
    "    label_folder = os.path.join(data_labels_folder, folder)\n",
    "    # folder = path to folder\n",
    "    folder = os.path.join(unlabeled_data_folder, folder)\n",
    "    for image in os.listdir(folder):\n",
    "        # Find corresponding XML_file\n",
    "        for label_file in os.listdir(label_folder):\n",
    "            # Ex: L1010277.JPG --> L1010277\n",
    "            if (image[:-4] in label_file):\n",
    "                label_filepath = os.path.join(label_folder, label_file)\n",
    "                image_filepath = os.path.join(folder, image)\n",
    "                shutil.copy2(label_filepath, coin_detection_annotations)\n",
    "                shutil.copy2(image_filepath, coin_detection_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `CoinDataset` is simply helping us parsing the images (.JPG) and annotations (.XML) in order to be fed into the network for training. After that, we will set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parser for data\n",
    "class CoinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.annotations = list(sorted(os.listdir(os.path.join(root, \"annotations\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        tree = ET.parse(ann_path)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # Background and coin\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Data transforms\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell if you want to train the model on the custom (coins) dataset. This cell actually loads in the data and starts the training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = CoinDataset(\"dataset/\", transforms=transforms)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "# Fine-tune the model based on our labelled training data\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"Beginning epoch\", epoch + 1)\n",
    "    for i, (images, targets) in enumerate(data_loader, 0):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(data_loader)}], Loss: {running_loss/10:.4f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"coin_detector.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the next cell if you already have a trained model, and skip the above cell. This is to prevent having to re-train the model every time we restart the kernel. Make sure `coin_detector.pth` is in the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['backbone.fpn.inner_blocks.0.weight', 'backbone.fpn.inner_blocks.0.bias', 'backbone.fpn.inner_blocks.1.weight', 'backbone.fpn.inner_blocks.1.bias', 'backbone.fpn.inner_blocks.2.weight', 'backbone.fpn.inner_blocks.2.bias', 'backbone.fpn.inner_blocks.3.weight', 'backbone.fpn.inner_blocks.3.bias', 'backbone.fpn.layer_blocks.0.weight', 'backbone.fpn.layer_blocks.0.bias', 'backbone.fpn.layer_blocks.1.weight', 'backbone.fpn.layer_blocks.1.bias', 'backbone.fpn.layer_blocks.2.weight', 'backbone.fpn.layer_blocks.2.bias', 'backbone.fpn.layer_blocks.3.weight', 'backbone.fpn.layer_blocks.3.bias', 'rpn.head.conv.weight', 'rpn.head.conv.bias'], unexpected_keys=['backbone.fpn.inner_blocks.0.0.weight', 'backbone.fpn.inner_blocks.0.0.bias', 'backbone.fpn.inner_blocks.1.0.weight', 'backbone.fpn.inner_blocks.1.0.bias', 'backbone.fpn.inner_blocks.2.0.weight', 'backbone.fpn.inner_blocks.2.0.bias', 'backbone.fpn.inner_blocks.3.0.weight', 'backbone.fpn.inner_blocks.3.0.bias', 'backbone.fpn.layer_blocks.0.0.weight', 'backbone.fpn.layer_blocks.0.0.bias', 'backbone.fpn.layer_blocks.1.0.weight', 'backbone.fpn.layer_blocks.1.0.bias', 'backbone.fpn.layer_blocks.2.0.weight', 'backbone.fpn.layer_blocks.2.0.bias', 'backbone.fpn.layer_blocks.3.0.weight', 'backbone.fpn.layer_blocks.3.0.bias', 'rpn.head.conv.0.0.weight', 'rpn.head.conv.0.0.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # Background and coin\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load(\"coin_detector.pth\"), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform object localization on test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will predict bounding boxes on the images, crop the coins out for each image and save them to an output folder.\n",
    "\n",
    "**IMPORTANT:** The folder structure of input test images must follow exactly:\n",
    "```\n",
    "data\n",
    "├── test\n",
    "│   ├── L1010277.JPG\n",
    "│   ├── L1010239.JPG\n",
    "```\n",
    "And the folder structure of output images will be:\n",
    "```\n",
    "output\n",
    "├── L0000000.JPG                        <---------------- This is a directory\n",
    "│   ├── 1920_1519_2595_2181.jpg         <---------------- This is an image of the 1st cropped coin, belonging to L0000000.JPG\n",
    "│   ├── 2780_2159_3326_2685.jpg         <---------------- This is an image of the 2nd cropped coin, belonging to L0000000.JPG\n",
    "├── L0000001.JPG                        \n",
    "│   ├── 1920_1519_2595_2181.jpg         \n",
    "│   ├── 2780_2159_3326_2685.jpg         \n",
    "```\n",
    "If an \"output\" folder already exists, it will get overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on test images\n",
    "model.eval()\n",
    "test_data_path = \"Data/test\"\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "for img_name in os.listdir(test_data_path):\n",
    "    img_path = os.path.join(test_data_path, img_name)\n",
    "    # Check if the file is an image\n",
    "    if not img_name.endswith(\".JPG\") and not img_name.endswith(\".jpg\"):\n",
    "        continue\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "    # Create a directory for each image, where cropped coins will be saved\n",
    "    os.mkdir(f\"output/{img_name}\")\n",
    "    boxes = outputs[0][\"boxes\"].cpu().numpy()\n",
    "    # Crop coins from current image\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        coin = img.crop((xmin, ymin, xmax, ymax))\n",
    "        cropped_coin_path = os.path.join(\n",
    "            f\"output/{img_name}\", f\"{xmin}_{ymin}_{xmax}_{ymax}.jpg\"\n",
    "        )\n",
    "        coin.save(cropped_coin_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m test_images_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Image transformations\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m([\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataset\u001b[39;00m(Dataset):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_paths, labels, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "test_images_path = \"output\"\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(os.listdir(images_classes_path))}\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for label in os.listdir(images_classes_path):\n",
    "    class_path = os.path.join(images_classes_path, label)\n",
    "    for img in os.listdir(class_path):\n",
    "        train_data.append(os.path.join(class_path, img))\n",
    "        train_labels.append(class_to_idx[label])\n",
    "\n",
    "train_dataset = CustomDataset(train_data, train_labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2 as cv\n",
    "\n",
    "# images_classes_path = \"./Extracted_Training_data\"\n",
    "# image_classes_list = os.listdir(images_classes_path)\n",
    "# num_classes = len(image_classes_list)\n",
    "# print(\"total {} classes\".format(num_classes))\n",
    "\n",
    "# test_images_path = \"output\"\n",
    "# images_dict = {}\n",
    "# image_name_list = os.listdir(test_images_path)\n",
    "# for image_folder in image_name_list:\n",
    "#     images_dict[image_folder] = []\n",
    "#     images_path = os.path.join(test_images_path, image_folder)\n",
    "#     # Check if the path is a directory\n",
    "#     if not os.path.isdir(images_path):\n",
    "#         continue\n",
    "#     for image in os.listdir(images_path):\n",
    "#         path_to_coin_image = os.path.join(images_path, image)\n",
    "#         images_dict[image_folder].append(path_to_coin_image)\n",
    "# num_images = len(image_name_list)\n",
    "# print(\"total {} test images\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "# import datetime\n",
    "# !rm -rf ./logs/\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.image as mpimg\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "This section imports the necessary libraries for image processing and augmentation `ImageDataGenerator`and related functions from `tensorflow.keras.preprocessing.image` are used for loading and augmenting images. The `os` module handles directory operation, and `tqdm` provides a progress bar for the augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ImageDataGenerator\n",
    "\n",
    "An instance of `ImageDataGenerator`is created with specific augmentation parameters. The purpose of this data augmentation is to artificially expand the training dataset by generating new, altered versions of the existing coin images. This is particularly beneficial since our original training dataset is small, so augmentation will help to prevent overfitting and improve the model's ability to generalize to new data. \n",
    "\n",
    "For this task, the chosen augmentations are Rescaling, Rotation, Brightness Adjustment and Zoom. Rescaling of pixel values from [0,255] range to [0,1] is done to facilitate faster convergence during training. We apply a random rotation of up to 90 degrees as well, this is to make the model robust to changes in orientation, improving its ability to recognize coins regardless of rotation. Since lighting conditions vary slightly in the training data a brightness adjustment is also implemented. Finally a slight zoom in is implemented since the segmentation algorithm can cut of some edges of coins and therefore preparing the model for that in the training data makes generalize better to the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create the directories to save the augmented images\n",
    "# augmented_data_path = 'path_to_save_augmented_images'\n",
    "# validation_data_path = 'path_to_save_validation_images'\n",
    "# os.makedirs(augmented_data_path, exist_ok=True)\n",
    "# os.makedirs(validation_data_path, exist_ok=True)\n",
    "\n",
    "# crazy_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=90,\n",
    "#     horizontal_flip=True,\n",
    "#     zoom_range=[0.8, 1.2],\n",
    "#     brightness_range=[0.5, 1.5],\n",
    "#     shear_range=0.2,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment and Save Images\n",
    "This loop iterates over each class directory and each image within that directory. Each image is loaded and converted to an array. The `datagen.flow` function generates augmented images which are saved to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# # Set the number of augmented images to generate per original image\n",
    "# num_augmented_images = 3\n",
    "\n",
    "# # Load the original training images\n",
    "# original_data_path = images_classes_path\n",
    "\n",
    "# # Calculate the total number of steps for the progress bar\n",
    "# all_files = sum([len(files) for r, d, files in os.walk(original_data_path)])\n",
    "# total_steps = int(all_files * 0.7 * num_augmented_images + all_files * 0.3)\n",
    "\n",
    "# # Initialize the progress bar\n",
    "# pbar = tqdm(total=total_steps, desc=\"Processing images\")\n",
    "\n",
    "# # Split the original images into 70% for augmentation and 30% for validation\n",
    "# for class_dir in os.listdir(original_data_path):\n",
    "#     class_path = os.path.join(original_data_path, class_dir)\n",
    "#     augmented_class_path = os.path.join(augmented_data_path, class_dir)\n",
    "#     validation_class_path = os.path.join(validation_data_path, class_dir)\n",
    "#     os.makedirs(augmented_class_path, exist_ok=True)\n",
    "#     os.makedirs(validation_class_path, exist_ok=True)\n",
    "    \n",
    "#     images = os.listdir(class_path)\n",
    "#     random.shuffle(images)\n",
    "#     split_index = int(len(images) * 0.7)\n",
    "    \n",
    "#     images_to_augment = images[:split_index]\n",
    "#     images_for_validation = images[split_index:]\n",
    "    \n",
    "#     # Save validation images to the validation data path without augmentation\n",
    "#     for img_name in images_for_validation:\n",
    "#         src_path = os.path.join(class_path, img_name)\n",
    "#         dst_path = os.path.join(validation_class_path, img_name)\n",
    "#         shutil.copy(src_path, dst_path)\n",
    "#         pbar.update(1)\n",
    "\n",
    "#     # Augment 70% of the images\n",
    "#     for img_name in images_to_augment:\n",
    "#         img_path = os.path.join(class_path, img_name)\n",
    "#         img = load_img(img_path)\n",
    "#         x = img_to_array(img)\n",
    "#         x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "#         # Generate and save augmented images\n",
    "#         i = 0\n",
    "#         for batch in crazy_datagen.flow(x, batch_size=1, save_to_dir=augmented_class_path, save_prefix='aug', save_format='jpeg'):\n",
    "#             i += 1\n",
    "#             pbar.update(1)\n",
    "#             if i >= num_augmented_images:\n",
    "#                 break\n",
    "\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data generators for Training and validation\n",
    "Two data generators are defined: one for training and one for validation. The images are rescaled, and the dataset is split into training and validation subsets. The images are resized to 224x224 picels, and a batch size of 32 is used. Different batch sizes were tested, but 32 was found to be a suitable batch size because of the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the data generators for training and validation\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255\n",
    "# )\n",
    "\n",
    "# val_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255\n",
    "# )\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     augmented_data_path,\n",
    "#     target_size=(224, 224),\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_generator = val_datagen.flow_from_directory(\n",
    "#     validation_data_path,\n",
    "#     target_size=(224, 224),\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # Test data generator remains the same\n",
    "# # test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# # test_generator = test_datagen.flow_from_directory(\n",
    "# #     test_images_path,\n",
    "# #     target_size=(224, 224),\n",
    "# #     shuffle=False,\n",
    "# #     batch_size=1,\n",
    "# #     class_mode='categorical'\n",
    "# # )\n",
    "\n",
    "# # Get filenames and number of samples\n",
    "# # filenames = test_generator.filenames\n",
    "# # nb_samples = len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a batch for verification\n",
    "# x_batch, y_batch = next(train_generator)\n",
    "# print(f\"Batch x_shape: {x_batch.shape}, Batch y_shape: {y_batch.shape}\")\n",
    "\n",
    "# # Verify the data generators\n",
    "# train_batch = next(iter(train_generator))\n",
    "# val_batch = next(iter(val_generator))\n",
    "# print(train_batch[0].shape, train_batch[1].shape)\n",
    "# print(val_batch[0].shape, val_batch[1].shape)\n",
    "\n",
    "# print(f\"Steps per epoch: {train_generator.samples // train_generator.batch_size}\")\n",
    "# print(f\"Validation steps: {val_generator.samples // val_generator.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration using Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:41<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 2.4404250144958497, Training Accuracy: 20.58823529411765%\n",
      "Epoch 1/50, Validation Loss: 1.9240372975667317, Validation Accuracy: 36.36363636363637%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Training Loss: 0.9178635716438294, Training Accuracy: 86.60130718954248%\n",
      "Epoch 2/50, Validation Loss: 1.1867733200391133, Validation Accuracy: 66.23376623376623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Training Loss: 0.39772829711437224, Training Accuracy: 98.69281045751634%\n",
      "Epoch 3/50, Validation Loss: 0.9397323330243429, Validation Accuracy: 71.42857142857143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Training Loss: 0.19249872714281083, Training Accuracy: 99.67320261437908%\n",
      "Epoch 4/50, Validation Loss: 0.7893547217051188, Validation Accuracy: 77.92207792207792%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Training Loss: 0.09853102043271064, Training Accuracy: 100.0%\n",
      "Epoch 5/50, Validation Loss: 0.7161141037940979, Validation Accuracy: 81.81818181818181%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Training Loss: 0.06033523604273796, Training Accuracy: 100.0%\n",
      "Epoch 6/50, Validation Loss: 0.689931849638621, Validation Accuracy: 80.51948051948052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Training Loss: 0.04575889818370342, Training Accuracy: 100.0%\n",
      "Epoch 7/50, Validation Loss: 0.6561654309431711, Validation Accuracy: 83.11688311688312%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:51<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Training Loss: 0.03555665910243988, Training Accuracy: 100.0%\n",
      "Epoch 8/50, Validation Loss: 0.6228011647860209, Validation Accuracy: 83.11688311688312%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Training Loss: 0.025934697687625886, Training Accuracy: 100.0%\n",
      "Epoch 9/50, Validation Loss: 0.6385479966799418, Validation Accuracy: 81.81818181818181%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:47<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Training Loss: 0.023324559535831214, Training Accuracy: 100.0%\n",
      "Epoch 10/50, Validation Loss: 0.6310490369796753, Validation Accuracy: 81.81818181818181%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Training Loss: 0.01748245470225811, Training Accuracy: 100.0%\n",
      "Epoch 11/50, Validation Loss: 0.61689559618632, Validation Accuracy: 81.81818181818181%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Training Loss: 0.015584654547274113, Training Accuracy: 100.0%\n",
      "Epoch 12/50, Validation Loss: 0.6099522411823273, Validation Accuracy: 81.81818181818181%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Training Loss: 0.017224364355206488, Training Accuracy: 100.0%\n",
      "Epoch 13/50, Validation Loss: 0.6023083925247192, Validation Accuracy: 83.11688311688312%\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model_epoch_13.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 156\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Load the best model (last saved model)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 156\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Evaluation on test data\u001b[39;00m\n\u001b[0;32m    159\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\katla\\miniconda3\\envs\\iapr\\lib\\site-packages\\torch\\serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    577\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\katla\\miniconda3\\envs\\iapr\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\katla\\miniconda3\\envs\\iapr\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_epoch_13.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "test_images_path = \"output\"\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(os.listdir(images_classes_path))}\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for label in os.listdir(images_classes_path):\n",
    "    class_path = os.path.join(images_classes_path, label)\n",
    "    for img in os.listdir(class_path):\n",
    "        train_data.append(os.path.join(class_path, img))\n",
    "        train_labels.append(class_to_idx[label])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(train_data, train_labels, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "for folder in os.listdir(test_images_path):\n",
    "    folder_path = os.path.join(test_images_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img in os.listdir(folder_path):\n",
    "            test_data.append(os.path.join(folder_path, img))\n",
    "\n",
    "test_dataset = CustomTestDataset(test_data, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_to_idx))  # Number of classes based on the training data\n",
    "\n",
    "# Training settings\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop with validation and early stopping\n",
    "num_epochs = 50\n",
    "patience = 6\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Training Accuracy: {train_accuracy}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy}%\")\n",
    "\n",
    "    # Early stopping and model checkpoint\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000000.JPG\\1912_1511_2592_2182.jpg\n",
      "ospath 1912_1511_2592_2182\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000000.JPG\\2774_2155_3329_2683.jpg\n",
      "ospath 2774_2155_3329_2683\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000000.JPG\\3337_2610_3858_3116.jpg\n",
      "ospath 3337_2610_3858_3116\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000001.JPG\\1217_1568_1743_2081.jpg\n",
      "ospath 1217_1568_1743_2081\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000001.JPG\\1679_2380_2254_2917.jpg\n",
      "ospath 1679_2380_2254_2917\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000001.JPG\\1858_1176_2540_1838.jpg\n",
      "ospath 1858_1176_2540_1838\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000001.JPG\\2448_3085_2897_3529.jpg\n",
      "ospath 2448_3085_2897_3529\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000001.JPG\\2617_2338_3051_2754.jpg\n",
      "ospath 2617_2338_3051_2754\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000001.JPG\\2802_1567_3399_2145.jpg\n",
      "ospath 2802_1567_3399_2145\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000001.JPG\\2972_2876_3507_3392.jpg\n",
      "ospath 2972_2876_3507_3392\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000001.JPG\\3473_1196_4054_1760.jpg\n",
      "ospath 3473_1196_4054_1760\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000001.JPG\\3537_2432_4121_2983.jpg\n",
      "ospath 3537_2432_4121_2983\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000001.JPG\\3969_1835_4501_2342.jpg\n",
      "ospath 3969_1835_4501_2342\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000002.JPG\\2277_606_3045_1366.jpg\n",
      "ospath 2277_606_3045_1366\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000002.JPG\\2323_1468_3085_2219.jpg\n",
      "ospath 2323_1468_3085_2219\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000002.JPG\\3049_692_3616_1245.jpg\n",
      "ospath 3049_692_3616_1245\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000002.JPG\\3115_932_3711_1615.jpg\n",
      "ospath 3115_932_3711_1615\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000002.JPG\\3231_1247_3722_1716.jpg\n",
      "ospath 3231_1247_3722_1716\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000003.JPG\\2117_1771_2594_2232.jpg\n",
      "ospath 2117_1771_2594_2232\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000003.JPG\\2383_1174_2856_1640.jpg\n",
      "ospath 2383_1174_2856_1640\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000003.JPG\\2421_2594_3065_3222.jpg\n",
      "ospath 2421_2594_3065_3222\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000003.JPG\\2698_2172_3142_2606.jpg\n",
      "ospath 2698_2172_3142_2606\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000003.JPG\\2751_1404_3409_2049.jpg\n",
      "ospath 2751_1404_3409_2049\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000003.JPG\\3101_801_3603_1282.jpg\n",
      "ospath 3101_801_3603_1282\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000003.JPG\\3538_1882_3967_2302.jpg\n",
      "ospath 3538_1882_3967_2302\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000003.JPG\\3596_1101_4048_1535.jpg\n",
      "ospath 3596_1101_4048_1535\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000004.JPG\\1097_2663_1669_3226.jpg\n",
      "ospath 1097_2663_1669_3226\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000004.JPG\\1212_1844_1693_2304.jpg\n",
      "ospath 1212_1844_1693_2304\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000004.JPG\\2180_1109_2764_1676.jpg\n",
      "ospath 2180_1109_2764_1676\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000004.JPG\\3228_2204_3674_2644.jpg\n",
      "ospath 3228_2204_3674_2644\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000004.JPG\\3271_1117_3757_1578.jpg\n",
      "ospath 3271_1117_3757_1578\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000004.JPG\\858_1040_1389_1558.jpg\n",
      "ospath 858_1040_1389_1558\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000005.JPG\\1004_1788_1491_2261.jpg\n",
      "ospath 1004_1788_1491_2261\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000005.JPG\\1097_984_1632_1498.jpg\n",
      "ospath 1097_984_1632_1498\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000005.JPG\\1209_2546_1780_3104.jpg\n",
      "ospath 1209_2546_1780_3104\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000005.JPG\\1709_630_2488_1387.jpg\n",
      "ospath 1709_630_2488_1387\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000005.JPG\\1926_1459_2451_1977.jpg\n",
      "ospath 1926_1459_2451_1977\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000005.JPG\\2325_2151_2803_2609.jpg\n",
      "ospath 2325_2151_2803_2609\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000005.JPG\\3147_795_3690_1311.jpg\n",
      "ospath 3147_795_3690_1311\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000005.JPG\\3230_1803_3692_2248.jpg\n",
      "ospath 3230_1803_3692_2248\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000006.JPG\\1304_2827_1785_3296.jpg\n",
      "ospath 1304_2827_1785_3296\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000006.JPG\\1439_2099_1922_2563.jpg\n",
      "ospath 1439_2099_1922_2563\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000006.JPG\\2305_1728_2903_2299.jpg\n",
      "ospath 2305_1728_2903_2299\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000006.JPG\\2372_2394_3041_3059.jpg\n",
      "ospath 2372_2394_3041_3059\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000006.JPG\\2912_1004_3584_1656.jpg\n",
      "ospath 2912_1004_3584_1656\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000006.JPG\\3121_1780_3651_2300.jpg\n",
      "ospath 3121_1780_3651_2300\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000006.JPG\\3266_2500_3837_3051.jpg\n",
      "ospath 3266_2500_3837_3051\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000006.JPG\\3513_561_4301_1325.jpg\n",
      "ospath 3513_561_4301_1325\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000007.JPG\\1776_1551_2200_1975.jpg\n",
      "ospath 1776_1551_2200_1975\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000007.JPG\\1927_2454_2454_2972.jpg\n",
      "ospath 1927_2454_2454_2972\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000007.JPG\\2268_1791_2745_2263.jpg\n",
      "ospath 2268_1791_2745_2263\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000007.JPG\\2525_2869_3024_3350.jpg\n",
      "ospath 2525_2869_3024_3350\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000007.JPG\\2709_2189_3381_2847.jpg\n",
      "ospath 2709_2189_3381_2847\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000007.JPG\\2904_1523_3478_2082.jpg\n",
      "ospath 2904_1523_3478_2082\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000008.JPG\\1469_2078_2053_2633.jpg\n",
      "ospath 1469_2078_2053_2633\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000008.JPG\\2676_1379_3318_2003.jpg\n",
      "ospath 2676_1379_3318_2003\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000008.JPG\\2989_2443_3661_3093.jpg\n",
      "ospath 2989_2443_3661_3093\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000008.JPG\\4171_1459_4633_1914.jpg\n",
      "ospath 4171_1459_4633_1914\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000009.JPG\\2629_1934_3137_2443.jpg\n",
      "ospath 2629_1934_3137_2443\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000009.JPG\\2705_1427_3264_1971.jpg\n",
      "ospath 2705_1427_3264_1971\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000009.JPG\\2732_1945_3507_2545.jpg\n",
      "ospath 2732_1945_3507_2545\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000009.JPG\\3094_1978_3736_2606.jpg\n",
      "ospath 3094_1978_3736_2606\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000009.JPG\\3308_1221_3893_1786.jpg\n",
      "ospath 3308_1221_3893_1786\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000010.JPG\\1380_1663_1902_2173.jpg\n",
      "ospath 1380_1663_1902_2173\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000010.JPG\\1802_2382_2436_3003.jpg\n",
      "ospath 1802_2382_2436_3003\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000010.JPG\\2205_1515_2685_1975.jpg\n",
      "ospath 2205_1515_2685_1975\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000010.JPG\\2735_2381_3235_2851.jpg\n",
      "ospath 2735_2381_3235_2851\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000010.JPG\\3183_1541_3826_2155.jpg\n",
      "ospath 3183_1541_3826_2155\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000011.JPG\\1181_1746_1818_2369.jpg\n",
      "ospath 1181_1746_1818_2369\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000011.JPG\\1376_2681_2019_3313.jpg\n",
      "ospath 1376_2681_2019_3313\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000011.JPG\\1804_1904_2574_2661.jpg\n",
      "ospath 1804_1904_2574_2661\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000011.JPG\\2337_2622_2837_3099.jpg\n",
      "ospath 2337_2622_2837_3099\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000011.JPG\\2545_1205_3219_1850.jpg\n",
      "ospath 2545_1205_3219_1850\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000011.JPG\\3346_3026_3808_3472.jpg\n",
      "ospath 3346_3026_3808_3472\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000011.JPG\\3621_2335_4068_2774.jpg\n",
      "ospath 3621_2335_4068_2774\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000011.JPG\\3907_1674_4438_2184.jpg\n",
      "ospath 3907_1674_4438_2184\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000012.JPG\\1500_2549_2026_3057.jpg\n",
      "ospath 1500_2549_2026_3057\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000012.JPG\\1931_1429_2413_1889.jpg\n",
      "ospath 1931_1429_2413_1889\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000012.JPG\\2216_3274_2747_3794.jpg\n",
      "ospath 2216_3274_2747_3794\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000012.JPG\\3595_1570_4195_2146.jpg\n",
      "ospath 3595_1570_4195_2146\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000012.JPG\\3595_2817_4192_3392.jpg\n",
      "ospath 3595_2817_4192_3392\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000012.JPG\\921_3237_1413_3719.jpg\n",
      "ospath 921_3237_1413_3719\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000013.JPG\\2002_1512_2569_2065.jpg\n",
      "ospath 2002_1512_2569_2065\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000013.JPG\\2382_2159_2920_2675.jpg\n",
      "ospath 2382_2159_2920_2675\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000013.JPG\\2757_1341_3235_1804.jpg\n",
      "ospath 2757_1341_3235_1804\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000013.JPG\\3133_2322_3541_2720.jpg\n",
      "ospath 3133_2322_3541_2720\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000013.JPG\\3428_1544_3962_2054.jpg\n",
      "ospath 3428_1544_3962_2054\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000014.JPG\\1897_2987_2380_3453.jpg\n",
      "ospath 1897_2987_2380_3453\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000014.JPG\\1936_1802_2379_2241.jpg\n",
      "ospath 1936_1802_2379_2241\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000014.JPG\\2721_2852_3363_3473.jpg\n",
      "ospath 2721_2852_3363_3473\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000014.JPG\\3459_1459_3990_1965.jpg\n",
      "ospath 3459_1459_3990_1965\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000014.JPG\\4148_2455_4832_3109.jpg\n",
      "ospath 4148_2455_4832_3109\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000015.JPG\\1566_2791_2060_3272.jpg\n",
      "ospath 1566_2791_2060_3272\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000015.JPG\\2249_1755_2777_2267.jpg\n",
      "ospath 2249_1755_2777_2267\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000015.JPG\\2251_2485_2780_3000.jpg\n",
      "ospath 2251_2485_2780_3000\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000015.JPG\\2722_3009_3146_3426.jpg\n",
      "ospath 2722_3009_3146_3426\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000015.JPG\\2752_1888_3253_2373.jpg\n",
      "ospath 2752_1888_3253_2373\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000016.JPG\\2158_1686_2565_2078.jpg\n",
      "ospath 2158_1686_2565_2078\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000016.JPG\\2446_2045_2910_2489.jpg\n",
      "ospath 2446_2045_2910_2489\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000016.JPG\\2515_1072_2981_1516.jpg\n",
      "ospath 2515_1072_2981_1516\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000016.JPG\\2880_2089_3332_2525.jpg\n",
      "ospath 2880_2089_3332_2525\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000016.JPG\\2961_1013_3523_1565.jpg\n",
      "ospath 2961_1013_3523_1565\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000016.JPG\\3216_1550_3750_2059.jpg\n",
      "ospath 3216_1550_3750_2059\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000017.JPG\\1487_1653_2062_2208.jpg\n",
      "ospath 1487_1653_2062_2208\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000017.JPG\\2632_2468_3400_3222.jpg\n",
      "ospath 2632_2468_3400_3222\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000017.JPG\\2827_1582_3399_2139.jpg\n",
      "ospath 2827_1582_3399_2139\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000017.JPG\\2841_684_3275_1100.jpg\n",
      "ospath 2841_684_3275_1100\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000017.JPG\\3356_1077_3893_1588.jpg\n",
      "ospath 3356_1077_3893_1588\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000017.JPG\\4301_2566_4862_3096.jpg\n",
      "ospath 4301_2566_4862_3096\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000017.JPG\\4653_934_5142_1398.jpg\n",
      "ospath 4653_934_5142_1398\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000018.JPG\\2679_887_3177_1365.jpg\n",
      "ospath 2679_887_3177_1365\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000018.JPG\\2691_1580_3169_2035.jpg\n",
      "ospath 2691_1580_3169_2035\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000018.JPG\\3153_551_3744_1127.jpg\n",
      "ospath 3153_551_3744_1127\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000018.JPG\\3339_1745_3768_2172.jpg\n",
      "ospath 3339_1745_3768_2172\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000018.JPG\\3446_1139_3894_1588.jpg\n",
      "ospath 3446_1139_3894_1588\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000019.JPG\\2366_867_3037_1516.jpg\n",
      "ospath 2366_867_3037_1516\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000019.JPG\\2982_1241_3753_1995.jpg\n",
      "ospath 2982_1241_3753_1995\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000019.JPG\\3095_500_3653_1035.jpg\n",
      "ospath 3095_500_3653_1035\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000020.JPG\\1702_1461_2227_1966.jpg\n",
      "ospath 1702_1461_2227_1966\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000020.JPG\\1818_2593_2391_3156.jpg\n",
      "ospath 1818_2593_2391_3156\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000020.JPG\\2892_1326_3511_1931.jpg\n",
      "ospath 2892_1326_3511_1931\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000020.JPG\\3150_2420_3627_2876.jpg\n",
      "ospath 3150_2420_3627_2876\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000020.JPG\\3938_2191_4473_2700.jpg\n",
      "ospath 3938_2191_4473_2700\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000020.JPG\\4800_1660_5405_2246.jpg\n",
      "ospath 4800_1660_5405_2246\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000021.JPG\\2097_1701_2644_2217.jpg\n",
      "ospath 2097_1701_2644_2217\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000021.JPG\\2713_2129_3325_2713.jpg\n",
      "ospath 2713_2129_3325_2713\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000021.JPG\\2847_1002_3537_1671.jpg\n",
      "ospath 2847_1002_3537_1671\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000022.JPG\\2261_1782_2888_2405.jpg\n",
      "ospath 2261_1782_2888_2405\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000022.JPG\\2285_960_2918_1592.jpg\n",
      "ospath 2285_960_2918_1592\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000022.JPG\\2901_1421_3573_2086.jpg\n",
      "ospath 2901_1421_3573_2086\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000022.JPG\\3313_612_3851_1121.jpg\n",
      "ospath 3313_612_3851_1121\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000022.JPG\\3334_744_3839_1384.jpg\n",
      "ospath 3334_744_3839_1384\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000022.JPG\\3391_1120_3795_1520.jpg\n",
      "ospath 3391_1120_3795_1520\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000022.JPG\\3429_1887_3984_2428.jpg\n",
      "ospath 3429_1887_3984_2428\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000023.JPG\\2080_1408_2856_2158.jpg\n",
      "ospath 2080_1408_2856_2158\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000023.JPG\\2347_2680_3015_3342.jpg\n",
      "ospath 2347_2680_3015_3342\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000023.JPG\\2892_1630_3530_2257.jpg\n",
      "ospath 2892_1630_3530_2257\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000023.JPG\\3008_2554_3462_2992.jpg\n",
      "ospath 3008_2554_3462_2992\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000023.JPG\\3181_982_3748_1524.jpg\n",
      "ospath 3181_982_3748_1524\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000023.JPG\\3498_2249_4049_2784.jpg\n",
      "ospath 3498_2249_4049_2784\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000023.JPG\\3930_2742_4375_3176.jpg\n",
      "ospath 3930_2742_4375_3176\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000023.JPG\\4189_1235_4733_1754.jpg\n",
      "ospath 4189_1235_4733_1754\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000023.JPG\\4475_2018_4983_2501.jpg\n",
      "ospath 4475_2018_4983_2501\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000023.JPG\\5003_2432_5579_2985.jpg\n",
      "ospath 5003_2432_5579_2985\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000023.JPG\\5101_1403_5597_1867.jpg\n",
      "ospath 5101_1403_5597_1867\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000024.JPG\\2616_1656_3209_2233.jpg\n",
      "ospath 2616_1656_3209_2233\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000024.JPG\\2652_835_3179_1343.jpg\n",
      "ospath 2652_835_3179_1343\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000024.JPG\\3315_194_3740_614.jpg\n",
      "ospath 3315_194_3740_614\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000024.JPG\\3443_918_4023_1459.jpg\n",
      "ospath 3443_918_4023_1459\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000024.JPG\\3531_1848_4097_2379.jpg\n",
      "ospath 3531_1848_4097_2379\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000025.JPG\\2556_1845_3221_2513.jpg\n",
      "ospath 2556_1845_3221_2513\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000025.JPG\\3045_1350_3659_1960.jpg\n",
      "ospath 3045_1350_3659_1960\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000025.JPG\\3212_2177_3898_2867.jpg\n",
      "ospath 3212_2177_3898_2867\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000026.JPG\\1315_2241_1883_2790.jpg\n",
      "ospath 1315_2241_1883_2790\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000026.JPG\\1630_1677_2192_2216.jpg\n",
      "ospath 1630_1677_2192_2216\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000026.JPG\\2107_2728_2681_3288.jpg\n",
      "ospath 2107_2728_2681_3288\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000026.JPG\\2369_1942_2897_2460.jpg\n",
      "ospath 2369_1942_2897_2460\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000026.JPG\\2561_1311_3071_1787.jpg\n",
      "ospath 2561_1311_3071_1787\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000026.JPG\\2907_2564_3465_3104.jpg\n",
      "ospath 2907_2564_3465_3104\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000027.JPG\\1931_1664_2520_2247.jpg\n",
      "ospath 1931_1664_2520_2247\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000027.JPG\\2315_899_2937_1510.jpg\n",
      "ospath 2315_899_2937_1510\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000027.JPG\\2668_2371_3343_3031.jpg\n",
      "ospath 2668_2371_3343_3031\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000027.JPG\\3145_1519_3686_2029.jpg\n",
      "ospath 3145_1519_3686_2029\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000027.JPG\\3633_391_4207_940.jpg\n",
      "ospath 3633_391_4207_940\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000027.JPG\\4085_1580_4846_2339.jpg\n",
      "ospath 4085_1580_4846_2339\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000028.JPG\\2593_2452_3024_2872.jpg\n",
      "ospath 2593_2452_3024_2872\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000029.JPG\\2021_1237_2526_1727.jpg\n",
      "ospath 2021_1237_2526_1727\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000029.JPG\\2364_2226_3163_3028.jpg\n",
      "ospath 2364_2226_3163_3028\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000029.JPG\\2787_1288_3362_1856.jpg\n",
      "ospath 2787_1288_3362_1856\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000029.JPG\\3344_2673_3926_3245.jpg\n",
      "ospath 3344_2673_3926_3245\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000029.JPG\\3396_2137_3884_2577.jpg\n",
      "ospath 3396_2137_3884_2577\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000030.JPG\\1092_1555_1669_2116.jpg\n",
      "ospath 1092_1555_1669_2116\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000030.JPG\\1397_2233_1878_2686.jpg\n",
      "ospath 1397_2233_1878_2686\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000030.JPG\\1791_419_2222_834.jpg\n",
      "ospath 1791_419_2222_834\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000030.JPG\\1967_1620_2734_2373.jpg\n",
      "ospath 1967_1620_2734_2373\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000030.JPG\\2194_999_2642_1448.jpg\n",
      "ospath 2194_999_2642_1448\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000030.JPG\\2604_446_3143_965.jpg\n",
      "ospath 2604_446_3143_965\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000030.JPG\\3148_1610_3630_2066.jpg\n",
      "ospath 3148_1610_3630_2066\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000030.JPG\\924_841_1457_1359.jpg\n",
      "ospath 924_841_1457_1359\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000031.JPG\\2855_1957_3421_2517.jpg\n",
      "ospath 2855_1957_3421_2517\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000032.JPG\\2778_1721_3389_2316.jpg\n",
      "ospath 2778_1721_3389_2316\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000032.JPG\\3230_2257_4050_3069.jpg\n",
      "ospath 3230_2257_4050_3069\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000032.JPG\\3347_1640_3847_2126.jpg\n",
      "ospath 3347_1640_3847_2126\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000033.JPG\\1474_777_2015_1294.jpg\n",
      "ospath 1474_777_2015_1294\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000033.JPG\\1955_1988_2583_2601.jpg\n",
      "ospath 1955_1988_2583_2601\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000033.JPG\\2285_716_2821_1226.jpg\n",
      "ospath 2285_716_2821_1226\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000033.JPG\\2764_1563_3202_1987.jpg\n",
      "ospath 2764_1563_3202_1987\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000033.JPG\\3332_1674_3809_2137.jpg\n",
      "ospath 3332_1674_3809_2137\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000033.JPG\\4238_2157_4911_2800.jpg\n",
      "ospath 4238_2157_4911_2800\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000033.JPG\\4330_494_4808_960.jpg\n",
      "ospath 4330_494_4808_960\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000033.JPG\\4527_1124_5071_1650.jpg\n",
      "ospath 4527_1124_5071_1650\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000033.JPG\\711_2433_1241_2950.jpg\n",
      "ospath 711_2433_1241_2950\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000034.JPG\\1874_2049_2505_2672.jpg\n",
      "ospath 1874_2049_2505_2672\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000034.JPG\\2233_1525_2727_2004.jpg\n",
      "ospath 2233_1525_2727_2004\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000034.JPG\\2292_2654_2865_3197.jpg\n",
      "ospath 2292_2654_2865_3197\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000034.JPG\\2670_2000_3389_2695.jpg\n",
      "ospath 2670_2000_3389_2695\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000035.JPG\\1474_2192_2035_2725.jpg\n",
      "ospath 1474_2192_2035_2725\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000035.JPG\\1766_1275_2333_1815.jpg\n",
      "ospath 1766_1275_2333_1815\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000035.JPG\\2403_2210_3002_2785.jpg\n",
      "ospath 2403_2210_3002_2785\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000035.JPG\\2807_1145_3457_1768.jpg\n",
      "ospath 2807_1145_3457_1768\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000036.JPG\\2141_1863_2740_2432.jpg\n",
      "ospath 2141_1863_2740_2432\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000036.JPG\\2616_2256_3230_2851.jpg\n",
      "ospath 2616_2256_3230_2851\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000036.JPG\\2686_1466_3334_2098.jpg\n",
      "ospath 2686_1466_3334_2098\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000037.JPG\\1503_2245_2114_2837.jpg\n",
      "ospath 1503_2245_2114_2837\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000037.JPG\\2110_1409_2695_1978.jpg\n",
      "ospath 2110_1409_2695_1978\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000037.JPG\\2357_2391_2784_2808.jpg\n",
      "ospath 2357_2391_2784_2808\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000037.JPG\\2433_3128_3061_3746.jpg\n",
      "ospath 2433_3128_3061_3746\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000037.JPG\\3141_1529_3619_1994.jpg\n",
      "ospath 3141_1529_3619_1994\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000038.JPG\\1675_1982_2171_2454.jpg\n",
      "ospath 1675_1982_2171_2454\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000038.JPG\\1904_2910_2397_3389.jpg\n",
      "ospath 1904_2910_2397_3389\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000038.JPG\\2516_2019_3295_2775.jpg\n",
      "ospath 2516_2019_3295_2775\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000039.JPG\\2556_1085_3173_1689.jpg\n",
      "ospath 2556_1085_3173_1689\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000039.JPG\\3011_656_3607_1242.jpg\n",
      "ospath 3011_656_3607_1242\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000039.JPG\\3021_1652_3662_2282.jpg\n",
      "ospath 3021_1652_3662_2282\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000039.JPG\\3494_1250_4074_1805.jpg\n",
      "ospath 3494_1250_4074_1805\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000040.JPG\\2501_1149_2957_1608.jpg\n",
      "ospath 2501_1149_2957_1608\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000040.JPG\\3091_1451_3859_2196.jpg\n",
      "ospath 3091_1451_3859_2196\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000040.JPG\\3253_710_3784_1218.jpg\n",
      "ospath 3253_710_3784_1218\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000041.JPG\\1408_1807_1884_2276.jpg\n",
      "ospath 1408_1807_1884_2276\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000041.JPG\\1556_2478_2036_2935.jpg\n",
      "ospath 1556_2478_2036_2935\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000041.JPG\\2149_2929_2558_3330.jpg\n",
      "ospath 2149_2929_2558_3330\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000041.JPG\\2161_1469_2794_2094.jpg\n",
      "ospath 2161_1469_2794_2094\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000041.JPG\\2502_2527_2983_2992.jpg\n",
      "ospath 2502_2527_2983_2992\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000041.JPG\\3149_1285_3591_1721.jpg\n",
      "ospath 3149_1285_3591_1721\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000041.JPG\\3245_2257_3710_2696.jpg\n",
      "ospath 3245_2257_3710_2696\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000042.JPG\\1293_888_1933_1506.jpg\n",
      "ospath 1293_888_1933_1506\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000042.JPG\\1782_1858_2259_2319.jpg\n",
      "ospath 1782_1858_2259_2319\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000042.JPG\\3066_1746_3604_2258.jpg\n",
      "ospath 3066_1746_3604_2258\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000042.JPG\\3385_2707_3890_3176.jpg\n",
      "ospath 3385_2707_3890_3176\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000042.JPG\\3629_986_4173_1500.jpg\n",
      "ospath 3629_986_4173_1500\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000042.JPG\\3900_1700_4402_2174.jpg\n",
      "ospath 3900_1700_4402_2174\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000043.JPG\\1861_2659_2494_3282.jpg\n",
      "ospath 1861_2659_2494_3282\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000043.JPG\\2105_1875_2661_2406.jpg\n",
      "ospath 2105_1875_2661_2406\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000043.JPG\\2595_2626_3073_3087.jpg\n",
      "ospath 2595_2626_3073_3087\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000043.JPG\\2858_1749_3387_2264.jpg\n",
      "ospath 2858_1749_3387_2264\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000044.JPG\\1941_2127_2375_2570.jpg\n",
      "ospath 1941_2127_2375_2570\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000044.JPG\\2230_1256_2775_1783.jpg\n",
      "ospath 2230_1256_2775_1783\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000044.JPG\\2452_2205_3024_2771.jpg\n",
      "ospath 2452_2205_3024_2771\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000045.JPG\\1459_1821_1904_2264.jpg\n",
      "ospath 1459_1821_1904_2264\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000045.JPG\\1716_307_2199_779.jpg\n",
      "ospath 1716_307_2199_779\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000045.JPG\\2427_644_2923_1131.jpg\n",
      "ospath 2427_644_2923_1131\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000045.JPG\\3057_1393_3557_1873.jpg\n",
      "ospath 3057_1393_3557_1873\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000045.JPG\\3375_1903_3804_2331.jpg\n",
      "ospath 3375_1903_3804_2331\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000045.JPG\\3398_73_3968_643.jpg\n",
      "ospath 3398_73_3968_643\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000045.JPG\\3681_972_4170_1447.jpg\n",
      "ospath 3681_972_4170_1447\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000045.JPG\\4591_0_5206_382.jpg\n",
      "ospath 4591_0_5206_382\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000045.JPG\\4664_788_5294_1393.jpg\n",
      "ospath 4664_788_5294_1393\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000046.JPG\\1812_1021_2342_1538.jpg\n",
      "ospath 1812_1021_2342_1538\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000046.JPG\\1826_2206_2600_2964.jpg\n",
      "ospath 1826_2206_2600_2964\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000046.JPG\\2622_1658_3025_2051.jpg\n",
      "ospath 2622_1658_3025_2051\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000046.JPG\\3026_2512_3594_3069.jpg\n",
      "ospath 3026_2512_3594_3069\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000046.JPG\\3382_1149_4055_1799.jpg\n",
      "ospath 3382_1149_4055_1799\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000047.JPG\\2407_1813_3040_2443.jpg\n",
      "ospath 2407_1813_3040_2443\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000047.JPG\\2674_1178_3249_1754.jpg\n",
      "ospath 2674_1178_3249_1754\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000047.JPG\\2979_2195_3506_2696.jpg\n",
      "ospath 2979_2195_3506_2696\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000047.JPG\\3126_1554_3833_2265.jpg\n",
      "ospath 3126_1554_3833_2265\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000047.JPG\\3316_903_3806_1370.jpg\n",
      "ospath 3316_903_3806_1370\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000048.JPG\\1098_2640_1598_3120.jpg\n",
      "ospath 1098_2640_1598_3120\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000048.JPG\\1798_1917_2312_2411.jpg\n",
      "ospath 1798_1917_2312_2411\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000048.JPG\\2573_703_3355_1464.jpg\n",
      "ospath 2573_703_3355_1464\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000048.JPG\\2959_2726_3481_3237.jpg\n",
      "ospath 2959_2726_3481_3237\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000048.JPG\\3225_1820_3768_2338.jpg\n",
      "ospath 3225_1820_3768_2338\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000048.JPG\\769_1330_1442_1986.jpg\n",
      "ospath 769_1330_1442_1986\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000049.JPG\\1297_1857_1857_2386.jpg\n",
      "ospath 1297_1857_1857_2386\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000049.JPG\\1895_2247_2389_2727.jpg\n",
      "ospath 1895_2247_2389_2727\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000049.JPG\\1930_1205_2605_1859.jpg\n",
      "ospath 1930_1205_2605_1859\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000049.JPG\\2438_2753_3078_3393.jpg\n",
      "ospath 2438_2753_3078_3393\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000049.JPG\\2517_2118_3080_2655.jpg\n",
      "ospath 2517_2118_3080_2655\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000049.JPG\\2775_1088_3365_1661.jpg\n",
      "ospath 2775_1088_3365_1661\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000050.JPG\\2655_1941_3176_2448.jpg\n",
      "ospath 2655_1941_3176_2448\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000050.JPG\\3199_2476_3674_2935.jpg\n",
      "ospath 3199_2476_3674_2935\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000050.JPG\\3454_1790_4008_2315.jpg\n",
      "ospath 3454_1790_4008_2315\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000051.JPG\\1990_2018_2614_2640.jpg\n",
      "ospath 1990_2018_2614_2640\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000051.JPG\\2359_1199_2902_1707.jpg\n",
      "ospath 2359_1199_2902_1707\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000051.JPG\\2698_2392_3267_2942.jpg\n",
      "ospath 2698_2392_3267_2942\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000051.JPG\\3054_1682_3556_2162.jpg\n",
      "ospath 3054_1682_3556_2162\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000052.JPG\\1049_2386_1812_3131.jpg\n",
      "ospath 1049_2386_1812_3131\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000052.JPG\\2195_1782_2741_2291.jpg\n",
      "ospath 2195_1782_2741_2291\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000052.JPG\\2373_3176_2850_3642.jpg\n",
      "ospath 2373_3176_2850_3642\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000052.JPG\\3450_1437_3952_1929.jpg\n",
      "ospath 3450_1437_3952_1929\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000052.JPG\\3657_2498_4233_3055.jpg\n",
      "ospath 3657_2498_4233_3055\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000052.JPG\\4969_2011_5551_2571.jpg\n",
      "ospath 4969_2011_5551_2571\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000053.JPG\\1052_1304_1475_1719.jpg\n",
      "ospath 1052_1304_1475_1719\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000053.JPG\\1455_2473_2042_3050.jpg\n",
      "ospath 1455_2473_2042_3050\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000053.JPG\\2745_2803_3242_3274.jpg\n",
      "ospath 2745_2803_3242_3274\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000053.JPG\\3138_1160_3540_1561.jpg\n",
      "ospath 3138_1160_3540_1561\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000053.JPG\\4172_2770_4764_3344.jpg\n",
      "ospath 4172_2770_4764_3344\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000053.JPG\\5145_2525_5607_2973.jpg\n",
      "ospath 5145_2525_5607_2973\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000053.JPG\\5228_946_5718_1411.jpg\n",
      "ospath 5228_946_5718_1411\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000054.JPG\\2382_728_3055_1370.jpg\n",
      "ospath 2382_728_3055_1370\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000054.JPG\\2886_1748_3324_2186.jpg\n",
      "ospath 2886_1748_3324_2186\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000054.JPG\\3229_390_3895_1034.jpg\n",
      "ospath 3229_390_3895_1034\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000054.JPG\\3394_1277_3828_1698.jpg\n",
      "ospath 3394_1277_3828_1698\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000055.JPG\\1071_1612_1698_2236.jpg\n",
      "ospath 1071_1612_1698_2236\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000055.JPG\\1568_599_2237_1237.jpg\n",
      "ospath 1568_599_2237_1237\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000055.JPG\\1721_1420_2299_1969.jpg\n",
      "ospath 1721_1420_2299_1969\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000055.JPG\\2557_555_3221_1203.jpg\n",
      "ospath 2557_555_3221_1203\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000055.JPG\\745_818_1295_1341.jpg\n",
      "ospath 745_818_1295_1341\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000056.JPG\\1592_2558_2165_3111.jpg\n",
      "ospath 1592_2558_2165_3111\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000056.JPG\\2564_1243_3147_1797.jpg\n",
      "ospath 2564_1243_3147_1797\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000056.JPG\\2881_2601_3514_3223.jpg\n",
      "ospath 2881_2601_3514_3223\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000056.JPG\\3823_2120_4357_2642.jpg\n",
      "ospath 3823_2120_4357_2642\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000056.JPG\\4027_1091_4801_1858.jpg\n",
      "ospath 4027_1091_4801_1858\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000056.JPG\\825_1805_1409_2381.jpg\n",
      "ospath 825_1805_1409_2381\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000057.JPG\\1223_2328_1869_2953.jpg\n",
      "ospath 1223_2328_1869_2953\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000057.JPG\\1303_1485_1896_2063.jpg\n",
      "ospath 1303_1485_1896_2063\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000057.JPG\\1932_1034_2509_1598.jpg\n",
      "ospath 1932_1034_2509_1598\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000057.JPG\\2383_2629_3058_3276.jpg\n",
      "ospath 2383_2629_3058_3276\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000057.JPG\\2495_1871_2896_2266.jpg\n",
      "ospath 2495_1871_2896_2266\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000057.JPG\\3383_1658_3812_2070.jpg\n",
      "ospath 3383_1658_3812_2070\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000058.JPG\\2073_1330_2898_2137.jpg\n",
      "ospath 2073_1330_2898_2137\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000058.JPG\\2131_2222_2812_2890.jpg\n",
      "ospath 2131_2222_2812_2890\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000058.JPG\\2761_1876_3445_2558.jpg\n",
      "ospath 2761_1876_3445_2558\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000059.JPG\\1299_2134_1837_2646.jpg\n",
      "ospath 1299_2134_1837_2646\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000059.JPG\\1878_912_2505_1525.jpg\n",
      "ospath 1878_912_2505_1525\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000059.JPG\\2101_1620_2769_2272.jpg\n",
      "ospath 2101_1620_2769_2272\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000059.JPG\\2519_2527_3197_3167.jpg\n",
      "ospath 2519_2527_3197_3167\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000059.JPG\\2631_1419_3034_1812.jpg\n",
      "ospath 2631_1419_3034_1812\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000059.JPG\\2963_896_3407_1341.jpg\n",
      "ospath 2963_896_3407_1341\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000059.JPG\\3235_1314_3774_1827.jpg\n",
      "ospath 3235_1314_3774_1827\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000059.JPG\\3304_2419_3932_3044.jpg\n",
      "ospath 3304_2419_3932_3044\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000059.JPG\\3872_1795_4467_2366.jpg\n",
      "ospath 3872_1795_4467_2366\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000059.JPG\\4332_2525_4867_3050.jpg\n",
      "ospath 4332_2525_4867_3050\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000059.JPG\\4494_799_5272_1560.jpg\n",
      "ospath 4494_799_5272_1560\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000059.JPG\\4936_2286_5499_2824.jpg\n",
      "ospath 4936_2286_5499_2824\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000059.JPG\\720_2900_1163_3333.jpg\n",
      "ospath 720_2900_1163_3333\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000059.JPG\\852_1399_1327_1853.jpg\n",
      "ospath 852_1399_1327_1853\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000060.JPG\\1368_1158_1860_1631.jpg\n",
      "ospath 1368_1158_1860_1631\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000060.JPG\\1826_1820_2350_2335.jpg\n",
      "ospath 1826_1820_2350_2335\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000060.JPG\\2325_517_2794_987.jpg\n",
      "ospath 2325_517_2794_987\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000060.JPG\\2624_2078_3284_2729.jpg\n",
      "ospath 2624_2078_3284_2729\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000060.JPG\\3004_904_3605_1489.jpg\n",
      "ospath 3004_904_3605_1489\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000060.JPG\\4146_1771_4918_2526.jpg\n",
      "ospath 4146_1771_4918_2526\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000060.JPG\\4464_612_5040_1170.jpg\n",
      "ospath 4464_612_5040_1170\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000061.JPG\\1926_1078_2473_1617.jpg\n",
      "ospath 1926_1078_2473_1617\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000061.JPG\\2394_2438_3162_3195.jpg\n",
      "ospath 2394_2438_3162_3195\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000061.JPG\\2778_783_3353_1334.jpg\n",
      "ospath 2778_783_3353_1334\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000061.JPG\\2937_1677_3573_2287.jpg\n",
      "ospath 2937_1677_3573_2287\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000061.JPG\\3587_645_4266_1303.jpg\n",
      "ospath 3587_645_4266_1303\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000062.JPG\\1488_1287_1944_1715.jpg\n",
      "ospath 1488_1287_1944_1715\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000062.JPG\\2128_1428_2711_2010.jpg\n",
      "ospath 2128_1428_2711_2010\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000062.JPG\\2177_2298_2766_2878.jpg\n",
      "ospath 2177_2298_2766_2878\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000062.JPG\\2673_786_3359_1447.jpg\n",
      "ospath 2673_786_3359_1447\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000062.JPG\\2896_1984_3402_2462.jpg\n",
      "ospath 2896_1984_3402_2462\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000063.JPG\\1985_1873_2574_2448.jpg\n",
      "ospath 1985_1873_2574_2448\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000063.JPG\\2514_2600_3190_3268.jpg\n",
      "ospath 2514_2600_3190_3268\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000063.JPG\\2816_1737_3406_2319.jpg\n",
      "ospath 2816_1737_3406_2319\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000064.JPG\\1605_1981_2370_2739.jpg\n",
      "ospath 1605_1981_2370_2739\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000064.JPG\\1670_1153_2261_1723.jpg\n",
      "ospath 1670_1153_2261_1723\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000064.JPG\\1726_3005_2315_3576.jpg\n",
      "ospath 1726_3005_2315_3576\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000064.JPG\\2366_1107_2912_1631.jpg\n",
      "ospath 2366_1107_2912_1631\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000064.JPG\\2492_2552_3047_3086.jpg\n",
      "ospath 2492_2552_3047_3086\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000064.JPG\\2607_1802_3240_2434.jpg\n",
      "ospath 2607_1802_3240_2434\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000064.JPG\\885_2647_1347_3100.jpg\n",
      "ospath 885_2647_1347_3100\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000064.JPG\\931_1732_1486_2264.jpg\n",
      "ospath 931_1732_1486_2264\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000065.JPG\\1449_1466_2225_2225.jpg\n",
      "ospath 1449_1466_2225_2225\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000065.JPG\\1863_2369_2289_2788.jpg\n",
      "ospath 1863_2369_2289_2788\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000065.JPG\\2166_770_2802_1401.jpg\n",
      "ospath 2166_770_2802_1401\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000065.JPG\\2646_2341_3072_2757.jpg\n",
      "ospath 2646_2341_3072_2757\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000065.JPG\\2746_1469_3315_1997.jpg\n",
      "ospath 2746_1469_3315_1997\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000065.JPG\\3523_2022_4308_2775.jpg\n",
      "ospath 3523_2022_4308_2775\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000065.JPG\\3798_1234_4259_1666.jpg\n",
      "ospath 3798_1234_4259_1666\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000066.JPG\\1524_1958_2014_2433.jpg\n",
      "ospath 1524_1958_2014_2433\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000066.JPG\\2090_2820_2642_3352.jpg\n",
      "ospath 2090_2820_2642_3352\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000066.JPG\\2198_1886_2857_2537.jpg\n",
      "ospath 2198_1886_2857_2537\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000067.JPG\\1952_1675_2583_2292.jpg\n",
      "ospath 1952_1675_2583_2292\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000067.JPG\\2130_2332_2753_2943.jpg\n",
      "ospath 2130_2332_2753_2943\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000067.JPG\\2536_1247_3166_1848.jpg\n",
      "ospath 2536_1247_3166_1848\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000067.JPG\\2718_2241_3431_2936.jpg\n",
      "ospath 2718_2241_3431_2936\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000068.JPG\\2792_1439_3371_2006.jpg\n",
      "ospath 2792_1439_3371_2006\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000068.JPG\\3126_2249_3700_2804.jpg\n",
      "ospath 3126_2249_3700_2804\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000068.JPG\\3743_1424_4322_1989.jpg\n",
      "ospath 3743_1424_4322_1989\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\1573_1026_2167_1609.jpg\n",
      "ospath 1573_1026_2167_1609\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000069.JPG\\1583_2497_2107_3005.jpg\n",
      "ospath 1583_2497_2107_3005\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000069.JPG\\1679_2033_2156_2489.jpg\n",
      "ospath 1679_2033_2156_2489\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\1909_596_2459_1129.jpg\n",
      "ospath 1909_596_2459_1129\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000069.JPG\\1986_2987_2555_3555.jpg\n",
      "ospath 1986_2987_2555_3555\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000069.JPG\\2328_1638_2918_2217.jpg\n",
      "ospath 2328_1638_2918_2217\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000069.JPG\\2371_2309_2899_2820.jpg\n",
      "ospath 2371_2309_2899_2820\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000069.JPG\\2403_813_2899_1297.jpg\n",
      "ospath 2403_813_2899_1297\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\3117_909_3730_1502.jpg\n",
      "ospath 3117_909_3730_1502\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000069.JPG\\3120_2089_3745_2713.jpg\n",
      "ospath 3120_2089_3745_2713\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000069.JPG\\3398_2782_4038_3409.jpg\n",
      "ospath 3398_2782_4038_3409\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000069.JPG\\3724_1113_4499_1878.jpg\n",
      "ospath 3724_1113_4499_1878\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\4073_2317_4675_2903.jpg\n",
      "ospath 4073_2317_4675_2903\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000069.JPG\\4085_557_4495_959.jpg\n",
      "ospath 4085_557_4495_959\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000069.JPG\\4537_2022_4988_2458.jpg\n",
      "ospath 4537_2022_4988_2458\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000069.JPG\\4662_1213_5120_1656.jpg\n",
      "ospath 4662_1213_5120_1656\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\5028_869_5632_1455.jpg\n",
      "ospath 5028_869_5632_1455\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000069.JPG\\5058_1783_5713_2413.jpg\n",
      "ospath 5058_1783_5713_2413\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000069.JPG\\5075_2424_5618_2941.jpg\n",
      "ospath 5075_2424_5618_2941\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000069.JPG\\868_1468_1458_2041.jpg\n",
      "ospath 868_1468_1458_2041\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000070.JPG\\1502_2107_2145_2733.jpg\n",
      "ospath 1502_2107_2145_2733\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000070.JPG\\2100_1669_2619_2168.jpg\n",
      "ospath 2100_1669_2619_2168\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000070.JPG\\2118_2701_2735_3298.jpg\n",
      "ospath 2118_2701_2735_3298\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000070.JPG\\2355_2197_2923_2764.jpg\n",
      "ospath 2355_2197_2923_2764\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000071.JPG\\1345_1842_1870_2349.jpg\n",
      "ospath 1345_1842_1870_2349\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000071.JPG\\1804_1604_2256_2042.jpg\n",
      "ospath 1804_1604_2256_2042\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000071.JPG\\2249_1243_2829_1800.jpg\n",
      "ospath 2249_1243_2829_1800\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000071.JPG\\2512_2349_3074_2874.jpg\n",
      "ospath 2512_2349_3074_2874\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000071.JPG\\2871_2755_3338_3214.jpg\n",
      "ospath 2871_2755_3338_3214\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000071.JPG\\2937_1108_3511_1668.jpg\n",
      "ospath 2937_1108_3511_1668\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000071.JPG\\3289_2921_3751_3390.jpg\n",
      "ospath 3289_2921_3751_3390\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000071.JPG\\3370_1837_3908_2341.jpg\n",
      "ospath 3370_1837_3908_2341\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000071.JPG\\3750_1381_4381_2004.jpg\n",
      "ospath 3750_1381_4381_2004\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000072.JPG\\2556_1911_3043_2380.jpg\n",
      "ospath 2556_1911_3043_2380\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000072.JPG\\2748_2296_3436_2980.jpg\n",
      "ospath 2748_2296_3436_2980\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000072.JPG\\2977_1629_3426_2073.jpg\n",
      "ospath 2977_1629_3426_2073\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000072.JPG\\3261_1923_3943_2589.jpg\n",
      "ospath 3261_1923_3943_2589\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000073.JPG\\2188_2139_2761_2701.jpg\n",
      "ospath 2188_2139_2761_2701\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000073.JPG\\2262_1059_2822_1581.jpg\n",
      "ospath 2262_1059_2822_1581\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000073.JPG\\2904_2373_3333_2790.jpg\n",
      "ospath 2904_2373_3333_2790\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000073.JPG\\3202_978_3701_1458.jpg\n",
      "ospath 3202_978_3701_1458\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000073.JPG\\3329_1776_3825_2253.jpg\n",
      "ospath 3329_1776_3825_2253\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000073.JPG\\4138_1251_4730_1831.jpg\n",
      "ospath 4138_1251_4730_1831\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000073.JPG\\4301_2370_4830_2877.jpg\n",
      "ospath 4301_2370_4830_2877\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000073.JPG\\4827_1295_5296_1737.jpg\n",
      "ospath 4827_1295_5296_1737\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000074.JPG\\1214_2103_1873_2744.jpg\n",
      "ospath 1214_2103_1873_2744\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000074.JPG\\1813_2929_2342_3440.jpg\n",
      "ospath 1813_2929_2342_3440\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000074.JPG\\2345_1762_2872_2275.jpg\n",
      "ospath 2345_1762_2872_2275\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000074.JPG\\2462_2492_3226_3248.jpg\n",
      "ospath 2462_2492_3226_3248\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000075.JPG\\1133_2103_1668_2614.jpg\n",
      "ospath 1133_2103_1668_2614\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000075.JPG\\2277_2882_2679_3279.jpg\n",
      "ospath 2277_2882_2679_3279\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000075.JPG\\2517_1851_3055_2366.jpg\n",
      "ospath 2517_1851_3055_2366\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000075.JPG\\3614_3202_4065_3630.jpg\n",
      "ospath 3614_3202_4065_3630\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000075.JPG\\3802_1286_4478_1941.jpg\n",
      "ospath 3802_1286_4478_1941\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000075.JPG\\4143_2150_4933_2901.jpg\n",
      "ospath 4143_2150_4933_2901\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000076.JPG\\2056_1824_2675_2434.jpg\n",
      "ospath 2056_1824_2675_2434\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000076.JPG\\2444_938_3109_1593.jpg\n",
      "ospath 2444_938_3109_1593\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000076.JPG\\2820_2118_3349_2629.jpg\n",
      "ospath 2820_2118_3349_2629\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000076.JPG\\3281_1090_3883_1666.jpg\n",
      "ospath 3281_1090_3883_1666\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000077.JPG\\1944_2270_2399_2735.jpg\n",
      "ospath 1944_2270_2399_2735\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000077.JPG\\2249_1370_2843_1949.jpg\n",
      "ospath 2249_1370_2843_1949\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000077.JPG\\2457_2560_3275_3369.jpg\n",
      "ospath 2457_2560_3275_3369\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000077.JPG\\2520_2097_2973_2538.jpg\n",
      "ospath 2520_2097_2973_2538\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000077.JPG\\3009_1548_3618_2130.jpg\n",
      "ospath 3009_1548_3618_2130\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000078.JPG\\1963_1535_2414_1964.jpg\n",
      "ospath 1963_1535_2414_1964\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000078.JPG\\2245_2428_2837_3005.jpg\n",
      "ospath 2245_2428_2837_3005\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000078.JPG\\2754_1301_3290_1807.jpg\n",
      "ospath 2754_1301_3290_1807\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000078.JPG\\3080_2333_3657_2890.jpg\n",
      "ospath 3080_2333_3657_2890\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000079.JPG\\2577_2023_3177_2624.jpg\n",
      "ospath 2577_2023_3177_2624\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000079.JPG\\2781_1418_3383_2014.jpg\n",
      "ospath 2781_1418_3383_2014\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000079.JPG\\3227_2318_3662_2738.jpg\n",
      "ospath 3227_2318_3662_2738\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000079.JPG\\3371_1718_3942_2257.jpg\n",
      "ospath 3371_1718_3942_2257\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000080.JPG\\1737_1479_2305_2040.jpg\n",
      "ospath 1737_1479_2305_2040\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000080.JPG\\2440_996_2922_1468.jpg\n",
      "ospath 2440_996_2922_1468\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000080.JPG\\2533_1774_3155_2378.jpg\n",
      "ospath 2533_1774_3155_2378\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000080.JPG\\3183_1354_3727_1882.jpg\n",
      "ospath 3183_1354_3727_1882\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000081.JPG\\2394_879_3101_1581.jpg\n",
      "ospath 2394_879_3101_1581\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000081.JPG\\2427_1594_3243_2409.jpg\n",
      "ospath 2427_1594_3243_2409\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000081.JPG\\3189_1469_3769_2017.jpg\n",
      "ospath 3189_1469_3769_2017\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000082.JPG\\2555_1475_3221_2136.jpg\n",
      "ospath 2555_1475_3221_2136\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000082.JPG\\2883_2183_3603_2891.jpg\n",
      "ospath 2883_2183_3603_2891\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000082.JPG\\3300_1608_3944_2222.jpg\n",
      "ospath 3300_1608_3944_2222\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000083.JPG\\1969_1234_2558_1808.jpg\n",
      "ospath 1969_1234_2558_1808\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000083.JPG\\2314_1812_2806_2284.jpg\n",
      "ospath 2314_1812_2806_2284\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000083.JPG\\2340_528_2917_1091.jpg\n",
      "ospath 2340_528_2917_1091\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000083.JPG\\2461_2515_2894_2937.jpg\n",
      "ospath 2461_2515_2894_2937\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000083.JPG\\2805_1440_3304_1912.jpg\n",
      "ospath 2805_1440_3304_1912\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000083.JPG\\2914_1987_3463_2495.jpg\n",
      "ospath 2914_1987_3463_2495\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000083.JPG\\3106_650_3780_1295.jpg\n",
      "ospath 3106_650_3780_1295\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000083.JPG\\3323_1252_3992_1909.jpg\n",
      "ospath 3323_1252_3992_1909\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000084.JPG\\1776_1786_2204_2206.jpg\n",
      "ospath 1776_1786_2204_2206\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000084.JPG\\2400_2294_2936_2807.jpg\n",
      "ospath 2400_2294_2936_2807\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000084.JPG\\3047_1246_3677_1867.jpg\n",
      "ospath 3047_1246_3677_1867\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000084.JPG\\3194_2838_3765_3397.jpg\n",
      "ospath 3194_2838_3765_3397\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000084.JPG\\3830_1826_4403_2379.jpg\n",
      "ospath 3830_1826_4403_2379\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000085.JPG\\1007_1949_1461_2391.jpg\n",
      "ospath 1007_1949_1461_2391\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000085.JPG\\1481_684_2149_1335.jpg\n",
      "ospath 1481_684_2149_1335\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000085.JPG\\1771_1855_2328_2387.jpg\n",
      "ospath 1771_1855_2328_2387\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000085.JPG\\2292_1423_2871_1992.jpg\n",
      "ospath 2292_1423_2871_1992\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000085.JPG\\2583_2121_3249_2765.jpg\n",
      "ospath 2583_2121_3249_2765\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000085.JPG\\3107_574_3676_1127.jpg\n",
      "ospath 3107_574_3676_1127\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000085.JPG\\3577_1535_4059_2004.jpg\n",
      "ospath 3577_1535_4059_2004\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000085.JPG\\4024_1633_4144_1919.jpg\n",
      "ospath 4024_1633_4144_1919\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000085.JPG\\964_1042_1553_1612.jpg\n",
      "ospath 964_1042_1553_1612\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000086.JPG\\1221_1578_1780_2111.jpg\n",
      "ospath 1221_1578_1780_2111\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000086.JPG\\2186_1244_2610_1664.jpg\n",
      "ospath 2186_1244_2610_1664\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000086.JPG\\2657_2596_3179_3110.jpg\n",
      "ospath 2657_2596_3179_3110\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000086.JPG\\3061_1282_3737_1946.jpg\n",
      "ospath 3061_1282_3737_1946\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000086.JPG\\4098_1298_4592_1784.jpg\n",
      "ospath 4098_1298_4592_1784\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000086.JPG\\4143_2177_4808_2822.jpg\n",
      "ospath 4143_2177_4808_2822\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000086.JPG\\4321_417_4883_963.jpg\n",
      "ospath 4321_417_4883_963\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000087.JPG\\2476_1396_3113_2023.jpg\n",
      "ospath 2476_1396_3113_2023\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000087.JPG\\2662_2001_3382_2705.jpg\n",
      "ospath 2662_2001_3382_2705\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000087.JPG\\3240_1623_3708_2069.jpg\n",
      "ospath 3240_1623_3708_2069\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000088.JPG\\2563_1213_3059_1691.jpg\n",
      "ospath 2563_1213_3059_1691\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000088.JPG\\2805_1765_3426_2372.jpg\n",
      "ospath 2805_1765_3426_2372\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000088.JPG\\3298_1398_3832_1927.jpg\n",
      "ospath 3298_1398_3832_1927\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000089.JPG\\1032_1175_1459_1592.jpg\n",
      "ospath 1032_1175_1459_1592\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000089.JPG\\1603_1633_2172_2179.jpg\n",
      "ospath 1603_1633_2172_2179\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000089.JPG\\1749_2631_2238_3098.jpg\n",
      "ospath 1749_2631_2238_3098\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000089.JPG\\2761_1319_3521_2068.jpg\n",
      "ospath 2761_1319_3521_2068\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000089.JPG\\562_2050_1106_2585.jpg\n",
      "ospath 562_2050_1106_2585\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000090.JPG\\1544_2197_1971_2609.jpg\n",
      "ospath 1544_2197_1971_2609\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000090.JPG\\2036_1075_2635_1654.jpg\n",
      "ospath 2036_1075_2635_1654\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000090.JPG\\2332_2967_2809_3423.jpg\n",
      "ospath 2332_2967_2809_3423\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000090.JPG\\2704_1940_3234_2454.jpg\n",
      "ospath 2704_1940_3234_2454\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000090.JPG\\3652_2596_4208_3137.jpg\n",
      "ospath 3652_2596_4208_3137\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000090.JPG\\3911_1463_4414_1954.jpg\n",
      "ospath 3911_1463_4414_1954\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000090.JPG\\4705_2092_5241_2605.jpg\n",
      "ospath 4705_2092_5241_2605\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000090.JPG\\5052_903_5702_1528.jpg\n",
      "ospath 5052_903_5702_1528\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000091.JPG\\2428_2721_3015_3290.jpg\n",
      "ospath 2428_2721_3015_3290\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000091.JPG\\2618_1103_3077_1540.jpg\n",
      "ospath 2618_1103_3077_1540\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000091.JPG\\4026_2918_4589_3445.jpg\n",
      "ospath 4026_2918_4589_3445\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000091.JPG\\4085_818_4573_1288.jpg\n",
      "ospath 4085_818_4573_1288\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000091.JPG\\4482_1791_5020_2307.jpg\n",
      "ospath 4482_1791_5020_2307\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000092.JPG\\2705_1612_3270_2152.jpg\n",
      "ospath 2705_1612_3270_2152\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000092.JPG\\2715_823_3206_1298.jpg\n",
      "ospath 2715_823_3206_1298\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000092.JPG\\2784_2184_3548_2934.jpg\n",
      "ospath 2784_2184_3548_2934\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000092.JPG\\3280_814_3852_1364.jpg\n",
      "ospath 3280_814_3852_1364\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000092.JPG\\3415_1621_3882_2078.jpg\n",
      "ospath 3415_1621_3882_2078\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000093.JPG\\1705_1575_2265_2115.jpg\n",
      "ospath 1705_1575_2265_2115\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000093.JPG\\2214_2407_2746_2925.jpg\n",
      "ospath 2214_2407_2746_2925\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000093.JPG\\2817_1383_3486_2026.jpg\n",
      "ospath 2817_1383_3486_2026\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000093.JPG\\3040_2749_3486_3187.jpg\n",
      "ospath 3040_2749_3486_3187\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000094.JPG\\2136_2616_2646_3101.jpg\n",
      "ospath 2136_2616_2646_3101\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000094.JPG\\2380_1325_3024_1954.jpg\n",
      "ospath 2380_1325_3024_1954\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000094.JPG\\3345_1989_4024_2641.jpg\n",
      "ospath 3345_1989_4024_2641\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000094.JPG\\4329_1312_4859_1823.jpg\n",
      "ospath 4329_1312_4859_1823\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000094.JPG\\4379_2574_4889_3058.jpg\n",
      "ospath 4379_2574_4889_3058\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000095.JPG\\2100_1481_2728_2105.jpg\n",
      "ospath 2100_1481_2728_2105\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000095.JPG\\2172_652_2752_1190.jpg\n",
      "ospath 2172_652_2752_1190\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000095.JPG\\2331_2251_2856_2773.jpg\n",
      "ospath 2331_2251_2856_2773\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000095.JPG\\2815_1691_3255_2133.jpg\n",
      "ospath 2815_1691_3255_2133\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000095.JPG\\2994_1078_3450_1525.jpg\n",
      "ospath 2994_1078_3450_1525\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000095.JPG\\3143_2598_3583_3047.jpg\n",
      "ospath 3143_2598_3583_3047\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000095.JPG\\3318_679_3801_1152.jpg\n",
      "ospath 3318_679_3801_1152\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000095.JPG\\3420_1755_4006_2325.jpg\n",
      "ospath 3420_1755_4006_2325\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000095.JPG\\3636_1264_4138_1734.jpg\n",
      "ospath 3636_1264_4138_1734\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000096.JPG\\2035_1128_2534_1602.jpg\n",
      "ospath 2035_1128_2534_1602\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000096.JPG\\2373_2046_2778_2440.jpg\n",
      "ospath 2373_2046_2778_2440\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000096.JPG\\2761_1173_3393_1789.jpg\n",
      "ospath 2761_1173_3393_1789\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000096.JPG\\3185_2206_3632_2654.jpg\n",
      "ospath 3185_2206_3632_2654\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000096.JPG\\3508_1367_4098_1924.jpg\n",
      "ospath 3508_1367_4098_1924\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000096.JPG\\4014_1645_4161_1911.jpg\n",
      "ospath 4014_1645_4161_1911\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000097.JPG\\1394_2126_2156_2880.jpg\n",
      "ospath 1394_2126_2156_2880\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000097.JPG\\2466_1410_3083_2007.jpg\n",
      "ospath 2466_1410_3083_2007\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000097.JPG\\3100_2304_3770_2946.jpg\n",
      "ospath 3100_2304_3770_2946\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000097.JPG\\4153_1436_4930_2196.jpg\n",
      "ospath 4153_1436_4930_2196\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000098.JPG\\1668_1933_2239_2495.jpg\n",
      "ospath 1668_1933_2239_2495\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000098.JPG\\2012_1197_2433_1615.jpg\n",
      "ospath 2012_1197_2433_1615\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000098.JPG\\2183_2605_2722_3117.jpg\n",
      "ospath 2183_2605_2722_3117\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000098.JPG\\2498_1384_3141_2010.jpg\n",
      "ospath 2498_1384_3141_2010\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000098.JPG\\2933_2071_3437_2550.jpg\n",
      "ospath 2933_2071_3437_2550\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000099.JPG\\2343_2376_2974_3011.jpg\n",
      "ospath 2343_2376_2974_3011\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000099.JPG\\2514_1623_3037_2124.jpg\n",
      "ospath 2514_1623_3037_2124\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000099.JPG\\3198_1834_3759_2387.jpg\n",
      "ospath 3198_1834_3759_2387\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000099.JPG\\3453_2514_3931_3005.jpg\n",
      "ospath 3453_2514_3931_3005\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000100.JPG\\1165_1630_1701_2120.jpg\n",
      "ospath 1165_1630_1701_2120\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000100.JPG\\1360_3211_1829_3674.jpg\n",
      "ospath 1360_3211_1829_3674\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000100.JPG\\1397_2593_1916_3111.jpg\n",
      "ospath 1397_2593_1916_3111\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000100.JPG\\1444_2216_1841_2611.jpg\n",
      "ospath 1444_2216_1841_2611\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000100.JPG\\1744_1691_2262_2194.jpg\n",
      "ospath 1744_1691_2262_2194\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000100.JPG\\1885_994_2370_1470.jpg\n",
      "ospath 1885_994_2370_1470\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000100.JPG\\1909_2171_2311_2566.jpg\n",
      "ospath 1909_2171_2311_2566\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000100.JPG\\2030_2466_2785_3213.jpg\n",
      "ospath 2030_2466_2785_3213\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000100.JPG\\2341_1798_3100_2550.jpg\n",
      "ospath 2341_1798_3100_2550\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000100.JPG\\2613_1227_3251_1840.jpg\n",
      "ospath 2613_1227_3251_1840\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000100.JPG\\2677_2527_2896_2737.jpg\n",
      "ospath 2677_2527_2896_2737\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000100.JPG\\2766_2517_3320_3055.jpg\n",
      "ospath 2766_2517_3320_3055\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000100.JPG\\3223_3132_3675_3578.jpg\n",
      "ospath 3223_3132_3675_3578\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000100.JPG\\3274_2377_3890_2984.jpg\n",
      "ospath 3274_2377_3890_2984\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000100.JPG\\3314_1837_3858_2345.jpg\n",
      "ospath 3314_1837_3858_2345\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000100.JPG\\371_2320_965_2891.jpg\n",
      "ospath 371_2320_965_2891\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000100.JPG\\768_3027_1269_3511.jpg\n",
      "ospath 768_3027_1269_3511\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000100.JPG\\907_2064_1473_2588.jpg\n",
      "ospath 907_2064_1473_2588\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000101.JPG\\1891_2550_2295_2942.jpg\n",
      "ospath 1891_2550_2295_2942\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000101.JPG\\2054_1098_2585_1609.jpg\n",
      "ospath 2054_1098_2585_1609\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000101.JPG\\2443_1843_3117_2491.jpg\n",
      "ospath 2443_1843_3117_2491\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000101.JPG\\2565_2808_3157_3381.jpg\n",
      "ospath 2565_2808_3157_3381\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000101.JPG\\2955_675_3480_1190.jpg\n",
      "ospath 2955_675_3480_1190\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000101.JPG\\3622_3076_4049_3498.jpg\n",
      "ospath 3622_3076_4049_3498\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000101.JPG\\3717_2476_4220_2943.jpg\n",
      "ospath 3717_2476_4220_2943\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000101.JPG\\3991_1041_4555_1599.jpg\n",
      "ospath 3991_1041_4555_1599\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000101.JPG\\4534_1813_4998_2266.jpg\n",
      "ospath 4534_1813_4998_2266\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000102.JPG\\1353_1185_2126_1943.jpg\n",
      "ospath 1353_1185_2126_1943\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000102.JPG\\3317_1969_4094_2720.jpg\n",
      "ospath 3317_1969_4094_2720\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000103.JPG\\2987_1685_3627_2307.jpg\n",
      "ospath 2987_1685_3627_2307\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000103.JPG\\3063_986_3645_1537.jpg\n",
      "ospath 3063_986_3645_1537\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000104.JPG\\1014_3058_1594_3609.jpg\n",
      "ospath 1014_3058_1594_3609\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000104.JPG\\112_1978_753_2595.jpg\n",
      "ospath 112_1978_753_2595\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000104.JPG\\1922_1531_2447_2045.jpg\n",
      "ospath 1922_1531_2447_2045\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000104.JPG\\2653_2794_3325_3440.jpg\n",
      "ospath 2653_2794_3325_3440\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000104.JPG\\3308_1189_3808_1659.jpg\n",
      "ospath 3308_1189_3808_1659\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000104.JPG\\3599_2687_4373_3437.jpg\n",
      "ospath 3599_2687_4373_3437\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000104.JPG\\94_554_765_1202.jpg\n",
      "ospath 94_554_765_1202\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000105.JPG\\2069_1902_2840_2658.jpg\n",
      "ospath 2069_1902_2840_2658\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000105.JPG\\3261_1624_3820_2155.jpg\n",
      "ospath 3261_1624_3820_2155\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000105.JPG\\3311_469_3879_1005.jpg\n",
      "ospath 3311_469_3879_1005\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000105.JPG\\3346_2760_4132_3512.jpg\n",
      "ospath 3346_2760_4132_3512\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000105.JPG\\4412_1832_4839_2251.jpg\n",
      "ospath 4412_1832_4839_2251\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000106.JPG\\1020_1468_1547_1979.jpg\n",
      "ospath 1020_1468_1547_1979\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000106.JPG\\1120_606_1694_1158.jpg\n",
      "ospath 1120_606_1694_1158\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000106.JPG\\1440_1785_2019_2338.jpg\n",
      "ospath 1440_1785_2019_2338\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000106.JPG\\1469_1041_2000_1556.jpg\n",
      "ospath 1469_1041_2000_1556\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000106.JPG\\1980_572_2553_1130.jpg\n",
      "ospath 1980_572_2553_1130\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000106.JPG\\2105_1801_2591_2270.jpg\n",
      "ospath 2105_1801_2591_2270\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000106.JPG\\2255_1074_2723_1535.jpg\n",
      "ospath 2255_1074_2723_1535\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000106.JPG\\2537_1589_3042_2064.jpg\n",
      "ospath 2537_1589_3042_2064\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000107.JPG\\1937_1323_2464_1833.jpg\n",
      "ospath 1937_1323_2464_1833\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000107.JPG\\1989_603_2515_1109.jpg\n",
      "ospath 1989_603_2515_1109\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000107.JPG\\2542_1732_3066_2228.jpg\n",
      "ospath 2542_1732_3066_2228\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000107.JPG\\2936_1418_3461_1930.jpg\n",
      "ospath 2936_1418_3461_1930\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000107.JPG\\3074_717_3486_1119.jpg\n",
      "ospath 3074_717_3486_1119\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000108.JPG\\2794_1828_3558_2587.jpg\n",
      "ospath 2794_1828_3558_2587\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000108.JPG\\3549_1513_4053_1996.jpg\n",
      "ospath 3549_1513_4053_1996\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000109.JPG\\2186_1288_2655_1746.jpg\n",
      "ospath 2186_1288_2655_1746\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000109.JPG\\2503_618_3040_1130.jpg\n",
      "ospath 2503_618_3040_1130\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000109.JPG\\2569_1839_3155_2422.jpg\n",
      "ospath 2569_1839_3155_2422\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000109.JPG\\2797_1175_3453_1820.jpg\n",
      "ospath 2797_1175_3453_1820\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000109.JPG\\3048_2348_3479_2774.jpg\n",
      "ospath 3048_2348_3479_2774\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000109.JPG\\3238_1694_3832_2279.jpg\n",
      "ospath 3238_1694_3832_2279\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000109.JPG\\3374_990_3841_1450.jpg\n",
      "ospath 3374_990_3841_1450\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000110.JPG\\1508_1792_2107_2368.jpg\n",
      "ospath 1508_1792_2107_2368\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000110.JPG\\1975_1293_2565_1865.jpg\n",
      "ospath 1975_1293_2565_1865\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000110.JPG\\2170_2307_2674_2787.jpg\n",
      "ospath 2170_2307_2674_2787\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000110.JPG\\2525_905_3120_1485.jpg\n",
      "ospath 2525_905_3120_1485\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000110.JPG\\2982_1603_3578_2188.jpg\n",
      "ospath 2982_1603_3578_2188\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000111.JPG\\1713_2071_2470_2827.jpg\n",
      "ospath 1713_2071_2470_2827\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000111.JPG\\2162_3221_2623_3676.jpg\n",
      "ospath 2162_3221_2623_3676\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000111.JPG\\2601_1788_3098_2276.jpg\n",
      "ospath 2601_1788_3098_2276\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000111.JPG\\2775_2656_3371_3232.jpg\n",
      "ospath 2775_2656_3371_3232\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000112.JPG\\2109_759_2669_1289.jpg\n",
      "ospath 2109_759_2669_1289\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000112.JPG\\2302_1473_3064_2233.jpg\n",
      "ospath 2302_1473_3064_2233\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000112.JPG\\2620_2687_3057_3104.jpg\n",
      "ospath 2620_2687_3057_3104\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000112.JPG\\2839_948_3377_1456.jpg\n",
      "ospath 2839_948_3377_1456\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000112.JPG\\3232_220_3661_645.jpg\n",
      "ospath 3232_220_3661_645\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000112.JPG\\3461_1595_3946_2053.jpg\n",
      "ospath 3461_1595_3946_2053\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000112.JPG\\3725_1179_4131_1574.jpg\n",
      "ospath 3725_1179_4131_1574\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000112.JPG\\4371_371_4839_838.jpg\n",
      "ospath 4371_371_4839_838\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000112.JPG\\4594_1028_5374_1797.jpg\n",
      "ospath 4594_1028_5374_1797\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000112.JPG\\4847_2117_5299_2562.jpg\n",
      "ospath 4847_2117_5299_2562\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000113.JPG\\1407_2063_2172_2811.jpg\n",
      "ospath 1407_2063_2172_2811\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000113.JPG\\1937_887_2709_1646.jpg\n",
      "ospath 1937_887_2709_1646\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000113.JPG\\2508_1693_3181_2337.jpg\n",
      "ospath 2508_1693_3181_2337\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000113.JPG\\829_1800_1323_2272.jpg\n",
      "ospath 829_1800_1323_2272\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000113.JPG\\977_944_1607_1565.jpg\n",
      "ospath 977_944_1607_1565\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000114.JPG\\2377_1839_2982_2470.jpg\n",
      "ospath 2377_1839_2982_2470\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000114.JPG\\2642_1314_3171_1829.jpg\n",
      "ospath 2642_1314_3171_1829\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000114.JPG\\2917_2240_3450_2745.jpg\n",
      "ospath 2917_2240_3450_2745\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000114.JPG\\3278_1773_3850_2323.jpg\n",
      "ospath 3278_1773_3850_2323\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000115.JPG\\2518_2238_3021_2745.jpg\n",
      "ospath 2518_2238_3021_2745\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000115.JPG\\2814_1738_3338_2258.jpg\n",
      "ospath 2814_1738_3338_2258\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000115.JPG\\3045_2466_3577_2982.jpg\n",
      "ospath 3045_2466_3577_2982\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000115.JPG\\3370_1963_3872_2451.jpg\n",
      "ospath 3370_1963_3872_2451\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000116.JPG\\1074_2869_1523_3305.jpg\n",
      "ospath 1074_2869_1523_3305\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000116.JPG\\1171_1200_1647_1659.jpg\n",
      "ospath 1171_1200_1647_1659\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000116.JPG\\1639_1994_2119_2451.jpg\n",
      "ospath 1639_1994_2119_2451\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000116.JPG\\2144_2909_2680_3417.jpg\n",
      "ospath 2144_2909_2680_3417\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000116.JPG\\2215_1401_2790_1959.jpg\n",
      "ospath 2215_1401_2790_1959\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000116.JPG\\2934_2251_3389_2682.jpg\n",
      "ospath 2934_2251_3389_2682\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000116.JPG\\920_2095_1324_2487.jpg\n",
      "ospath 920_2095_1324_2487\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000117.JPG\\2850_742_3430_1301.jpg\n",
      "ospath 2850_742_3430_1301\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000117.JPG\\2884_3677_3105_3783.jpg\n",
      "ospath 2884_3677_3105_3783\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000117.JPG\\2919_1487_3592_2128.jpg\n",
      "ospath 2919_1487_3592_2128\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000117.JPG\\3314_1044_3887_1610.jpg\n",
      "ospath 3314_1044_3887_1610\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000118.JPG\\1051_1596_1641_2173.jpg\n",
      "ospath 1051_1596_1641_2173\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000118.JPG\\1379_2800_1943_3335.jpg\n",
      "ospath 1379_2800_1943_3335\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000118.JPG\\1467_2045_1943_2515.jpg\n",
      "ospath 1467_2045_1943_2515\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000118.JPG\\1779_1193_2370_1767.jpg\n",
      "ospath 1779_1193_2370_1767\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000118.JPG\\1953_2702_2403_3140.jpg\n",
      "ospath 1953_2702_2403_3140\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000118.JPG\\2181_1854_2742_2381.jpg\n",
      "ospath 2181_1854_2742_2381\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000118.JPG\\2202_825_2769_1383.jpg\n",
      "ospath 2202_825_2769_1383\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000118.JPG\\2724_1167_3257_1686.jpg\n",
      "ospath 2724_1167_3257_1686\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000118.JPG\\2766_1444_3293_2027.jpg\n",
      "ospath 2766_1444_3293_2027\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000118.JPG\\2809_2657_3385_3208.jpg\n",
      "ospath 2809_2657_3385_3208\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000118.JPG\\2846_1666_3275_2092.jpg\n",
      "ospath 2846_1666_3275_2092\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000118.JPG\\3540_1920_3986_2364.jpg\n",
      "ospath 3540_1920_3986_2364\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000119.JPG\\1231_1654_1784_2173.jpg\n",
      "ospath 1231_1654_1784_2173\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000119.JPG\\1403_2951_1877_3407.jpg\n",
      "ospath 1403_2951_1877_3407\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000119.JPG\\2075_1982_2555_2456.jpg\n",
      "ospath 2075_1982_2555_2456\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000119.JPG\\2983_2678_3517_3184.jpg\n",
      "ospath 2983_2678_3517_3184\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000119.JPG\\3164_1474_3652_1930.jpg\n",
      "ospath 3164_1474_3652_1930\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000119.JPG\\3836_1931_4364_2441.jpg\n",
      "ospath 3836_1931_4364_2441\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000120.JPG\\2504_1527_2968_1982.jpg\n",
      "ospath 2504_1527_2968_1982\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000120.JPG\\2858_785_3388_1300.jpg\n",
      "ospath 2858_785_3388_1300\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000120.JPG\\3025_1872_3552_2377.jpg\n",
      "ospath 3025_1872_3552_2377\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000120.JPG\\3346_1146_3975_1776.jpg\n",
      "ospath 3346_1146_3975_1776\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000121.JPG\\1635_1555_2194_2081.jpg\n",
      "ospath 1635_1555_2194_2081\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000121.JPG\\2358_2723_2956_3296.jpg\n",
      "ospath 2358_2723_2956_3296\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000121.JPG\\2515_1788_3096_2356.jpg\n",
      "ospath 2515_1788_3096_2356\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000121.JPG\\2586_1019_3074_1490.jpg\n",
      "ospath 2586_1019_3074_1490\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000121.JPG\\3710_1192_4118_1587.jpg\n",
      "ospath 3710_1192_4118_1587\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000122.JPG\\1194_2810_1770_3366.jpg\n",
      "ospath 1194_2810_1770_3366\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000122.JPG\\1526_2081_2112_2655.jpg\n",
      "ospath 1526_2081_2112_2655\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000122.JPG\\1871_3075_2548_3724.jpg\n",
      "ospath 1871_3075_2548_3724\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000122.JPG\\2211_1728_2684_2185.jpg\n",
      "ospath 2211_1728_2684_2185\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000122.JPG\\2446_2343_2907_2787.jpg\n",
      "ospath 2446_2343_2907_2787\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000123.JPG\\1762_1680_2279_2182.jpg\n",
      "ospath 1762_1680_2279_2182\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000123.JPG\\2176_2592_2642_3059.jpg\n",
      "ospath 2176_2592_2642_3059\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000123.JPG\\2585_2865_3217_3493.jpg\n",
      "ospath 2585_2865_3217_3493\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000123.JPG\\2588_2049_3115_2571.jpg\n",
      "ospath 2588_2049_3115_2571\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000123.JPG\\3187_1939_3845_2586.jpg\n",
      "ospath 3187_1939_3845_2586\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000124.JPG\\1532_2257_2063_2765.jpg\n",
      "ospath 1532_2257_2063_2765\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000124.JPG\\2056_1534_2585_2050.jpg\n",
      "ospath 2056_1534_2585_2050\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000124.JPG\\2273_2149_2939_2801.jpg\n",
      "ospath 2273_2149_2939_2801\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000124.JPG\\2340_2905_2834_3396.jpg\n",
      "ospath 2340_2905_2834_3396\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000124.JPG\\2968_1815_3428_2284.jpg\n",
      "ospath 2968_1815_3428_2284\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000125.JPG\\1386_3113_1921_3634.jpg\n",
      "ospath 1386_3113_1921_3634\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000125.JPG\\1614_1888_2275_2539.jpg\n",
      "ospath 1614_1888_2275_2539\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000125.JPG\\2176_826_2947_1580.jpg\n",
      "ospath 2176_826_2947_1580\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000125.JPG\\2261_3000_2817_3534.jpg\n",
      "ospath 2261_3000_2817_3534\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000125.JPG\\3079_1695_3636_2224.jpg\n",
      "ospath 3079_1695_3636_2224\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000125.JPG\\3122_2897_3659_3403.jpg\n",
      "ospath 3122_2897_3659_3403\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000126.JPG\\1848_1575_2608_2324.jpg\n",
      "ospath 1848_1575_2608_2324\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000126.JPG\\2484_2695_2912_3107.jpg\n",
      "ospath 2484_2695_2912_3107\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000126.JPG\\3084_2001_3642_2533.jpg\n",
      "ospath 3084_2001_3642_2533\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000127.JPG\\2453_1553_3090_2161.jpg\n",
      "ospath 2453_1553_3090_2161\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000127.JPG\\3022_963_3525_1459.jpg\n",
      "ospath 3022_963_3525_1459\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000127.JPG\\3283_1868_3977_2542.jpg\n",
      "ospath 3283_1868_3977_2542\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000127.JPG\\3555_1226_4121_1768.jpg\n",
      "ospath 3555_1226_4121_1768\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000128.JPG\\2283_1200_3096_2007.jpg\n",
      "ospath 2283_1200_3096_2007\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000128.JPG\\2318_2049_2966_2668.jpg\n",
      "ospath 2318_2049_2966_2668\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000128.JPG\\3004_1608_3637_2227.jpg\n",
      "ospath 3004_1608_3637_2227\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000129.JPG\\2173_1160_2820_1797.jpg\n",
      "ospath 2173_1160_2820_1797\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000129.JPG\\2699_2026_3108_2417.jpg\n",
      "ospath 2699_2026_3108_2417\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000129.JPG\\3041_1158_3656_1759.jpg\n",
      "ospath 3041_1158_3656_1759\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000129.JPG\\3371_1866_3819_2308.jpg\n",
      "ospath 3371_1866_3819_2308\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000130.JPG\\2076_2915_2624_3457.jpg\n",
      "ospath 2076_2915_2624_3457\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000130.JPG\\2095_2391_2552_2836.jpg\n",
      "ospath 2095_2391_2552_2836\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000130.JPG\\2793_1976_3388_2564.jpg\n",
      "ospath 2793_1976_3388_2564\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000130.JPG\\2855_2781_3310_3219.jpg\n",
      "ospath 2855_2781_3310_3219\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000131.JPG\\1614_2237_2200_2810.jpg\n",
      "ospath 1614_2237_2200_2810\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000131.JPG\\2364_2825_2940_3379.jpg\n",
      "ospath 2364_2825_2940_3379\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000131.JPG\\2369_1564_2941_2118.jpg\n",
      "ospath 2369_1564_2941_2118\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000131.JPG\\2661_2153_3148_2621.jpg\n",
      "ospath 2661_2153_3148_2621\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000131.JPG\\3290_1862_3721_2273.jpg\n",
      "ospath 3290_1862_3721_2273\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000132.JPG\\3101_1948_3503_2339.jpg\n",
      "ospath 3101_1948_3503_2339\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000133.JPG\\2895_1185_3673_1938.jpg\n",
      "ospath 2895_1185_3673_1938\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000134.JPG\\1318_1935_1858_2449.jpg\n",
      "ospath 1318_1935_1858_2449\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000134.JPG\\2427_1926_3195_2676.jpg\n",
      "ospath 2427_1926_3195_2676\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000134.JPG\\2436_570_2932_1045.jpg\n",
      "ospath 2436_570_2932_1045\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000134.JPG\\3393_1634_3822_2056.jpg\n",
      "ospath 3393_1634_3822_2056\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000134.JPG\\3563_872_4098_1382.jpg\n",
      "ospath 3563_872_4098_1382\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000135.JPG\\2282_1913_2770_2396.jpg\n",
      "ospath 2282_1913_2770_2396\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000135.JPG\\2320_2524_2906_3090.jpg\n",
      "ospath 2320_2524_2906_3090\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000135.JPG\\2869_1947_3425_2478.jpg\n",
      "ospath 2869_1947_3425_2478\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000136.JPG\\2629_1404_3212_1971.jpg\n",
      "ospath 2629_1404_3212_1971\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000136.JPG\\2884_2021_3575_2709.jpg\n",
      "ospath 2884_2021_3575_2709\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000136.JPG\\3442_1744_4049_2336.jpg\n",
      "ospath 3442_1744_4049_2336\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000137.JPG\\2453_1398_3029_1953.jpg\n",
      "ospath 2453_1398_3029_1953\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000137.JPG\\2493_2194_2962_2656.jpg\n",
      "ospath 2493_2194_2962_2656\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000137.JPG\\2594_714_3079_1179.jpg\n",
      "ospath 2594_714_3079_1179\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000137.JPG\\3086_2183_3722_2796.jpg\n",
      "ospath 3086_2183_3722_2796\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000137.JPG\\3121_546_3543_965.jpg\n",
      "ospath 3121_546_3543_965\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000137.JPG\\3125_786_3581_1456.jpg\n",
      "ospath 3125_786_3581_1456\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000137.JPG\\3154_1111_3578_1526.jpg\n",
      "ospath 3154_1111_3578_1526\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000137.JPG\\3549_1659_3995_2095.jpg\n",
      "ospath 3549_1659_3995_2095\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000137.JPG\\3779_825_4309_1339.jpg\n",
      "ospath 3779_825_4309_1339\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000138.JPG\\1002_984_1572_1535.jpg\n",
      "ospath 1002_984_1572_1535\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000138.JPG\\1282_1812_1746_2274.jpg\n",
      "ospath 1282_1812_1746_2274\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000138.JPG\\1805_587_2295_1063.jpg\n",
      "ospath 1805_587_2295_1063\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000138.JPG\\1984_1202_2626_1822.jpg\n",
      "ospath 1984_1202_2626_1822\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000138.JPG\\1994_2434_2421_2851.jpg\n",
      "ospath 1994_2434_2421_2851\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000138.JPG\\2603_2064_3272_2716.jpg\n",
      "ospath 2603_2064_3272_2716\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000138.JPG\\2681_259_3183_734.jpg\n",
      "ospath 2681_259_3183_734\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000138.JPG\\3256_1317_3830_1880.jpg\n",
      "ospath 3256_1317_3830_1880\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000138.JPG\\3339_671_3920_1231.jpg\n",
      "ospath 3339_671_3920_1231\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000139.JPG\\1147_2358_1552_2755.jpg\n",
      "ospath 1147_2358_1552_2755\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000139.JPG\\1861_910_2396_1427.jpg\n",
      "ospath 1861_910_2396_1427\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000139.JPG\\2773_1479_3369_2054.jpg\n",
      "ospath 2773_1479_3369_2054\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000139.JPG\\3595_2448_4146_2982.jpg\n",
      "ospath 3595_2448_4146_2982\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000139.JPG\\377_1669_858_2135.jpg\n",
      "ospath 377_1669_858_2135\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000139.JPG\\3963_1804_4645_2456.jpg\n",
      "ospath 3963_1804_4645_2456\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000139.JPG\\4910_805_5474_1338.jpg\n",
      "ospath 4910_805_5474_1338\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000140.JPG\\2452_1829_3061_2430.jpg\n",
      "ospath 2452_1829_3061_2430\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000140.JPG\\2843_1031_3533_1693.jpg\n",
      "ospath 2843_1031_3533_1693\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000140.JPG\\2970_2093_3570_2649.jpg\n",
      "ospath 2970_2093_3570_2649\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000140.JPG\\3417_1658_4054_2294.jpg\n",
      "ospath 3417_1658_4054_2294\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000141.JPG\\1403_1310_1898_1789.jpg\n",
      "ospath 1403_1310_1898_1789\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000141.JPG\\1762_2242_2328_2795.jpg\n",
      "ospath 1762_2242_2328_2795\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000141.JPG\\2513_2826_3052_3332.jpg\n",
      "ospath 2513_2826_3052_3332\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000141.JPG\\2822_1624_3406_2197.jpg\n",
      "ospath 2822_1624_3406_2197\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000141.JPG\\3807_2352_4300_2817.jpg\n",
      "ospath 3807_2352_4300_2817\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000141.JPG\\4513_1601_5098_2161.jpg\n",
      "ospath 4513_1601_5098_2161\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000142.JPG\\2819_1791_3417_2389.jpg\n",
      "ospath 2819_1791_3417_2389\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000142.JPG\\3223_2347_3847_2957.jpg\n",
      "ospath 3223_2347_3847_2957\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000142.JPG\\3398_1820_3970_2389.jpg\n",
      "ospath 3398_1820_3970_2389\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000143.JPG\\1591_1755_2075_2212.jpg\n",
      "ospath 1591_1755_2075_2212\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000143.JPG\\2048_1087_2713_1735.jpg\n",
      "ospath 2048_1087_2713_1735\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000143.JPG\\2231_2714_2678_3154.jpg\n",
      "ospath 2231_2714_2678_3154\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000143.JPG\\2337_2020_2895_2545.jpg\n",
      "ospath 2337_2020_2895_2545\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000143.JPG\\2835_1350_3236_1749.jpg\n",
      "ospath 2835_1350_3236_1749\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000143.JPG\\3140_2068_3811_2721.jpg\n",
      "ospath 3140_2068_3811_2721\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000144.JPG\\2706_1469_3203_1942.jpg\n",
      "ospath 2706_1469_3203_1942\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000144.JPG\\3324_2119_3778_2568.jpg\n",
      "ospath 3324_2119_3778_2568\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000144.JPG\\3657_670_4141_1151.jpg\n",
      "ospath 3657_670_4141_1151\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000144.JPG\\4340_1227_4880_1745.jpg\n",
      "ospath 4340_1227_4880_1745\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000144.JPG\\5083_1345_5730_1974.jpg\n",
      "ospath 5083_1345_5730_1974\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000145.JPG\\1960_1013_2717_1764.jpg\n",
      "ospath 1960_1013_2717_1764\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000145.JPG\\2180_2131_2628_2581.jpg\n",
      "ospath 2180_2131_2628_2581\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000145.JPG\\2810_658_3307_1147.jpg\n",
      "ospath 2810_658_3307_1147\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000145.JPG\\2990_1975_3551_2506.jpg\n",
      "ospath 2990_1975_3551_2506\n",
      "tensor([0])\n",
      "0.01EUR\n",
      "key: output\\L0000145.JPG\\3381_1224_3865_1695.jpg\n",
      "ospath 3381_1224_3865_1695\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000145.JPG\\3878_1682_4544_2353.jpg\n",
      "ospath 3878_1682_4544_2353\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000145.JPG\\3997_1574_4225_1780.jpg\n",
      "ospath 3997_1574_4225_1780\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000145.JPG\\4151_456_4788_1094.jpg\n",
      "ospath 4151_456_4788_1094\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000145.JPG\\4762_546_5252_1024.jpg\n",
      "ospath 4762_546_5252_1024\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000145.JPG\\5104_1279_5649_1795.jpg\n",
      "ospath 5104_1279_5649_1795\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000146.JPG\\2460_906_3080_1514.jpg\n",
      "ospath 2460_906_3080_1514\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000146.JPG\\2632_383_3182_912.jpg\n",
      "ospath 2632_383_3182_912\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000146.JPG\\2688_1482_3167_1955.jpg\n",
      "ospath 2688_1482_3167_1955\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000146.JPG\\3315_1437_3802_1902.jpg\n",
      "ospath 3315_1437_3802_1902\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000146.JPG\\3359_433_3830_893.jpg\n",
      "ospath 3359_433_3830_893\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000147.JPG\\1545_1042_2144_1619.jpg\n",
      "ospath 1545_1042_2144_1619\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000147.JPG\\2337_204_2906_763.jpg\n",
      "ospath 2337_204_2906_763\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000147.JPG\\2431_1753_2968_2269.jpg\n",
      "ospath 2431_1753_2968_2269\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000147.JPG\\3414_393_3944_906.jpg\n",
      "ospath 3414_393_3944_906\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000147.JPG\\3562_1434_4104_1960.jpg\n",
      "ospath 3562_1434_4104_1960\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000147.JPG\\4389_557_5068_1204.jpg\n",
      "ospath 4389_557_5068_1204\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000148.JPG\\1527_1188_2058_1700.jpg\n",
      "ospath 1527_1188_2058_1700\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000148.JPG\\2167_738_2700_1257.jpg\n",
      "ospath 2167_738_2700_1257\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000148.JPG\\2313_1457_2987_2115.jpg\n",
      "ospath 2313_1457_2987_2115\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000148.JPG\\3129_1592_3650_2097.jpg\n",
      "ospath 3129_1592_3650_2097\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000149.JPG\\1785_2337_2347_2868.jpg\n",
      "ospath 1785_2337_2347_2868\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000149.JPG\\2292_1811_2841_2340.jpg\n",
      "ospath 2292_1811_2841_2340\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000149.JPG\\2363_2858_2851_3341.jpg\n",
      "ospath 2363_2858_2851_3341\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000149.JPG\\2950_2489_3429_2955.jpg\n",
      "ospath 2950_2489_3429_2955\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000150.JPG\\1199_1215_1680_1681.jpg\n",
      "ospath 1199_1215_1680_1681\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000150.JPG\\1527_1964_1952_2382.jpg\n",
      "ospath 1527_1964_1952_2382\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000150.JPG\\1911_556_2690_1324.jpg\n",
      "ospath 1911_556_2690_1324\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000150.JPG\\1959_2260_2443_2719.jpg\n",
      "ospath 1959_2260_2443_2719\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000150.JPG\\3138_2237_3619_2695.jpg\n",
      "ospath 3138_2237_3619_2695\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000150.JPG\\3305_835_3905_1420.jpg\n",
      "ospath 3305_835_3905_1420\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000150.JPG\\3686_1741_4257_2294.jpg\n",
      "ospath 3686_1741_4257_2294\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000151.JPG\\1003_1231_1583_1795.jpg\n",
      "ospath 1003_1231_1583_1795\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000151.JPG\\1182_2593_1674_3070.jpg\n",
      "ospath 1182_2593_1674_3070\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000151.JPG\\1647_1869_2136_2337.jpg\n",
      "ospath 1647_1869_2136_2337\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000151.JPG\\1801_794_2387_1351.jpg\n",
      "ospath 1801_794_2387_1351\n",
      "tensor([1])\n",
      "0.02EUR\n",
      "key: output\\L0000151.JPG\\2229_1635_2704_2100.jpg\n",
      "ospath 2229_1635_2704_2100\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000151.JPG\\2494_2776_2939_3219.jpg\n",
      "ospath 2494_2776_2939_3219\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000151.JPG\\2974_782_3576_1351.jpg\n",
      "ospath 2974_782_3576_1351\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000151.JPG\\3316_1792_3817_2269.jpg\n",
      "ospath 3316_1792_3817_2269\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000152.JPG\\2033_1935_2675_2560.jpg\n",
      "ospath 2033_1935_2675_2560\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000152.JPG\\2366_1424_2989_2045.jpg\n",
      "ospath 2366_1424_2989_2045\n",
      "tensor([12])\n",
      "2CHF\n",
      "key: output\\L0000152.JPG\\2534_2371_3254_3080.jpg\n",
      "ospath 2534_2371_3254_3080\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000152.JPG\\2925_2020_3358_2431.jpg\n",
      "ospath 2925_2020_3358_2431\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000153.JPG\\2544_1111_3020_1567.jpg\n",
      "ospath 2544_1111_3020_1567\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000153.JPG\\2784_466_3328_995.jpg\n",
      "ospath 2784_466_3328_995\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000153.JPG\\3125_1318_3770_1944.jpg\n",
      "ospath 3125_1318_3770_1944\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000153.JPG\\3512_590_4087_1149.jpg\n",
      "ospath 3512_590_4087_1149\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000154.JPG\\1715_2644_2176_3078.jpg\n",
      "ospath 1715_2644_2176_3078\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000154.JPG\\1757_1846_2436_2518.jpg\n",
      "ospath 1757_1846_2436_2518\n",
      "tensor([10])\n",
      "1CHF\n",
      "key: output\\L0000154.JPG\\2237_2822_2963_3534.jpg\n",
      "ospath 2237_2822_2963_3534\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000154.JPG\\2390_1995_3263_2883.jpg\n",
      "ospath 2390_1995_3263_2883\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000155.JPG\\2012_1362_2722_2080.jpg\n",
      "ospath 2012_1362_2722_2080\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000155.JPG\\3432_2559_4147_3526.jpg\n",
      "ospath 3432_2559_4147_3526\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000156.JPG\\2194_1457_2839_2084.jpg\n",
      "ospath 2194_1457_2839_2084\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000156.JPG\\3070_2129_3607_2637.jpg\n",
      "ospath 3070_2129_3607_2637\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000156.JPG\\4276_1592_4851_2126.jpg\n",
      "ospath 4276_1592_4851_2126\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000157.JPG\\2244_2139_2989_2850.jpg\n",
      "ospath 2244_2139_2989_2850\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000157.JPG\\2317_1051_3012_1735.jpg\n",
      "ospath 2317_1051_3012_1735\n",
      "tensor([13])\n",
      "2EUR\n",
      "key: output\\L0000157.JPG\\3087_2598_3784_3283.jpg\n",
      "ospath 3087_2598_3784_3283\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000157.JPG\\3107_1488_3697_2040.jpg\n",
      "ospath 3107_1488_3697_2040\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000158.JPG\\1550_1961_2094_2472.jpg\n",
      "ospath 1550_1961_2094_2472\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000158.JPG\\1957_2404_2438_2864.jpg\n",
      "ospath 1957_2404_2438_2864\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000158.JPG\\2539_2752_2969_3171.jpg\n",
      "ospath 2539_2752_2969_3171\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000158.JPG\\2599_1856_3088_2318.jpg\n",
      "ospath 2599_1856_3088_2318\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000158.JPG\\3378_2521_3869_2989.jpg\n",
      "ospath 3378_2521_3869_2989\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000158.JPG\\3500_1119_4105_1698.jpg\n",
      "ospath 3500_1119_4105_1698\n",
      "tensor([14])\n",
      "5CHF\n",
      "key: output\\L0000158.JPG\\4735_1824_5513_2582.jpg\n",
      "ospath 4735_1824_5513_2582\n",
      "tensor([11])\n",
      "1EUR\n",
      "key: output\\L0000159.JPG\\1084_1713_1664_2263.jpg\n",
      "ospath 1084_1713_1664_2263\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000159.JPG\\2623_1772_3064_2204.jpg\n",
      "ospath 2623_1772_3064_2204\n",
      "tensor([4])\n",
      "0.1CHF\n",
      "key: output\\L0000160.JPG\\1943_1292_2436_1765.jpg\n",
      "ospath 1943_1292_2436_1765\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000160.JPG\\2471_1680_2958_2149.jpg\n",
      "ospath 2471_1680_2958_2149\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000160.JPG\\2729_1158_3286_1699.jpg\n",
      "ospath 2729_1158_3286_1699\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000160.JPG\\2793_2193_3216_2614.jpg\n",
      "ospath 2793_2193_3216_2614\n",
      "tensor([6])\n",
      "0.2CHF\n",
      "key: output\\L0000160.JPG\\3160_700_3702_1212.jpg\n",
      "ospath 3160_700_3702_1212\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000160.JPG\\3349_1724_3855_2205.jpg\n",
      "ospath 3349_1724_3855_2205\n",
      "tensor([5])\n",
      "0.1EUR\n",
      "key: output\\L0000160.JPG\\3661_1126_4164_1605.jpg\n",
      "ospath 3661_1126_4164_1605\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000161.JPG\\1702_1680_2300_2266.jpg\n",
      "ospath 1702_1680_2300_2266\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000161.JPG\\2314_2867_2806_3339.jpg\n",
      "ospath 2314_2867_2806_3339\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000161.JPG\\2652_2170_3118_2619.jpg\n",
      "ospath 2652_2170_3118_2619\n",
      "tensor([8])\n",
      "0.5CHF\n",
      "key: output\\L0000161.JPG\\2766_1403_3217_1837.jpg\n",
      "ospath 2766_1403_3217_1837\n",
      "tensor([3])\n",
      "0.05EUR\n",
      "key: output\\L0000161.JPG\\3383_2090_3849_2546.jpg\n",
      "ospath 3383_2090_3849_2546\n",
      "tensor([7])\n",
      "0.2EUR\n",
      "key: output\\L0000161.JPG\\3522_1390_4080_1924.jpg\n",
      "ospath 3522_1390_4080_1924\n",
      "tensor([9])\n",
      "0.5EUR\n",
      "key: output\\L0000161.JPG\\3834_2364_4429_2941.jpg\n",
      "ospath 3834_2364_4429_2941\n",
      "tensor([15])\n",
      "OOD\n",
      "key: output\\L0000161.JPG\\4393_1099_5076_1761.jpg\n",
      "ospath 4393_1099_5076_1761\n",
      "tensor([2])\n",
      "0.05CHF\n",
      "key: output\\L0000161.JPG\\4544_2106_4980_2526.jpg\n",
      "ospath 4544_2106_4980_2526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the best model (last saved model)\n",
    "best_model_path = f'best_model_epoch_{7}.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Generate the submission\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for key in test_data:\n",
    "    labels_count = {label: 0 for label in class_to_idx.keys()}\n",
    "    image = transform(Image.open(key).convert(\"RGB\")).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        print(predicted_class)\n",
    "        predicted_label = list(class_to_idx.keys())[predicted_class.item()]\n",
    "        print(predicted_label)\n",
    "        labels_count[predicted_label] += 1\n",
    "    \n",
    "    row_to_write = submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]]\n",
    "    print('key:',key)\n",
    "    print('ospath', os.path.basename(key)[:-4])\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]] = row_to_write\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 16)  # Assuming 16 classes\n",
    "\n",
    "# Training settings\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "test_images_path = \"output\"\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(os.listdir(images_classes_path))}\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for label in os.listdir(images_classes_path):\n",
    "    class_path = os.path.join(images_classes_path, label)\n",
    "    for img in os.listdir(class_path):\n",
    "        train_data.append(os.path.join(class_path, img))\n",
    "        train_labels.append(class_to_idx[label])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(train_data, train_labels, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "for folder in os.listdir(test_images_path):\n",
    "    folder_path = os.path.join(test_images_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img in os.listdir(folder_path):\n",
    "            test_data.append(os.path.join(folder_path, img))\n",
    "\n",
    "test_dataset = CustomTestDataset(test_data, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_to_idx))  # Number of classes based on the training data\n",
    "\n",
    "# Training settings\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Training Accuracy: {train_accuracy}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test data\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Generate the submission\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for key in test_data:\n",
    "    labels_count = {label: 0 for label in class_to_idx.keys()}\n",
    "    image = transform_val(Image.open(key).convert(\"RGB\")).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        predicted_label = list(class_to_idx.keys())[predicted_class.item()]\n",
    "        labels_count[predicted_label] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]]\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]] = row_to_write\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "# Generate the submission\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(test_data)\n",
    "for key in test_data:\n",
    "    print('test')\n",
    "    labels_count = {label: 0 for label in class_to_idx.keys()}\n",
    "    image = transform_val(Image.open(key).convert(\"RGB\")).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        predicted_label = list(class_to_idx.keys())[predicted_class.item()]\n",
    "        labels_count[predicted_label] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]]\n",
    "    print(os.path.basename(key)[:-4])\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == os.path.basename(key)[:-4]] = row_to_write\n",
    "\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "test_images_path = \"output\"\n",
    "\n",
    "# Image transformations with data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(os.listdir(images_classes_path))}\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for label in os.listdir(images_classes_path):\n",
    "    class_path = os.path.join(images_classes_path, label)\n",
    "    for img in os.listdir(class_path):\n",
    "        train_data.append(os.path.join(class_path, img))\n",
    "        train_labels.append(class_to_idx[label])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(train_data, train_labels, transform=transform_train)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Update the validation dataset to use transform_val\n",
    "val_dataset.dataset.transform = transform_val\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "for folder in os.listdir(test_images_path):\n",
    "    folder_path = os.path.join(test_images_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img in os.listdir(folder_path):\n",
    "            test_data.append(os.path.join(folder_path, img))\n",
    "\n",
    "test_dataset = CustomTestDataset(test_data, transform=transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model with dropout regularization\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = CustomResNet(len(class_to_idx))\n",
    "\n",
    "# Training settings\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Training loop with validation and early stopping\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Training Accuracy: {train_accuracy}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy}%\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Input\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.optimizers import SGD, Adam\n",
    "\n",
    "# from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Customizing InceptionV3 Base Model\n",
    "The pre-trained InceptionV3 model is loaded with weights from ImageNet, excluding the top fully connected layers. This allows us to customize the output layers for the coin classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "# from keras.applications.xception import Xception\n",
    "base_model = InceptionV3(include_top=False, # Since we will create our own\n",
    "                    weights='imagenet', \n",
    "                    input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Custom Output Layers\n",
    "\n",
    "The ouput of the IncdeptionV3 base model is passed through a series of custom layers:\n",
    "\n",
    "**Flatten:** Converst the feature maps to a 1D feature vector\\\n",
    "**BatchNormalization:** Normalizes the activations to improve training stability\\\n",
    "**Dense (512 units):** Fully connected layer with ReLy activation\\\n",
    "**Dropout (50%):** Regularization technique to prevent overfitting\\\n",
    "**Dense (num_classes):** Output layer with softmax actiavtion for the final coin classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = inceptionv3_base.output\n",
    "# # x = Flatten()(x)\n",
    "# # x = BatchNormalization()(x)\n",
    "# # x = Dense(512, activation='relu')(x)\n",
    "# # x = Dropout(rate = .5)(x)\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing of Base layers\n",
    "\n",
    "This block freezes all the layer except the last 7 in order to retrain the pre-trained weights during initial training. The last 7 layers remain trainable to fine-tune them for coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing in order to only train the last 7 layers\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "The model is compiled with the Adam optimizer, which is chosen for its efficiency and adaptability. The learning rate is set to 0.0001. The loss function is categorical cross-entropy, which is suitibla for the multi-class coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "# optimizer = SGD(learning_rate=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Callbacks\n",
    "Three callbacks are defined to enhance the training process:\n",
    "\n",
    "**ModelCheckpoint:** Saves the model with the best validation accuracy\\\n",
    "**TensorBoard:** Logs training metrics for visualization.\\\n",
    "**EarlyStopping:** Stops training if the validation does not improve for 6 consecutive epochs, restoring the best weights.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.keras',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "my_callbacks = [checkpoint, tensorboard_callback, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=30,\n",
    "          callbacks=my_callbacks)\n",
    "print('Training done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label = \"\"\n",
    "# model.save(model_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over all the layers in the model and sets the trainable attribute to True. This enables all layers to be updated during the training process. Fine-tuning the entire model can help improve performance, especially if the initial training with frozen layers led to suboptimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.fit(train_generator, \n",
    "                                validation_data=val_generator, \n",
    "                                epochs=30,\n",
    "                                callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracies and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = np.array(history.history['accuracy'])\n",
    "    val_acc = np.array(history.history['val_accuracy'])\n",
    "    loss = np.array(history.history['loss'])\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'history' is the History object returned from model.fit\n",
    "plot_history(history)\n",
    "plot_history(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "os.system('pkill -f \"tensorboard\"')\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir\", \"logs/fit\"])\n",
    "time.sleep(5)\n",
    "display(HTML(f'<a href=\"http://localhost:6006\" target=\"_blank\">Open TensorBoard</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on given test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(path, verbose=False, show_image=False):\n",
    "    # Load and display the image\n",
    "    img = mpimg.imread(path)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = Image.open(path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dimensions to fit model input\n",
    "\n",
    "    # Predict the probabilities\n",
    "    probabilities = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get class labels\n",
    "    class_labels = {v: k for k, v in val_generator.class_indices.items()}\n",
    "\n",
    "    # Prepare data for a nice display\n",
    "    sorted_indices = np.argsort(probabilities[0])[::-1]\n",
    "    results = [(class_labels[i], probabilities[0][i]) for i in sorted_indices]\n",
    "\n",
    "    # Print sorted probabilities and their corresponding class labels\n",
    "    if verbose:\n",
    "        print(\"Class\".ljust(15), \"Probability\")\n",
    "        print(\"-\" * 30)\n",
    "        for label, prob in results:\n",
    "            print(f\"{label.ljust(15)} : {prob:.4f}\")\n",
    "\n",
    "    # Return the class label with the highest probability\n",
    "    return results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing submission\n",
    "import pandas as pd\n",
    "\n",
    "# This will used as a template to build the submission.\n",
    "if not os.path.exists(\"sample_submission.csv\"):\n",
    "    raise FileNotFoundError(\"sample_submission.csv not found!\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.head()\n",
    "\n",
    "# Perform prediction on all test images.\n",
    "for key in images_dict:\n",
    "    labels_count = {key: 0 for key in image_classes_list}\n",
    "    for coin_path in images_dict[key]:\n",
    "        predicted_class = predict_images(coin_path, verbose=True, show_image=True)\n",
    "        labels_count[predicted_class] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == key[:-4]]\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == key[:-4]] = row_to_write\n",
    "\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "Y_pred = model.predict(val_generator, 1167//32+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cfm = confusion_matrix(val_generator.classes, y_pred)\n",
    "cfm = np.around(cfm.astype('float')/cfm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "classes = ['0.1CHF', '0.1EUR', '0.01EUR', '0.2CHF', '0.2EUR', '0.02EUR', '0.5CHF', '0.05CHF', '0.5EUR', '0.05EUR', '1CHF', '1EUR', '2CHF', '2EUR', '5CHF', 'OOD']\n",
    "cfm_pd = pd.DataFrame(cfm, index = classes, columns = classes)\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cfm_pd, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
