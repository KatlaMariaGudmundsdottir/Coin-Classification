{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling training data\n",
    "Given the relatively small size of our training data, we manually labelled the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\katla\\miniconda3\\envs\\iapr\\lib\\site-packages (10.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install imaging library Pillow\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data from Kaggle\n",
    "unlabeled_data_folder = os.path.join(\"Data\", \"train\")\n",
    "\n",
    "# XML files created by LabelIMG\n",
    "data_labels_folder = os.path.join(\"Labelled_Training_Data\")\n",
    "\n",
    "# Cropped images sorted by coin type\n",
    "extracted_data_folder = os.path.join(\"Extracted_Training_Data\")\n",
    "os.makedirs(extracted_data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(input_image_path, output_folder, object_list):\n",
    "    # Open the input image\n",
    "    with Image.open(input_image_path) as img:\n",
    "        for index, object in enumerate(object_list):\n",
    "            # Crop the image\n",
    "            name = object['name']\n",
    "            cropped_img = img.crop((int(object['xmin']), int(object['ymin']), int(object['xmax']), int(object['ymax'])))\n",
    "            output_coin_type = os.path.join(output_folder, name)\n",
    "            # Ensure the output folder exists\n",
    "            os.makedirs(output_coin_type, exist_ok=True)\n",
    "            \n",
    "            # Construct the output image path, weird naming to avoid duplicates from same image, index placed before filetype\n",
    "            image_name = os.path.basename(input_image_path[:-4]) + '_' + str(index) + '.JPG'\n",
    "            output_image_path = os.path.join(output_coin_type, image_name)\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_img.save(output_image_path)\n",
    "            print(f'Cropped image saved to: {output_image_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_file_content(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    object_list = []\n",
    "    for object in root.findall('object'):\n",
    "        d = dict()\n",
    "        d['name'] = object.findtext('name')\n",
    "        bounding_box = object.find('bndbox')\n",
    "        d['xmin'] = bounding_box.findtext('xmin')\n",
    "        d['ymin'] = bounding_box.findtext('ymin')\n",
    "        d['xmax'] = bounding_box.findtext('xmax')\n",
    "        d['ymax'] = bounding_box.findtext('ymax')\n",
    "        object_list.append(d)\n",
    "\n",
    "    return object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010277_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010277_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010277_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010277_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010277_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010279_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010279_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010279_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010279_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010281_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010281_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010281_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010281_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010281_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010281_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010281_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010281_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010283_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010283_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010283_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010283_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010283_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010283_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010287_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010287_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010287_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010287_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010287_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010287_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010287_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010287_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010288_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010288_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010288_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010288_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010288_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010294_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010294_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010294_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010294_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010294_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010297_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010297_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010297_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010297_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010298_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010298_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010298_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010298_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010298_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010300_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010300_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010300_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010300_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010300_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010308_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010308_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010308_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010308_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010308_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010308_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010308_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010308_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010308_8.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010310_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010310_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010310_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010310_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010310_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010310_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010310_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010311_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010311_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010311_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010311_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010311_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010311_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010318_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010318_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010318_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010318_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010318_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010318_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010318_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010318_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010318_8.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010318_9.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010318_10.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010321_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010321_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010321_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010321_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010321_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010321_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010321_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010321_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010321_8.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010323_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010323_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010323_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010323_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010323_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010323_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010323_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010323_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010325_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010325_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010325_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010325_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010325_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010328_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010328_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010328_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010328_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010331_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010331_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010331_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010331_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010331_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010331_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010331_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010331_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010335_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010335_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010335_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010335_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010341_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010341_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010341_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010341_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010345_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010345_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010345_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010349_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010349_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010349_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010349_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010352_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010352_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010352_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010353_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010353_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010353_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010356_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010356_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010356_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010356_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010356_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010356_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010361_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010365_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010365_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010365_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010367_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010367_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010367_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010367_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010367_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010367_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010367_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010367_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010369_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010369_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010369_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010369_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010369_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010369_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010369_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010369_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010369_8.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010370_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010370_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010370_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010370_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010370_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010370_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010370_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010370_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010370_8.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010370_9.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010370_10.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010370_11.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010370_12.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010370_13.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010370_14.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010370_15.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010370_16.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010373_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010373_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010373_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010375_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010375_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010375_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010375_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010377_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010377_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010377_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010377_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010378_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010378_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010378_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010378_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010378_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010382_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010383_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010383_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010383_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010383_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010383_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010388_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010388_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010388_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010388_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010390_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010390_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010390_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010390_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010391_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010391_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010391_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010395_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010395_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010395_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010395_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010405_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010405_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010405_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010405_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010406_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010406_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010406_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010406_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010406_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010408_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010408_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010410_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010410_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010410_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010413_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010413_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010413_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010413_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010418_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010418_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010419_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010419_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010419_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010421_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010421_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010421_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010421_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010422_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010422_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010422_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010424_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010424_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010424_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010424_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010424_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010434_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010434_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010434_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010434_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010436_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010440_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010440_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010440_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010440_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010440_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010440_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010440_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010440_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010445_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010445_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010445_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010446_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010446_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010446_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010446_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010446_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010450_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010450_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010450_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010450_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010450_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010454_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010454_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010462_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010462_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010462_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010462_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010468_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010468_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010470_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010470_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010470_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010470_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010475_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010475_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010475_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010476_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010476_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010476_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010476_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010476_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010476_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010476_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010477_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010477_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010477_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010477_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010477_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010478_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010478_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010478_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010478_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010478_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2CHF\\L1010479_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010479_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010479_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05CHF\\L1010479_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010479_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010483_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010483_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010483_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010483_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010485_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010485_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010485_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010485_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010486_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010486_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5CHF\\L1010486_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010486_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1EUR\\L1010486_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010486_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010487_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010487_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010487_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010487_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010487_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010491_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010491_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010491_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010491_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010491_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010491_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010491_6.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010491_7.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010500_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010500_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2CHF\\L1010500_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010500_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.01EUR\\L1010500_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1CHF\\L1010500_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010501_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010503_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010503_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010503_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010503_3.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010503_4.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.05EUR\\L1010503_5.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.02EUR\\L1010506_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010506_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010506_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.2EUR\\L1010510_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010510_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010510_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.1CHF\\L1010515_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010515_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010515_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010517_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010517_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010517_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010521_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010521_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010521_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\2EUR\\L1010523_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010523_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\1EUR\\L1010523_2.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\0.5EUR\\L1010524_0.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\OOD\\L1010524_1.JPG\n",
      "Cropped image saved to: Extracted_Training_Data\\5CHF\\L1010524_2.JPG\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(unlabeled_data_folder):\n",
    "    label_folder = os.path.join(data_labels_folder, folder)\n",
    "    folder = os.path.join(unlabeled_data_folder, folder)\n",
    "    for image in os.listdir(folder):\n",
    "        # Find corresponding XML_file\n",
    "        for label_file in os.listdir(label_folder):\n",
    "            if (image[:-4] in label_file):\n",
    "                label_filepath = os.path.join(label_folder, label_file)\n",
    "                image_filepath = os.path.join(folder, image)\n",
    "                object_list = read_xml_file_content(label_filepath)\n",
    "\n",
    "                crop_and_save_image(image_filepath, extracted_data_folder, object_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt with classical segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object localization with transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16 classes\n",
      "total 0 test images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "image_classes_list = os.listdir(images_classes_path)\n",
    "num_classes = len(image_classes_list)\n",
    "print(\"total {} classes\".format(num_classes))\n",
    "\n",
    "test_images_path = \"output\"\n",
    "images_dict = {}\n",
    "image_name_list = os.listdir(test_images_path)\n",
    "for image_folder in image_name_list:\n",
    "    images_dict[image_folder] = []\n",
    "    images_path = os.path.join(test_images_path, image_folder)\n",
    "    # Check if the path is a directory\n",
    "    if not os.path.isdir(images_path):\n",
    "        continue\n",
    "    for image in os.listdir(images_path):\n",
    "        path_to_coin_image = os.path.join(images_path, image)\n",
    "        images_dict[image_folder].append(path_to_coin_image)\n",
    "num_images = len(image_name_list)\n",
    "print(\"total {} test images\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "!rm -rf ./logs/\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "This section imports the necessary libraries for image processing and augmentation `ImageDataGenerator`and related functions from `tensorflow.keras.preprocessing.image` are used for loading and augmenting images. The `os` module handles directory operation, and `tqdm` provides a progress bar for the augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ImageDataGenerator\n",
    "\n",
    "An instance of `ImageDataGenerator`is created with specific augmentation parameters. The purpose of this data augmentation is to artificially expand the training dataset by generating new, altered versions of the existing coin images. This is particularly beneficial since our original training dataset is small, so augmentation will help to prevent overfitting and improve the model's ability to generalize to new data. \n",
    "\n",
    "For this task, the chosen augmentations are Rescaling, Rotation, Brightness Adjustment and Zoom. Rescaling of pixel values from [0,255] range to [0,1] is done to facilitate faster convergence during training. We apply a random rotation of up to 90 degrees as well, this is to make the model robust to changes in orientation, improving its ability to recognize coins regardless of rotation. Since lighting conditions vary slightly in the training data a brightness adjustment is also implemented. Finally a slight zoom in is implemented since the segmentation algorithm can cut of some edges of coins and therefore preparing the model for that in the training data makes generalize better to the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the directories to save the augmented images\n",
    "augmented_data_path = 'path_to_save_augmented_images'\n",
    "validation_data_path = 'path_to_save_validation_images'\n",
    "os.makedirs(augmented_data_path, exist_ok=True)\n",
    "os.makedirs(validation_data_path, exist_ok=True)\n",
    "\n",
    "crazy_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    shear_range=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment and Save Images\n",
    "This loop iterates over each class directory and each image within that directory. Each image is loaded and converted to an array. The `datagen.flow` function generates augmented images which are saved to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  98%|█████████▊| 905/919 [00:58<00:00, 15.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "# Set the number of augmented images to generate per original image\n",
    "num_augmented_images = 3\n",
    "\n",
    "# Load the original training images\n",
    "original_data_path = images_classes_path\n",
    "\n",
    "# Calculate the total number of steps for the progress bar\n",
    "all_files = sum([len(files) for r, d, files in os.walk(original_data_path)])\n",
    "total_steps = int(all_files * 0.7 * num_augmented_images + all_files * 0.3)\n",
    "\n",
    "# Initialize the progress bar\n",
    "pbar = tqdm(total=total_steps, desc=\"Processing images\")\n",
    "\n",
    "# Split the original images into 70% for augmentation and 30% for validation\n",
    "for class_dir in os.listdir(original_data_path):\n",
    "    class_path = os.path.join(original_data_path, class_dir)\n",
    "    augmented_class_path = os.path.join(augmented_data_path, class_dir)\n",
    "    validation_class_path = os.path.join(validation_data_path, class_dir)\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "    os.makedirs(validation_class_path, exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "    split_index = int(len(images) * 0.7)\n",
    "    \n",
    "    images_to_augment = images[:split_index]\n",
    "    images_for_validation = images[split_index:]\n",
    "    \n",
    "    # Save validation images to the validation data path without augmentation\n",
    "    for img_name in images_for_validation:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(validation_class_path, img_name)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Augment 70% of the images\n",
    "    for img_name in images_to_augment:\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "        # Generate and save augmented images\n",
    "        i = 0\n",
    "        for batch in crazy_datagen.flow(x, batch_size=1, save_to_dir=augmented_class_path, save_prefix='aug', save_format='jpeg'):\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data generators for Training and validation\n",
    "Two data generators are defined: one for training and one for validation. The images are rescaled, and the dataset is split into training and validation subsets. The images are resized to 224x224 picels, and a batch size of 32 is used. Different batch sizes were tested, but 32 was found to be a suitable batch size because of the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2562 images belonging to 16 classes.\n",
      "Found 274 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    augmented_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    validation_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Test data generator remains the same\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     test_images_path,\n",
    "#     target_size=(224, 224),\n",
    "#     shuffle=False,\n",
    "#     batch_size=1,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "\n",
    "# Get filenames and number of samples\n",
    "# filenames = test_generator.filenames\n",
    "# nb_samples = len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x_shape: (32, 224, 224, 3), Batch y_shape: (32, 16)\n",
      "(32, 224, 224, 3) (32, 16)\n",
      "(32, 224, 224, 3) (32, 16)\n",
      "Steps per epoch: 80\n",
      "Validation steps: 8\n"
     ]
    }
   ],
   "source": [
    "# Print a batch for verification\n",
    "x_batch, y_batch = next(train_generator)\n",
    "print(f\"Batch x_shape: {x_batch.shape}, Batch y_shape: {y_batch.shape}\")\n",
    "\n",
    "# Verify the data generators\n",
    "train_batch = next(iter(train_generator))\n",
    "val_batch = next(iter(val_generator))\n",
    "print(train_batch[0].shape, train_batch[1].shape)\n",
    "print(val_batch[0].shape, val_batch[1].shape)\n",
    "\n",
    "print(f\"Steps per epoch: {train_generator.samples // train_generator.batch_size}\")\n",
    "print(f\"Validation steps: {val_generator.samples // val_generator.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration using Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Customizing InceptionV3 Base Model\n",
    "The pre-trained InceptionV3 model is loaded with weights from ImageNet, excluding the top fully connected layers. This allows us to customize the output layers for the coin classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "# from keras.applications.xception import Xception\n",
    "base_model = InceptionV3(include_top=False, # Since we will create our own\n",
    "                    weights='imagenet', \n",
    "                    input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Custom Output Layers\n",
    "\n",
    "The ouput of the IncdeptionV3 base model is passed through a series of custom layers:\n",
    "\n",
    "**Flatten:** Converst the feature maps to a 1D feature vector\\\n",
    "**BatchNormalization:** Normalizes the activations to improve training stability\\\n",
    "**Dense (512 units):** Fully connected layer with ReLy activation\\\n",
    "**Dropout (50%):** Regularization technique to prevent overfitting\\\n",
    "**Dense (num_classes):** Output layer with softmax actiavtion for the final coin classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = inceptionv3_base.output\n",
    "# # x = Flatten()(x)\n",
    "# # x = BatchNormalization()(x)\n",
    "# # x = Dense(512, activation='relu')(x)\n",
    "# # x = Dropout(rate = .5)(x)\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing of Base layers\n",
    "\n",
    "This block freezes all the layer except the last 7 in order to retrain the pre-trained weights during initial training. The last 7 layers remain trainable to fine-tune them for coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing in order to only train the last 7 layers\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "The model is compiled with the Adam optimizer, which is chosen for its efficiency and adaptability. The learning rate is set to 0.0001. The loss function is categorical cross-entropy, which is suitibla for the multi-class coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "# optimizer = SGD(learning_rate=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Callbacks\n",
    "Three callbacks are defined to enhance the training process:\n",
    "\n",
    "**ModelCheckpoint:** Saves the model with the best validation accuracy\\\n",
    "**TensorBoard:** Logs training metrics for visualization.\\\n",
    "**EarlyStopping:** Stops training if the validation does not improve for 6 consecutive epochs, restoring the best weights.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.keras',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "my_callbacks = [checkpoint, tensorboard_callback, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 5s/step - accuracy: 0.1519 - loss: 5.3837 - val_accuracy: 0.4885 - val_loss: 1.5666\n",
      "Epoch 2/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 7s/step - accuracy: 0.6017 - loss: 1.1679 - val_accuracy: 0.6107 - val_loss: 1.1468\n",
      "Epoch 3/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 5s/step - accuracy: 0.8202 - loss: 0.5376 - val_accuracy: 0.6336 - val_loss: 1.1122\n",
      "Epoch 4/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 6s/step - accuracy: 0.8908 - loss: 0.3408 - val_accuracy: 0.6336 - val_loss: 1.0378\n",
      "Epoch 5/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 5s/step - accuracy: 0.8930 - loss: 0.3145 - val_accuracy: 0.6870 - val_loss: 0.9315\n",
      "Epoch 6/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 5s/step - accuracy: 0.9524 - loss: 0.1667 - val_accuracy: 0.6870 - val_loss: 0.9934\n",
      "Epoch 7/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 5s/step - accuracy: 0.9678 - loss: 0.1257 - val_accuracy: 0.6870 - val_loss: 1.0160\n",
      "Epoch 8/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 4s/step - accuracy: 0.9855 - loss: 0.0824 - val_accuracy: 0.6947 - val_loss: 0.9979\n",
      "Epoch 9/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4s/step - accuracy: 0.9850 - loss: 0.0619 - val_accuracy: 0.6794 - val_loss: 1.0043\n",
      "Epoch 10/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 4s/step - accuracy: 0.9879 - loss: 0.0608 - val_accuracy: 0.6947 - val_loss: 0.9683\n",
      "Epoch 11/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 4s/step - accuracy: 0.9942 - loss: 0.0423 - val_accuracy: 0.7099 - val_loss: 0.9240\n",
      "Epoch 12/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 5s/step - accuracy: 0.9940 - loss: 0.0278 - val_accuracy: 0.7405 - val_loss: 0.8960\n",
      "Epoch 13/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 4s/step - accuracy: 0.9940 - loss: 0.0309 - val_accuracy: 0.7023 - val_loss: 1.0533\n",
      "Epoch 14/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4s/step - accuracy: 0.9972 - loss: 0.0195 - val_accuracy: 0.7176 - val_loss: 0.9047\n",
      "Epoch 15/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 5s/step - accuracy: 0.9967 - loss: 0.0158 - val_accuracy: 0.7023 - val_loss: 1.0176\n",
      "Epoch 16/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.7252 - val_loss: 0.9997\n",
      "Epoch 17/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 4s/step - accuracy: 0.9978 - loss: 0.0114 - val_accuracy: 0.7405 - val_loss: 0.9082\n",
      "Epoch 18/30\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 5s/step - accuracy: 0.9993 - loss: 0.0160 - val_accuracy: 0.7176 - val_loss: 0.9181\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=30,\n",
    "          callbacks=my_callbacks)\n",
    "print('Training done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = \"\"\n",
    "model.save(model_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.fit(train_generator, \n",
    "                                validation_data=val_generator, \n",
    "                                epochs=15,\n",
    "                                callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracies and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = np.array(history.history['accuracy'])\n",
    "    val_acc = np.array(history.history['val_accuracy'])\n",
    "    loss = np.array(history.history['loss'])\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'history' is the History object returned from model.fit\n",
    "plot_history(history)\n",
    "plot_history(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "os.system('pkill -f \"tensorboard\"')\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir\", \"logs/fit\"])\n",
    "time.sleep(5)\n",
    "display(HTML(f'<a href=\"http://localhost:6006\" target=\"_blank\">Open TensorBoard</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on given test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(path, verbose=False, show_image=False):\n",
    "    # Load and display the image\n",
    "    img = mpimg.imread(path)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = Image.open(path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dimensions to fit model input\n",
    "\n",
    "    # Predict the probabilities\n",
    "    probabilities = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get class labels\n",
    "    class_labels = {v: k for k, v in val_generator.class_indices.items()}\n",
    "\n",
    "    # Prepare data for a nice display\n",
    "    sorted_indices = np.argsort(probabilities[0])[::-1]\n",
    "    results = [(class_labels[i], probabilities[0][i]) for i in sorted_indices]\n",
    "\n",
    "    # Print sorted probabilities and their corresponding class labels\n",
    "    if verbose:\n",
    "        print(\"Class\".ljust(15), \"Probability\")\n",
    "        print(\"-\" * 30)\n",
    "        for label, prob in results:\n",
    "            print(f\"{label.ljust(15)} : {prob:.4f}\")\n",
    "\n",
    "    # Return the class label with the highest probability\n",
    "    return results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing submission\n",
    "import pandas as pd\n",
    "\n",
    "# This will used as a template to build the submission.\n",
    "if not os.path.exists(\"sample_submission.csv\"):\n",
    "    raise FileNotFoundError(\"sample_submission.csv not found!\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.head()\n",
    "\n",
    "# Perform prediction on all test images.\n",
    "for key in images_dict:\n",
    "    labels_count = {key: 0 for key in image_classes_list}\n",
    "    for coin_path in images_dict[key]:\n",
    "        predicted_class = predict_images(coin_path)\n",
    "        labels_count[predicted_class] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == key[:-4]]\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == key[:-4]] = row_to_write\n",
    "\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "Y_pred = model.predict(val_generator, 1167//32+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cfm = confusion_matrix(val_generator.classes, y_pred)\n",
    "cfm = np.around(cfm.astype('float')/cfm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "classes = ['0.1CHF', '0.1EUR', '0.01EUR', '0.2CHF', '0.2EUR', '0.02EUR', '0.5CHF', '0.05CHF', '0.5EUR', '0.05EUR', '1CHF', '1EUR', '2CHF', '2EUR', '5CHF', 'OOD']\n",
    "cfm_pd = pd.DataFrame(cfm, index = classes, columns = classes)\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cfm_pd, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
