{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datasets\n",
    "The dataset is loaded from the specified directory, and the total number of classes is determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16 classes\n",
      "total 0 test images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "image_classes_list = os.listdir(images_classes_path)\n",
    "num_classes = len(image_classes_list)\n",
    "print(\"total {} classes\".format(num_classes))\n",
    "\n",
    "test_images_path = \"output\"\n",
    "images_dict = {}\n",
    "image_name_list = os.listdir(test_images_path)\n",
    "for image_folder in image_name_list:\n",
    "    images_dict[image_folder] = []\n",
    "    images_path = os.path.join(test_images_path, image_folder)\n",
    "    for image in os.listdir(images_path):\n",
    "        path_to_coin_image = os.path.join(images_path, image)\n",
    "        images_dict[image_folder].append(path_to_coin_image)\n",
    "num_images = len(image_name_list)\n",
    "print(\"total {} test images\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "!rm -rf ./logs/\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "# Create the directories to save the augmented images\n",
    "augmented_data_path = 'path_to_save_augmented_images'\n",
    "os.makedirs(augmented_data_path, exist_ok=True)\n",
    "\n",
    "crazy_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    zoom_range = [1, 1.2],   \n",
    ")\n",
    "\n",
    "# Set the number of augmented images to generate per original image\n",
    "num_augmented_images = 10\n",
    "\n",
    "# Load the original images\n",
    "original_data_path = images_classes_path\n",
    "\n",
    "# Calculate the total number of files\n",
    "all_files = sum([len(files) for r, d, files in os.walk(original_data_path)])\n",
    "pbar = tqdm(total=all_files * num_augmented_images, desc=\"Augmenting images\")\n",
    "\n",
    "for class_dir in os.listdir(original_data_path):\n",
    "    class_path = os.path.join(original_data_path, class_dir)\n",
    "    save_class_path = os.path.join(augmented_data_path, class_dir)\n",
    "    os.makedirs(save_class_path, exist_ok=True)\n",
    "    \n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "        # Generate and save augmented images\n",
    "        i = 0\n",
    "        for batch in crazy_datagen.flow(x, batch_size=1, save_to_dir=save_class_path, save_prefix='aug', save_format='jpeg'):\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    augmented_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    augmented_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# # Test data generator remains the same\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     test_images_path,\n",
    "#     target_size=(224, 224),\n",
    "#     shuffle=False,\n",
    "#     batch_size=1,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "\n",
    "# # Get filenames and number of samples\n",
    "# filenames = test_generator.filenames\n",
    "# nb_samples = len(filenames)\n",
    "\n",
    "# Print a batch for verification\n",
    "x_batch, y_batch = next(train_generator)\n",
    "print(f\"Batch x_shape: {x_batch.shape}, Batch y_shape: {y_batch.shape}\")\n",
    "\n",
    "# Verify the data generators\n",
    "train_batch = next(iter(train_generator))\n",
    "val_batch = next(iter(val_generator))\n",
    "print(train_batch[0].shape, train_batch[1].shape)\n",
    "print(val_batch[0].shape, val_batch[1].shape)\n",
    "\n",
    "print(f\"Steps per epoch: {train_generator.samples // train_generator.batch_size}\")\n",
    "print(f\"Validation steps: {val_generator.samples // val_generator.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_base = InceptionV3(include_top=False, # Since we will create our own\n",
    "                    weights='imagenet', \n",
    "                    input_shape=(224, 224, 3))\n",
    "inceptionv3_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inceptionv3_base.output\n",
    "x = Flatten()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(rate = .5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inceptionv3_base.inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freezing in order to only train the last 7 layers\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.keras',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "my_callbacks = [checkpoint, tensorboard_callback, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=7,\n",
    "          callbacks=my_callbacks)\n",
    "print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = model.fit(train_generator, \n",
    "                                # validation_data=val_generator, \n",
    "                                # epochs=10,\n",
    "                                # callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = np.array(history.history['accuracy'])\n",
    "    val_acc = np.array(history.history['val_accuracy'])\n",
    "    loss = np.array(history.history['loss'])\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'history' is the History object returned from model.fit\n",
    "plot_history(history)\n",
    "plot_history(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "os.system('pkill -f \"tensorboard\"')\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir\", \"logs/fit\"])\n",
    "time.sleep(5)\n",
    "display(HTML(f'<a href=\"http://localhost:6006\" target=\"_blank\">Open TensorBoard</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on given test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(path, verbose=False, show_image=False):\n",
    "    # Load and display the image\n",
    "    img = mpimg.imread(path)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = Image.open(path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dimensions to fit model input\n",
    "\n",
    "    # Predict the probabilities\n",
    "    probabilities = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get class labels\n",
    "    class_labels = {v: k for k, v in val_generator.class_indices.items()}\n",
    "\n",
    "    # Prepare data for a nice display\n",
    "    sorted_indices = np.argsort(probabilities[0])[::-1]\n",
    "    results = [(class_labels[i], probabilities[0][i]) for i in sorted_indices]\n",
    "\n",
    "    # Print sorted probabilities and their corresponding class labels\n",
    "    if verbose:\n",
    "        print(\"Class\".ljust(15), \"Probability\")\n",
    "        print(\"-\" * 30)\n",
    "        for label, prob in results:\n",
    "            print(f\"{label.ljust(15)} : {prob:.4f}\")\n",
    "\n",
    "    # Return the class label with the highest probability\n",
    "    return results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing submission\n",
    "import pandas as pd\n",
    "\n",
    "# This will used as a template to build the submission.\n",
    "if not os.path.exists(\"sample_submission.csv\"):\n",
    "    raise FileNotFoundError(\"sample_submission.csv not found!\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.head()\n",
    "\n",
    "# Perform prediction on all test images.\n",
    "for key in images_dict:\n",
    "    labels_count = {key: 0 for key in image_classes_list}\n",
    "    for coin_path in images_dict[key]:\n",
    "        predicted_class = predict_images(coin_path)\n",
    "        labels_count[predicted_class] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == key[:-4]]\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == key[:-4]] = row_to_write\n",
    "\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confustion matrix on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "Y_pred = model.predict(val_generator, 1167//32+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cfm = confusion_matrix(val_generator.classes, y_pred)\n",
    "cfm = np.around(cfm.astype('float')/cfm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "classes = ['0.1CHF', '0.1EUR', '0.01EUR', '0.2CHF', '0.2EUR', '0.02EUR', '0.5CHF', '0.05CHF', '0.5EUR', '0.05EUR', '1CHF', '1EUR', '2CHF', '2EUR', '5CHF', 'OOD']\n",
    "cfm_pd = pd.DataFrame(cfm, index = classes, columns = classes)\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cfm_pd, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
