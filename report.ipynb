{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling training data\n",
    "Given the relatively small size of our training data, we manually labelled the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imaging library Pillow\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data from Kaggle\n",
    "unlabeled_data_folder = os.path.join(\"Data\", \"train\")\n",
    "\n",
    "# XML files created by LabelIMG\n",
    "data_labels_folder = os.path.join(\"Labelled_Training_Data\")\n",
    "\n",
    "# Cropped images sorted by coin type\n",
    "extracted_data_folder = os.path.join(\"Extracted_Training_Data\")\n",
    "os.makedirs(extracted_data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(input_image_path, output_folder, object_list):\n",
    "    # Open the input image\n",
    "    with Image.open(input_image_path) as img:\n",
    "        for index, object in enumerate(object_list):\n",
    "            # Crop the image\n",
    "            name = object['name']\n",
    "            cropped_img = img.crop((int(object['xmin']), int(object['ymin']), int(object['xmax']), int(object['ymax'])))\n",
    "            output_coin_type = os.path.join(output_folder, name)\n",
    "            # Ensure the output folder exists\n",
    "            os.makedirs(output_coin_type, exist_ok=True)\n",
    "            \n",
    "            # Construct the output image path, weird naming to avoid duplicates from same image, index placed before filetype\n",
    "            image_name = os.path.basename(input_image_path[:-4]) + '_' + str(index) + '.JPG'\n",
    "            output_image_path = os.path.join(output_coin_type, image_name)\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_img.save(output_image_path)\n",
    "            print(f'Cropped image saved to: {output_image_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_file_content(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    object_list = []\n",
    "    for object in root.findall('object'):\n",
    "        d = dict()\n",
    "        d['name'] = object.findtext('name')\n",
    "        bounding_box = object.find('bndbox')\n",
    "        d['xmin'] = bounding_box.findtext('xmin')\n",
    "        d['ymin'] = bounding_box.findtext('ymin')\n",
    "        d['xmax'] = bounding_box.findtext('xmax')\n",
    "        d['ymax'] = bounding_box.findtext('ymax')\n",
    "        object_list.append(d)\n",
    "\n",
    "    return object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(unlabeled_data_folder):\n",
    "    label_folder = os.path.join(data_labels_folder, folder)\n",
    "    folder = os.path.join(unlabeled_data_folder, folder)\n",
    "    for image in os.listdir(folder):\n",
    "        # Find corresponding XML_file\n",
    "        for label_file in os.listdir(label_folder):\n",
    "            if (image[:-4] in label_file):\n",
    "                label_filepath = os.path.join(label_folder, label_file)\n",
    "                image_filepath = os.path.join(folder, image)\n",
    "                object_list = read_xml_file_content(label_filepath)\n",
    "\n",
    "                crop_and_save_image(image_filepath, extracted_data_folder, object_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt with classical segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in images\n",
    "Let's first load in the images into the notebook, based on the folders downloaded from the Kaggle challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"Data/train\"\n",
    "train_folders = sorted(os.listdir(path_to_train))\n",
    "\n",
    "# Load in all images from the training set into this dictionary\n",
    "train_images_grouped = {}\n",
    "\n",
    "for folder in train_folders:\n",
    "    train_images = []\n",
    "    folder_path = os.path.join(path_to_train, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    images = os.listdir(folder_path)\n",
    "    for image in images:\n",
    "        image_path = os.path.join(folder_path, image)\n",
    "        loaded_image = cv.imread(image_path)\n",
    "        loaded_image = cv.cvtColor(loaded_image, cv.COLOR_BGR2RGB) \n",
    "        train_images.append(loaded_image)\n",
    "    folder = folder.replace(\" \", \"\")  # Remove whitespace\n",
    "    import re\n",
    "    folder = re.sub(\"[0-9.]\", \"\", folder) # Remove numbers and '.'\n",
    "    train_images_grouped[str(folder)] = train_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some samples from the six different background types for testing our segmentation model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the image_nums at once to see different images\n",
    "image_num = 2\n",
    "\n",
    "# Or modify each index individually to see different images\n",
    "neutral_bg_sample = train_images_grouped[\"neutral_bg\"][image_num]\n",
    "noisy_bg_sample = train_images_grouped[\"noisy_bg\"][image_num]\n",
    "hand_bg_sample = train_images_grouped[\"hand\"][image_num]\n",
    "neutral_bg_sample_ood = train_images_grouped[\"neutral_bg_outliers\"][image_num]\n",
    "noisy_bg_sample_ood = train_images_grouped[\"noisy_bg_outliers\"][image_num]\n",
    "hand_bg_sample_ood = train_images_grouped[\"hand_outliers\"][image_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Here we check if the number of training images loaded for each class is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0 \n",
    "for group in train_images_grouped:\n",
    "    group_len = len(train_images_grouped[group])\n",
    "    N += group_len\n",
    "    print(f\"Group name: '{group}' \\n with length: {group_len} \\n\")\n",
    "\n",
    "print(f\"N: {N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with thresholding, morphology & watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions. \n",
    "\n",
    "Since we used GIMP to analyse the overall colors of each coin based on the reference images, `gimp2opencvHSV` helps us convert from GIMP HSV values to OpenCV HSV values. \n",
    "\n",
    "And using HSV as our thresholding is standard, as it's easier to segment colors with this colorspace. This was also apparent in Lab 1. Thus, we have `extract_hsv_channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimp2opencvHSV(h, s, v):\n",
    "    \"\"\"\n",
    "    Convert GIMP HSV values to OpenCV HSV values.\n",
    "    GIMP uses the range [0, 360] for H, [0, 100] for S and V.\n",
    "    OpenCV uses the range [0, 180] for H, [0, 255] for S and V.\n",
    "    \"\"\"\n",
    "    return 180 * (h / 360), 255 * (s / 100), 255 * (v / 100)\n",
    "\n",
    "def extract_hsv_channels(image):\n",
    "    \"\"\"\n",
    "    Extracts the HSV channels from an image.\n",
    "    \"\"\"\n",
    "    hsv_image = cv.cvtColor(image, cv.COLOR_RGB2HSV)\n",
    "    h, s, v = cv.split(hsv_image)\n",
    "    return h, s, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the actual segmentation model. It involves\n",
    "\n",
    "1. **Thresholding based on HSV:** The threshold values were estimated from the reference images and their analysis in GIMP. The problems we face here is finding good thresholds which generalizes well.\n",
    "2. **Thresholding based on grayscale:** Here, we utilize simple grayscale thresholding, which is simple thresholding that binarizes the image. We also tried adding Otsu's thresholding, which uses the grayscale histogram of an image to detect an optimal threshold value that separates two regions with maximum inter-class variance. We tried all combinations, each with subpar results.\n",
    "3. **Morphology with closing and opening:** This is to close the holes *inside* the coins, which were removed by the threshold. If we didn't do this, the later openings and watershed algorithm would completely destroy the \"hole\" coin. Then, we applied opening. This was to break down the bridges connecting two or more coins, as well as remove background noise which were missed by the threshold.\n",
    "4. **Watershed algorithm:** The [watershed](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html) method is popular to find contours of shapes that \"touch\" each other. We need this because in the given images, coins may touch even after thresholding. \n",
    "5. **Finding contours based on results from watershed algorithm:** Now that we have the \"marked shapes\" from the watershed algorithm, we find the contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(img_original):\n",
    "    img = img_original.copy()\n",
    "\n",
    "    # We will use HSV color space for thresholding\n",
    "    h, s, v = extract_hsv_channels(img)\n",
    "\n",
    "    # The following are GIMP HSV values\n",
    "    h_upper = 65\n",
    "    h_lower = 18\n",
    "\n",
    "    s_upper = 85\n",
    "    s_lower = 10\n",
    "\n",
    "    v_upper = 95\n",
    "    v_lower = 40\n",
    "\n",
    "    # Convert to HSV values\n",
    "    img = cv.cvtColor(img, cv.COLOR_RGB2HSV)\n",
    "\n",
    "    # Main thresholding for coin and background separation\n",
    "    img_thres = cv.inRange(\n",
    "        img,\n",
    "        gimp2opencvHSV(h_lower, s_lower, v_lower),\n",
    "        gimp2opencvHSV(h_upper, s_upper, v_upper),\n",
    "    )\n",
    "\n",
    "    # Additional mask for dimly lit background. This was NOT getting thresholded.\n",
    "    background_mask = cv.inRange(\n",
    "        img, gimp2opencvHSV(32, 12, 72), gimp2opencvHSV(38, 28, 78)\n",
    "    )\n",
    "\n",
    "    # Additional mask for light silver coins. This was getting thresholded, likely due to bright lighting issues.\n",
    "    light_silver_coin_mask = cv.inRange(\n",
    "        img, gimp2opencvHSV(190, 6.2, 72.3), gimp2opencvHSV(205, 10.3, 81.2)\n",
    "    )\n",
    "\n",
    "    # Apply the masks\n",
    "    img[img_thres == 0 & ~(light_silver_coin_mask == 255)] = 0\n",
    "    img[background_mask == 255] = 0\n",
    "\n",
    "    # For some reason, there isn't a direct way to convert from HSV from gray?\n",
    "    img = cv.cvtColor(img, cv.COLOR_HSV2RGB)\n",
    "\n",
    "    # Threshold on grayscale image using simple thresholding & Otsu's thresholding\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    ret, img_thresh_gray = cv.threshold(\n",
    "        img_gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU\n",
    "    )\n",
    "\n",
    "    thresh = cv.bitwise_not(img_thresh_gray)\n",
    "\n",
    "    # Closing small holes\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, kernel, iterations=5)\n",
    "\n",
    "    # Removing small background noise and breaking down \"bridges\"\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations=8)\n",
    "\n",
    "    # Closing small holes again\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, kernel, iterations=5)\n",
    "\n",
    "    # Watershed algorithm\n",
    "    # Sure background area\n",
    "    sure_bg = cv.dilate(thresh, kernel, iterations=8)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv.distanceTransform(thresh, cv.DIST_L2, 5)\n",
    "    ret, sure_fg = cv.threshold(dist_transform, 0.2 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # The signal wasn't strong enough for a clear distance transform,\n",
    "    # so increasing the threshold (0.2) we get less of the coins but less of the bridges,\n",
    "    # but by lowering the threshold, we preserve more of the coins but also more of the bridges...\n",
    "    # thus we tried to erode again to remove the bridges again.\n",
    "\n",
    "    kernel = np.ones((16, 16), np.uint8)\n",
    "    sure_fg = cv.erode(sure_fg, kernel, iterations=8)\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    sure_fg = cv.dilate(sure_fg, kernel, iterations=4)\n",
    "\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Marker labelling\n",
    "    # Connected Components determines the connectivity of blob-like regions in a binary image.\n",
    "    ret, markers = cv.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "\n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    markers = cv.watershed(img, markers)\n",
    "    img[markers == -1] = [255, 0, 0]  # Optionally mark -1 boundaries if needed\n",
    "\n",
    "    # Create an output image to draw on\n",
    "    output_img = img_original.copy()\n",
    "\n",
    "    all_contours = []\n",
    "    # Process each region\n",
    "    for label in np.unique(markers):\n",
    "        if label == 0 or label == 1:  # Background or borders\n",
    "            continue\n",
    "\n",
    "        # Create a mask for the current region\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        mask[markers == label] = 255\n",
    "\n",
    "        # Find contours and get the bounding box\n",
    "        contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for cntr in contours:\n",
    "            area = cv.contourArea(cntr)\n",
    "\n",
    "            # Check if contour is big enough\n",
    "            if area < 70000 or area > 700000:\n",
    "                continue\n",
    "            \n",
    "            # The contours found above weren't good circles either, so the conditions below were removed.\n",
    "            # # Check if contour is \"cirular\" enough\n",
    "            # perimeter = cv.arcLength(cntr, True)\n",
    "\n",
    "            # # Calculate circularity\n",
    "            # if perimeter == 0:\n",
    "            #     continue  # Avoid division by zero\n",
    "\n",
    "            # circularity = 4 * np.pi * (area / (perimeter**2))\n",
    "\n",
    "            # if circularity < 0.1:\n",
    "            #     continue\n",
    "\n",
    "            # # Calculate convexity\n",
    "            # hull = cv.convexHull(cntr)\n",
    "            # hull_area = cv.contourArea(hull)\n",
    "\n",
    "            # if hull_area == 0:\n",
    "            #     continue\n",
    "            # convexity = area / hull_area\n",
    "\n",
    "            # if convexity < 0.8:\n",
    "            #     continue\n",
    "\n",
    "            # Calculate bounding box\n",
    "            x, y, w, h = cv.boundingRect(cntr)\n",
    "            # Draw bounding box\n",
    "            cv.rectangle(output_img, (x, y), (x + w, y + h), (255, 0, 0), 15)\n",
    "\n",
    "            # Draw contour (optional)\n",
    "            # cv.drawContours(output_img, [cntr], -1, (0, 0, 255), 2)\n",
    "\n",
    "            all_contours.append(cntr)\n",
    "\n",
    "    return output_img, all_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, coins are getting recognized, okay. But there are a few obvious problems:\n",
    "1. Backgrounds are getting falsely detected as coins. This is most likely due to insufficient thresholding and morphology operations. See the yellow spots of noisy background and parts of the hand. We have tried to calculate the shape attributes, such as convexity and circularity, but they don't work well because the contours of real coins and false coins were both also imperfect.\n",
    "2. The bounding boxes of coins are not perfectly alligned. Although not a big issue if we train our classifier well, it introduces an extra layer of noise and imperfection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to show more images to test on\n",
    "TOTAL_IMAGES = 1\n",
    "\n",
    "for image_num in range(TOTAL_IMAGES):\n",
    "    neutral_bg_sample = train_images_grouped[\"neutral_bg\"][image_num]\n",
    "    noisy_bg_sample = train_images_grouped[\"noisy_bg\"][image_num]\n",
    "    hand_bg_sample = train_images_grouped[\"hand\"][image_num]\n",
    "    neutral_bg_sample_ood = train_images_grouped[\"neutral_bg_outliers\"][image_num]\n",
    "    noisy_bg_sample_ood = train_images_grouped[\"noisy_bg_outliers\"][image_num]\n",
    "    hand_bg_sample_ood = train_images_grouped[\"hand_outliers\"][image_num]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(get_contours(neutral_bg_sample)[0])\n",
    "    plt.title(\"Neutral background\")\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(get_contours(noisy_bg_sample)[0])\n",
    "    plt.title(\"Noisy background\")\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(get_contours(hand_bg_sample)[0])\n",
    "    plt.title(\"Hand background\")\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(get_contours(neutral_bg_sample_ood)[0])\n",
    "    plt.title(\"Neutral background\")\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(get_contours(noisy_bg_sample_ood)[0])\n",
    "    plt.title(\"Noisy background\")\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(get_contours(hand_bg_sample_ood)[0])\n",
    "    plt.title(\"Hand background\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of poor segmentation. We can see why the segmentation model might misclassify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the bounding boxes from the images\n",
    "def crop_bounding_boxes(img_original, contours):\n",
    "    '''\n",
    "    Takes in an image with multiple coins and its contours that represent the coins.\n",
    "    Outputs a list of cropped images.\n",
    "    '''\n",
    "\n",
    "    img = img_original.copy()\n",
    "    cropped_images = []\n",
    "\n",
    "    for cntr in contours:\n",
    "        area = cv.contourArea(cntr)\n",
    "\n",
    "        # Check if contour is big enough\n",
    "        if area < 70000 or area > 700000:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv.boundingRect(cntr)\n",
    "        cropped_images.append(img[y:y+h, x:x+w])\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "# Apply filters\n",
    "noisy_bg_sample_filtered, contours = get_contours(noisy_bg_sample)\n",
    "\n",
    "# Remember to pass in the original image, otherwise we get the red bounding boxes as well\n",
    "cropped_coins = crop_bounding_boxes(noisy_bg_sample, contours)\n",
    "\n",
    "# Plot the cropped coins\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, coin in enumerate(cropped_coins):\n",
    "    plt.subplot(1, len(cropped_coins), i+1)\n",
    "    plt.imshow(coin)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other failed classical segmentation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried using hough transform and background subtraction as well, both with hard-to-refine results. They weren't that much better than attempt 1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message for Om\n",
    "I didn't add any other segmentation techniques before feeding hough transform here, it just converts to grayscale. If you want to feed it a thresholded image you can do that as well, you just have to take out the imread(src) and maybe the resizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_transform(image):\n",
    "    # Loads an image\n",
    "    src = image\n",
    "    # Check if image is loaded fine\n",
    "    if src is None:\n",
    "        print ('Error: No Imgae')\n",
    "\n",
    "    # Load image\n",
    "    large_img = cv.imread(src)\n",
    "    small_to_large_image_size_ratio = 0.2\n",
    "    small_img = cv.resize(large_img, # original image\n",
    "                       (0,0), # set fx and fy, not the final size\n",
    "                       fx=small_to_large_image_size_ratio, \n",
    "                       fy=small_to_large_image_size_ratio, \n",
    "                       interpolation=cv.INTER_NEAREST)\n",
    "    gray = cv.cvtColor(small_img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 30,\n",
    "    param1=60, param2=80,\n",
    "    minRadius=25, maxRadius=90)\n",
    "\n",
    "    # Add circles and centerpoint\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1])\n",
    "            # circle center\n",
    "            cv.circle(small_img, center, 1, (0, 100, 100), 3)\n",
    "            # circle outline\n",
    "            radius = i[2]\n",
    "            cv.circle(small_img, center, radius, (255, 0, 255), 3)\n",
    " \n",
    "    plt.imshow(cv.cvtColor(small_img, cv.COLOR_RGB2BGR))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define paths\n",
    "path_neutral_bg = os.path.join(\"data/train/\", \"1. neutral_bg\", \"L1010277.JPG\")\n",
    "path_noisy_bg = os.path.join(\"data/train/\", \"2. noisy_bg\", \"L1010325.JPG\")\n",
    "\n",
    "hough_transform(path_neutral_bg)\n",
    "hough_transform(path_noisy_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four distinct backgrounds in the given images. One for neutral background, two for noisy backgrounds, and one for hand background. For each type, the images are aligned, even for test images. So, we can find take the simple mean for each set, and get a model for each type of background. Then, by subtracting the test image with the background model, we theoretically should get only the coins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the background models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of neutral background\n",
    "# ---\n",
    "# Get the background model\n",
    "neutral_bg_images = train_images_grouped[\"neutral_bg\"]\n",
    "neutral_bg_images_ood = train_images_grouped[\"neutral_bg_outliers\"]\n",
    "print(f\"Number of images in neutral background: {len(neutral_bg_images)}\")\n",
    "print(f\"Number of images in neutral background ood: {len(neutral_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "neutral_bg_images = np.array(neutral_bg_images)\n",
    "neutral_bg_images_ood = np.array(neutral_bg_images_ood)\n",
    "\n",
    "neutral_bg_avg = np.mean(neutral_bg_images, axis=0, dtype=np.int32)\n",
    "neutral_bg_avg_ood = np.mean(neutral_bg_images_ood, axis=0, dtype=np.int32)\n",
    "\n",
    "# Take average between non-ood and ood\n",
    "neutral_bg_avg = (neutral_bg_avg + neutral_bg_avg_ood) // 2\n",
    "# ---\n",
    "\n",
    "# Distribution of noisy background type A\n",
    "# ---\n",
    "# Get the background model\n",
    "noisy_bg_images = train_images_grouped[\"noisy_bg\"]\n",
    "print(f\"Number of images in noisy background: {len(noisy_bg_images)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "noisy_bg_images = np.array(noisy_bg_images)\n",
    "noisy_bg_avg = np.mean(noisy_bg_images, axis=0, dtype=np.int32)\n",
    "# ---\n",
    "\n",
    "# Distribution of noisy background type B\n",
    "# ---\n",
    "# Get the background model\n",
    "noisy_bg_images_ood = train_images_grouped[\"noisy_bg_outliers\"]\n",
    "print(f\"Number of images in noisy background ood: {len(noisy_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "noisy_bg_images_ood = np.array(noisy_bg_images_ood)\n",
    "noisy_bg_avg_ood = np.mean(noisy_bg_images_ood, axis=0, dtype=np.int32)\n",
    "# ---\n",
    "\n",
    "# Distribution of hand background\n",
    "# ---\n",
    "# Get the background model\n",
    "hand_bg_images = train_images_grouped[\"hand\"]\n",
    "hand_bg_images_ood = train_images_grouped[\"hand_outliers\"]\n",
    "print(f\"Number of images in hand background: {len(hand_bg_images)}\")\n",
    "print(f\"Number of images in hand background ood: {len(hand_bg_images_ood)}\")\n",
    "\n",
    "# Get pixel-wise average of the images\n",
    "hand_bg_images = np.array(hand_bg_images)\n",
    "hand_bg_avg = np.mean(hand_bg_images, axis=0, dtype=np.int32)\n",
    "\n",
    "hand_bg_images_ood = np.array(hand_bg_images_ood)\n",
    "hand_bg_avg_ood = np.mean(hand_bg_images_ood, axis=0, dtype=np.int32)\n",
    "\n",
    "# Take average between non-ood and ood\n",
    "hand_bg_avg = (hand_bg_avg + hand_bg_avg_ood) // 2\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the segmentation model with background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt something similar with attempt 1, but with background subtraction instead. This is the pipeline:\n",
    "\n",
    "1. **Subtract background:** We first detect the background, by calculating the MSE between the test image and the background model. The background model with the lowest MSE is the detected background. If the background is a hand, we will perform normal thresholding and not do background subtraction. This is because the hands are not aligned. Otherwise, background subtraction is performed.\n",
    "2. **Enhance contrast:** Increasing contrast will help separate the coins from the background more. We also tried this above with the 1st attempt, with poor results. This means the image will be in grayscale.\n",
    "3. **Thresholding:** Since we are in grayscale, let's rely on the increased contrast and Li thresholding. It worked better than Otsu, which is why we used it.\n",
    "4. **Morphology:** We used morphology to remove small holes (which again, were thresholded away) and eroded it once more. We didn't want to dilate it back to reduce the chances of bridges between coins.\n",
    "5. **Contour detection:** We used the contour finding function from OpenCV as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import filters, measure, morphology\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_image(image, title=\"Image\", cmap_type=\"gray\"):\n",
    "    plt.imshow(image, cmap=cmap_type)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def enhance_contrast(image, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:  # Color image\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        cl = clahe.apply(l)\n",
    "        limg = cv2.merge([cl, a, b])\n",
    "        enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    else:  # Grayscale image\n",
    "        enhanced_img = clahe.apply(image)\n",
    "    return enhanced_img\n",
    "\n",
    "\n",
    "# Apply thresholding\n",
    "def apply_threshold(image, method=\"otsu\"):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
    "    thresh_val = (\n",
    "        filters.threshold_otsu(gray) if method == \"otsu\" else filters.threshold_li(gray)\n",
    "    )\n",
    "    binary = gray > thresh_val\n",
    "    return binary\n",
    "\n",
    "\n",
    "def clean_image(binary_image):\n",
    "    # Removing small holes\n",
    "    cleaned = morphology.remove_small_holes(binary_image, area_threshold=500)\n",
    "\n",
    "    # Erode without going back to the original size\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (13, 13))\n",
    "    cleaned = cv2.erode(cleaned.astype(np.uint8), kernel, iterations=3)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def detect_coins(binary_image, original_image):\n",
    "    all_contours = []\n",
    "    output_img = original_image.copy()\n",
    "    contours, _ = cv2.findContours(\n",
    "        binary_image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    for contour in contours:\n",
    "        if (\n",
    "            cv2.contourArea(contour) > 70000 and cv2.contourArea(contour) < 700000\n",
    "        ):  # Adjust size threshold based on your need\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_img, (x, y), (x + w, y + h), (255, 0, 0), 10)\n",
    "            all_contours.append(contour)\n",
    "\n",
    "    return output_img, all_contours\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    # Compute the mean squared error between the two images\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "\n",
    "def detect_background(image):\n",
    "    metrics = {}\n",
    "    backgrounds = [neutral_bg_avg, noisy_bg_avg, noisy_bg_avg_ood, hand_bg_avg]\n",
    "\n",
    "    # Compute MSE for each background\n",
    "    for idx, bg in enumerate(\n",
    "        [neutral_bg_avg, noisy_bg_avg, noisy_bg_avg_ood, hand_bg_avg], 0\n",
    "    ):\n",
    "        mse_val = mse(image, bg)\n",
    "        metrics[idx] = mse_val\n",
    "\n",
    "    best_match_by_mse = min(metrics, key=lambda k: metrics[k])\n",
    "\n",
    "    print(\"Metrics (MSE):\", metrics)\n",
    "    print(\"Best match by MSE:\", best_match_by_mse)\n",
    "\n",
    "    return backgrounds[best_match_by_mse], best_match_by_mse\n",
    "\n",
    "\n",
    "def apply(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    bg, idx = detect_background(image)\n",
    "    background = bg.copy()\n",
    "\n",
    "    if idx == 3:  # Hand background\n",
    "\n",
    "        # Convert to HSV\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "        # Threshold hand away\n",
    "        hand_mask_light_skin = cv2.inRange(\n",
    "            img, gimp2opencvHSV(310, 5, 70), gimp2opencvHSV(360, 25, 85)\n",
    "        )\n",
    "        hand_mask_dark_skin = cv2.inRange(\n",
    "            img, gimp2opencvHSV(0, 20, 65), gimp2opencvHSV(25, 55, 83)\n",
    "        )\n",
    "\n",
    "        # Remove hand\n",
    "        img[hand_mask_light_skin == 255] = 0\n",
    "        img[hand_mask_dark_skin == 255] = 0\n",
    "\n",
    "        # Convert back to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        # Enhance contrast\n",
    "        enhance_contrast_image = enhance_contrast(img, clip_limit=5.0)\n",
    "\n",
    "        # Apply thresholding\n",
    "        thresholded_image = apply_threshold(enhance_contrast_image)\n",
    "\n",
    "        # Clean image\n",
    "        cleaned_image = clean_image(thresholded_image)\n",
    "\n",
    "        # Detect coins\n",
    "        detected_coins_image, contours = detect_coins(\n",
    "            cleaned_image.astype(np.uint8), img\n",
    "        )\n",
    "\n",
    "        return detected_coins_image, contours\n",
    "\n",
    "    else:\n",
    "        img = np.array(img)\n",
    "        img = img.astype(np.int32)\n",
    "        img = cv2.absdiff(img, background)\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        enhance_contrast_image = enhance_contrast(img)\n",
    "\n",
    "        thresholded_image = apply_threshold(enhance_contrast_image, method=\"li\")\n",
    "    \n",
    "        cleaned_image = clean_image(thresholded_image)\n",
    "\n",
    "        detected_coins_image, contours = detect_coins(\n",
    "            cleaned_image.astype(np.uint8), image\n",
    "        )\n",
    "\n",
    "        return detected_coins_image, contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the background detection works, but it is still much affected by the operations we performed, such as thresholding and morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = neutral_bg_sample.copy()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "detected_coins_image, contours = apply(image)\n",
    "\n",
    "plt.imshow(detected_coins_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet-50 for object localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time was running out, and classical segmentation methods were too sensitive to the parameters. We went with a transfer learning approach to find the bounding boxes of coins, i.e. object localization. In this sense, we went with ResNet-50, pre-trained on ImageNet, with size [97.8MB](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html).\n",
    "\n",
    "More specifically, we are using the [faster R-CNN model with a ResNet50 backbone](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html).\n",
    "\n",
    "Region-based Convolutional Neural Networks (R-CNN) is specifically to tackle object detection in computer vision tasks. Faster R-CNN is not strictly necessary, but it potentially lets us generalize the application to real-time coin detection in the future.\n",
    "\n",
    "Moreover, ResNet50 is a residual neural network. It is quite deep (50 layers) which helps it learn more detailed features of the image. Since our images are quite noisy and contains a lot of variation, this will make it more accurate for coin localization. Moreover, it is also one of the only two provided pre-trained models from PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** The folder structure of the training dataset must follow exactly:\n",
    "```\n",
    "dataset\n",
    "├── images\n",
    "│   ├── L1010277.JPG\n",
    "├── annotations\n",
    "│   ├── L1010277 [1].xml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the labelled training dataset into the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File folder organization\n",
    "unlabeled_data_folder = os.path.join(\"Data\", \"train\")\n",
    "\n",
    "data_labels_folder = os.path.join(\"Labelled_Training_Data\")\n",
    "\n",
    "coin_detection_dataset = os.path.join(\"dataset\")\n",
    "coin_detection_images = os.path.join(\"dataset\", \"images\")\n",
    "coin_detection_annotations = os.path.join(\"dataset\", \"annotations\")\n",
    "os.makedirs(coin_detection_dataset, exist_ok=True)\n",
    "os.makedirs(os.path.join(coin_detection_dataset, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(coin_detection_dataset, \"annotations\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Data/train/folder\n",
    "for folder in os.listdir(unlabeled_data_folder):\n",
    "    # Labelled_Training_Data/label_folder\n",
    "    label_folder = os.path.join(data_labels_folder, folder)\n",
    "    # folder = path to folder\n",
    "    folder = os.path.join(unlabeled_data_folder, folder)\n",
    "    for image in os.listdir(folder):\n",
    "        # Find corresponding XML_file\n",
    "        for label_file in os.listdir(label_folder):\n",
    "            # Ex: L1010277.JPG --> L1010277\n",
    "            if (image[:-4] in label_file):\n",
    "                label_filepath = os.path.join(label_folder, label_file)\n",
    "                image_filepath = os.path.join(folder, image)\n",
    "                shutil.copy2(label_filepath, coin_detection_annotations)\n",
    "                shutil.copy2(image_filepath, coin_detection_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `CoinDataset` is simply helping us parsing the images (.JPG) and annotations (.XML) in order to be fed into the network for training. After that, we will set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser for data\n",
    "class CoinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.annotations = list(sorted(os.listdir(os.path.join(root, \"annotations\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        tree = ET.parse(ann_path)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # Background and coin\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Data transforms\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell if you want to train the model on the custom (coins) dataset. This cell actually loads in the data and starts the training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = CoinDataset(\"dataset/\", transforms=transforms)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "# Fine-tune the model based on our labelled training data\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"Beginning epoch\", epoch + 1)\n",
    "    for i, (images, targets) in enumerate(data_loader, 0):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(data_loader)}], Loss: {running_loss/10:.4f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"coin_detector.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the next cell if you already have a trained model, and skip the above cell. This is to prevent having to re-train the model every time we restart the kernel. Make sure `coin_detector.pth` is in the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.utils.data\n",
    "# import torchvision\n",
    "# from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load the trained model\n",
    "# model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# num_classes = 2  # Background and coin\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"coin_detector.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform object localization on test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will predict bounding boxes on the images, crop the coins out for each image and save them to an output folder.\n",
    "\n",
    "**IMPORTANT:** The folder structure of input test images must follow exactly:\n",
    "```\n",
    "data\n",
    "├── test\n",
    "│   ├── L1010277.JPG\n",
    "│   ├── L1010239.JPG\n",
    "```\n",
    "And the folder structure of output images will be:\n",
    "```\n",
    "output\n",
    "├── L0000000.JPG                        <---------------- This is a directory\n",
    "│   ├── 1920_1519_2595_2181.jpg         <---------------- This is an image of the 1st cropped coin, belonging to L0000000.JPG\n",
    "│   ├── 2780_2159_3326_2685.jpg         <---------------- This is an image of the 2nd cropped coin, belonging to L0000000.JPG\n",
    "├── L0000001.JPG                        \n",
    "│   ├── 1920_1519_2595_2181.jpg         \n",
    "│   ├── 2780_2159_3326_2685.jpg         \n",
    "```\n",
    "If an \"output\" folder already exists, it will get overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on test images\n",
    "model.eval()\n",
    "test_data_path = \"Data/test\"\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "for img_name in os.listdir(test_data_path):\n",
    "    img_path = os.path.join(test_data_path, img_name)\n",
    "    # Check if the file is an image\n",
    "    if not img_name.endswith(\".JPG\") and not img_name.endswith(\".jpg\"):\n",
    "        continue\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "    # Create a directory for each image, where cropped coins will be saved\n",
    "    os.mkdir(f\"output/{img_name}\")\n",
    "    boxes = outputs[0][\"boxes\"].cpu().numpy()\n",
    "    # Crop coins from current image\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        coin = img.crop((xmin, ymin, xmax, ymax))\n",
    "        cropped_coin_path = os.path.join(\n",
    "            f\"output/{img_name}\", f\"{xmin}_{ymin}_{xmax}_{ymax}.jpg\"\n",
    "        )\n",
    "        coin.save(cropped_coin_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "images_classes_path = \"./Extracted_Training_data\"\n",
    "image_classes_list = os.listdir(images_classes_path)\n",
    "num_classes = len(image_classes_list)\n",
    "print(\"total {} classes\".format(num_classes))\n",
    "\n",
    "test_images_path = \"output\"\n",
    "images_dict = {}\n",
    "image_name_list = os.listdir(test_images_path)\n",
    "for image_folder in image_name_list:\n",
    "    images_dict[image_folder] = []\n",
    "    images_path = os.path.join(test_images_path, image_folder)\n",
    "    # Check if the path is a directory\n",
    "    if not os.path.isdir(images_path):\n",
    "        continue\n",
    "    for image in os.listdir(images_path):\n",
    "        path_to_coin_image = os.path.join(images_path, image)\n",
    "        images_dict[image_folder].append(path_to_coin_image)\n",
    "num_images = len(image_name_list)\n",
    "print(\"total {} test images\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "!rm -rf ./logs/\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "This section imports the necessary libraries for image processing and augmentation `ImageDataGenerator`and related functions from `tensorflow.keras.preprocessing.image` are used for loading and augmenting images. The `os` module handles directory operation, and `tqdm` provides a progress bar for the augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ImageDataGenerator\n",
    "\n",
    "An instance of `ImageDataGenerator`is created with specific augmentation parameters. The purpose of this data augmentation is to artificially expand the training dataset by generating new, altered versions of the existing coin images. This is particularly beneficial since our original training dataset is small, so augmentation will help to prevent overfitting and improve the model's ability to generalize to new data. \n",
    "\n",
    "For this task, the chosen augmentations are Rescaling, Rotation, Brightness Adjustment and Zoom. Rescaling of pixel values from [0,255] range to [0,1] is done to facilitate faster convergence during training. We apply a random rotation of up to 90 degrees as well, this is to make the model robust to changes in orientation, improving its ability to recognize coins regardless of rotation. Since lighting conditions vary slightly in the training data a brightness adjustment is also implemented. Finally a slight zoom in is implemented since the segmentation algorithm can cut of some edges of coins and therefore preparing the model for that in the training data makes generalize better to the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the directories to save the augmented images\n",
    "augmented_data_path = 'path_to_save_augmented_images'\n",
    "os.makedirs(augmented_data_path, exist_ok=True)\n",
    "\n",
    "crazy_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    shear_range=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment and Save Images\n",
    "This loop iterates over each class directory and each image within that directory. Each image is loaded and converted to an array. The `datagen.flow` function generates augmented images which are saved to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of augmented images to generate per original image\n",
    "num_augmented_images = 10\n",
    "\n",
    "# Load the original images\n",
    "original_data_path = images_classes_path\n",
    "\n",
    "# Calculate the total number of files\n",
    "all_files = sum([len(files) for r, d, files in os.walk(original_data_path)])\n",
    "pbar = tqdm(total=all_files * num_augmented_images, desc=\"Augmenting images\")\n",
    "\n",
    "for class_dir in os.listdir(original_data_path):\n",
    "    class_path = os.path.join(original_data_path, class_dir)\n",
    "    save_class_path = os.path.join(augmented_data_path, class_dir)\n",
    "    os.makedirs(save_class_path, exist_ok=True)\n",
    "    \n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "        # Generate and save augmented images\n",
    "        i = 0\n",
    "        for batch in crazy_datagen.flow(x, batch_size=1, save_to_dir=save_class_path, save_prefix='aug', save_format='jpeg'):\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data generators for Training and validation\n",
    "Two data generators are defined: one for training and one for validation. The images are rescaled, and the dataset is split into training and validation subsets. The images are resized to 224x224 picels, and a batch size of 32 is used. Different batch sizes were tested, but 32 was found to be a suitable batch size because of the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    augmented_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    augmented_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# # Test data generator remains the same\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     test_images_path,\n",
    "#     target_size=(224, 224),\n",
    "#     shuffle=False,\n",
    "#     batch_size=1,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "\n",
    "# # Get filenames and number of samples\n",
    "# filenames = test_generator.filenames\n",
    "# nb_samples = len(filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a batch for verification\n",
    "x_batch, y_batch = next(train_generator)\n",
    "print(f\"Batch x_shape: {x_batch.shape}, Batch y_shape: {y_batch.shape}\")\n",
    "\n",
    "# Verify the data generators\n",
    "train_batch = next(iter(train_generator))\n",
    "val_batch = next(iter(val_generator))\n",
    "print(train_batch[0].shape, train_batch[1].shape)\n",
    "print(val_batch[0].shape, val_batch[1].shape)\n",
    "\n",
    "print(f\"Steps per epoch: {train_generator.samples // train_generator.batch_size}\")\n",
    "print(f\"Validation steps: {val_generator.samples // val_generator.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration using Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Customizing InceptionV3 Base Model\n",
    "The pre-trained InceptionV3 model is loaded with weights from ImageNet, excluding the top fully connected layers. This allows us to customize the output layers for the coin classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras.applications.xception import Xception\n",
    "# base_model = InceptionV3(include_top=False, # Since we will create our own\n",
    "#                     weights='imagenet', \n",
    "#                     input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# base_model.summary()\n",
    "\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Custom Output Layers\n",
    "\n",
    "The ouput of the IncdeptionV3 base model is passed through a series of custom layers:\n",
    "\n",
    "**Flatten:** Converst the feature maps to a 1D feature vector\\\n",
    "**BatchNormalization:** Normalizes the activations to improve training stability\\\n",
    "**Dense (512 units):** Fully connected layer with ReLy activation\\\n",
    "**Dropout (50%):** Regularization technique to prevent overfitting\\\n",
    "**Dense (num_classes):** Output layer with softmax actiavtion for the final coin classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = inceptionv3_base.output\n",
    "# # x = Flatten()(x)\n",
    "# # x = BatchNormalization()(x)\n",
    "# # x = Dense(512, activation='relu')(x)\n",
    "# # x = Dropout(rate = .5)(x)\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing of Base layers\n",
    "\n",
    "This block freezes all the layer except the last 7 in order to retrain the pre-trained weights during initial training. The last 7 layers remain trainable to fine-tune them for coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing in order to only train the last 7 layers\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "The model is compiled with the Adam optimizer, which is chosen for its efficiency and adaptability. The learning rate is set to 0.0001. The loss function is categorical cross-entropy, which is suitibla for the multi-class coin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "# optimizer = SGD(lr=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Callbacks\n",
    "Three callbacks are defined to enhance the training process:\n",
    "\n",
    "**ModelCheckpoint:** Saves the model with the best validation accuracy\\\n",
    "**TensorBoard:** Logs training metrics for visualization.\\\n",
    "**EarlyStopping:** Stops training if the validation does not improve for 6 consecutive epochs, restoring the best weights.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.keras',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "my_callbacks = [checkpoint, tensorboard_callback, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=15,\n",
    "          callbacks=my_callbacks)\n",
    "print('Training done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label = \"\"\n",
    "# model.save(model_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.fit(train_generator, \n",
    "                                validation_data=val_generator, \n",
    "                                epochs=30,\n",
    "                                callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracies and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = np.array(history.history['accuracy'])\n",
    "    val_acc = np.array(history.history['val_accuracy'])\n",
    "    loss = np.array(history.history['loss'])\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'history' is the History object returned from model.fit\n",
    "plot_history(history)\n",
    "plot_history(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "os.system('pkill -f \"tensorboard\"')\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir\", \"logs/fit\"])\n",
    "time.sleep(5)\n",
    "display(HTML(f'<a href=\"http://localhost:6006\" target=\"_blank\">Open TensorBoard</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on given test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(path, verbose=False, show_image=False):\n",
    "    # Load and display the image\n",
    "    img = mpimg.imread(path)\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = Image.open(path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dimensions to fit model input\n",
    "\n",
    "    # Predict the probabilities\n",
    "    probabilities = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get class labels\n",
    "    class_labels = {v: k for k, v in val_generator.class_indices.items()}\n",
    "\n",
    "    # Prepare data for a nice display\n",
    "    sorted_indices = np.argsort(probabilities[0])[::-1]\n",
    "    results = [(class_labels[i], probabilities[0][i]) for i in sorted_indices]\n",
    "\n",
    "    # Print sorted probabilities and their corresponding class labels\n",
    "    if verbose:\n",
    "        print(\"Class\".ljust(15), \"Probability\")\n",
    "        print(\"-\" * 30)\n",
    "        for label, prob in results:\n",
    "            print(f\"{label.ljust(15)} : {prob:.4f}\")\n",
    "\n",
    "    # Return the class label with the highest probability\n",
    "    return results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing submission\n",
    "import pandas as pd\n",
    "\n",
    "# This will used as a template to build the submission.\n",
    "if not os.path.exists(\"sample_submission.csv\"):\n",
    "    raise FileNotFoundError(\"sample_submission.csv not found!\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.head()\n",
    "\n",
    "# Perform prediction on all test images.\n",
    "for key in images_dict:\n",
    "    labels_count = {key: 0 for key in image_classes_list}\n",
    "    for coin_path in images_dict[key]:\n",
    "        predicted_class = predict_images(coin_path, verbose=True, show_image=True)\n",
    "        labels_count[predicted_class] += 1\n",
    "\n",
    "    row_to_write = submission.loc[submission[\"id\"] == key[:-4]]\n",
    "\n",
    "    for label in labels_count:\n",
    "        row_to_write[label] = labels_count[label]\n",
    "\n",
    "    submission.loc[submission[\"id\"] == key[:-4]] = row_to_write\n",
    "\n",
    "\n",
    "# Save the submission\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "Y_pred = model.predict(val_generator, 1167//32+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cfm = confusion_matrix(val_generator.classes, y_pred)\n",
    "cfm = np.around(cfm.astype('float')/cfm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "classes = ['0.1CHF', '0.1EUR', '0.01EUR', '0.2CHF', '0.2EUR', '0.02EUR', '0.5CHF', '0.05CHF', '0.5EUR', '0.05EUR', '1CHF', '1EUR', '2CHF', '2EUR', '5CHF', 'OOD']\n",
    "cfm_pd = pd.DataFrame(cfm, index = classes, columns = classes)\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cfm_pd, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
